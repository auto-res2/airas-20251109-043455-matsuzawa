
LLM Name: o3-2025-04-16
Input:
You are an accomplished researcher in the field of machine learning. Based on the instructions below, please refine the research hypothesis provided in "Hypothesis Info" to make it more novel and academically as well as socially valuable.

# Instructions
- Carefully read the research theme described in "Research Topic" and understand the problems this research should address, as well as the broader impact it aims to achieve.
- "Hypothesis Info" contains a newly proposed research hypothesis related to the "Research Topic." Read the hypothesis thoroughly and refine it so that it becomes a research contribution with stronger novelty and significance.
- The reasoning behind the evaluation of novelty is provided in "Novelty," and the reasoning for significance is provided in "Significance." Use these evaluations as references to improve the research hypothesis.
- "Research Study List" provides a set of related prior studies. Each entry includes a summary of its title, main contributions, methodologies, results, and limitations. Read through these summaries to understand the direction and focus of research in this domain.
- "Hypothesis Info History" contains past research hypotheses and their evaluation reasoning. If provided, use it as a reference for refining the current hypothesis.
- Pay close attention to how each prior study builds upon earlier work and which limitations remain unresolved. Organize this information to form a clear understanding of the current research landscape.
- Identify the key gaps, challenges, or unmet needs that persist across these studies. Also, consider whether methods or concepts from other domains could help address these limitations.
- Reflect on unexplored aspects or areas for improvement (e.g., new techniques, new evaluation metrics, novel datasets, or methods for generalizing findings). Ensure that the refined hypothesis is broadly applicable and not overly dependent on a specific dataset or model.
- Please limit research hypotheses to those that can be validated with a Python script.
- Please also consider ways to enhance the feasibility of validation and improve accordingly.

# Research Topic
Learning rate optimization for fine-tuning Qwen3-0.6B on GSM8K elementary math problems

# Current Hypothesis
{
    "Open Problems": "P1  Existing curvature-/noise-aware LR rules (NCC, SNAC) are purely reactive: they decide from the *current* statistics and ignore the *remaining compute budget*.  They therefore (i) often start to cool down too early when long budgets are available and (ii) provide no guarantee that training will have settled before the run terminates.\nP2  Hutchinson top-eigen or trace probes still cost ≈1 backward.  When a budget monitor tells that only few updates are left we would like to spend *zero* extra FLOPs on LR control.\nP3  The diagonal 2-nd-moment that AdamW already maintains is a noisy but free proxy for both gradient noise σ² and bulk curvature λ̄.  No scheduler exploits this to *predict* the safe LR instead of merely bounding it.\nP4  A single global LR under-utilises flat LoRA blocks late in training.  Static √(λ̄/dλ_i) rescaling (NCC-L) no longer works once λ̄ itself starts to fall during cooldown.",
    "Methods": "Budget-Aware Curvature-Noise (BACON) Scheduler\nA  Two-phase horizon planning\n   • Let T be the user-supplied *budget* in optimiser steps.  Every W steps keep a running counter t.\n   • Phase-1 (exploration) while  t/T < τ  :  follow an *analytic* constant-LR rule  lr_t = min( lr_max , α/(σ̂²+λ̄_√) )  that maximises expected loss descent per step under the GNT formula; here σ̂² and λ̄_√ are cheap diagonal estimates (see C).\n   • Phase-2 (cool-down) once  t/T ≥ τ  :  freeze M (grad-acc) and drive lr_t to 0.1·lr_max with a (1−√p) decay, p=(t−τT)/(T−τT).\n   The switch fraction τ≈0.6 is theory-derived from compute-optimal scaling laws and needs no tuning.\nB  Budget-aware noise ↔ batch trade-off\n   • Keep the stability ratio ρ_t = lr_t·λ̂_max /(1+σ̂²) inside [0.05,0.15] *by preference* via grad-acc mul M (doubling / halving) as long as t<T−200 – i.e. only when enough budget remains to amortise the bigger batches.\nC  Zero-cost curvature / noise surrogates\n   • Use AdamW’s exponential 2nd moment v_t to form\n        λ̄_t = mean_i √v_{t,i} ,  σ̂²_t = var_i √v_{t,i} .\n     No extra backward pass – just two .mean() calls each W steps.\n   • λ̂_max  is approximated by  λ̄_t·(1+κ_t)  with  κ_t = max_i √v_{t,i}/λ̄_t  .  This is <3 % off on GSM8K.\nD  Dynamic block scaling\n   • Per-LoRA block local LR_i = lr_t·(λ̄_t/√v_{t,b})  re-evaluated every W steps; this follows the time-varying bulk curvature automatically.\nE  Asymptotically free implementation\n   • One Python class ~50 lines; no HVP, no extra grads, O(#params) ops once per W=32 steps – <0.15 % wall time.",
    "Experimental Setup": "Model  Qwen3-0.6B 4-bit NF4  (LoRA r=64, α=16) 1×A100-80G.\nData   GSM8K train 8 k, eval on test.\nBudgets T∈{2 k,4 k,8 k} optimiser updates (≈0.5,1,2 epochs) to test compute-optimality.\nBaselines  constant-LR, cosine, SNAC (best prior), schedule-free AdamW.\nBACON hyper-params  lr_max=3e-4,  τ=0.6,  W=32,  β2=0.95 (inherits AdamW),  M₀=8,  M_max=32.\n3 random seeds.",
    "Primary Metric": "Pass@1 accuracy on GSM8K test at equal *wall-clock* budget.  Aux: final training loss, #divergences, actual vs. target step budget.",
    "Experimental Code": "class BACON(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self,opt,T,lr_max=3e-4,tau=0.6,W=32,rho_lo=0.05,rho_hi=0.15):\n        self.opt=opt; self.T=T; self.t=0; self.lr=lr_max; self.tau=tau; self.W=W\n        self.rlo=rho_lo; self.rhi=rho_hi; self.M=opt.param_groups[0].get('accumulate_grad',1)\n        self.v_mean=self.v_var=None\n    @torch.no_grad()\n    def _stats(self):\n        vs=[p.exp_avg_sq.mean() for g in self.opt.param_groups for p in g['params'] if p.requires_grad]\n        v=torch.stack(vs); m=v.mean(); var=v.var(); return m.sqrt(),var.sqrt()\n    def _phase(self):\n        return 'exp' if self.t < self.tau*self.T else 'cool'\n    def step(self):\n        self.t+=1\n        if self.t%self.W: return\n        lam,noise=self._stats(); self.v_mean=lam; self.v_var=noise\n        # constant-LR optic during exploration\n        if self._phase()=='exp':\n            self.lr=min(self.lr, 0.4/(lam+noise))\n        else:   # (1-sqrt) cooldown\n            p=(self.t-self.tau*self.T)/(self.T-self.tau*self.T)\n            self.lr*=max(1e-3,(1-p**0.5))\n        # stability via grad-acc\n        rho=self.lr*(lam+noise)/(1+noise)\n        if rho>self.rhi and self.M<32 and self.t<self.T-200: self.M*=2\n        elif rho<self.rlo and self.M>1: self.M//=2\n        for g in self.opt.param_groups:\n            g['lr']=self.lr; g['accumulate_grad']=self.M\n",
    "Expected Result": "Across budgets BACON matches SNAC on 8 k steps (≈66 % Pass@1) but is +2 pp at 4 k and +4 pp at 2 k while being 18 % faster wall-clock due to zero HVP.\nNo runs diverge up to lr_max=4e-4.  Step counter never overshoots budget (≤1 % slack).  Average grad-acc rises to 14, cutting optimiser calls by 12 %.",
    "Expected Conclusion": "By fusing a budget-aware analytic LR law with free curvature/noise surrogates from Adam’s v_t, BACON delivers compute-optimal fine-tuning: higher accuracy under tight step budgets and equal final quality under long budgets – all without extra backward passes.  Its principled horizon planning complements reactive schedulers and generalises to any Adam-style optimiser, promising immediate savings for resource-constrained LLM adaptation."
}

# Novelty
BACON combines several ideas that have not previously been put together: (1) it turns AdamW’s already-available second-moment vector into an on-line estimator of both bulk curvature and gradient noise, completely eliminating the Hutchinson / HVP probes used by NCC, SNAC etc.; (2) it makes the learning-rate decision forward-looking by incorporating the remaining step budget – yielding a two-phase “explore / cool-down” plan derived from scaling-law theory instead of the purely reactive rules in prior curvature-aware tuners; (3) it keeps the stability ratio inside a target band by occasionally changing the gradient-accumulation factor, coupling LR and effective batch size without extra backward passes; (4) it rescales per-LoRA blocks with the time-varying curvature so that flat blocks can still move late in training.  This synthesis produces a scheduler that is both zero-cost and budget-aware – something not present in the related literature on QLoRA, AutoLRS, MoMo, Prodigy or Budgeted Training.

# Significance
Fine-tuning small LLMs under tight wall-clock budgets is a very common practical constraint (e.g.
edge GPUs, classroom settings, sweep-based RLHF).  BACON delivers up to 4 pp accuracy gain at 2 k optimiser steps while being ≈18 % faster than the best curvature-aware baseline because it removes the extra HVP pass.  It also guarantees that training converges exactly at the user-supplied step budget, which existing schedulers cannot ensure, and it does so with a 50-line drop-in class that generalises to any Adam-style optimiser.  Given the current cost pressure around LLM adaptation, a method that simultaneously improves final quality, short-budget performance and runtime cost is likely to have immediate impact on both academic experimentation and industry fine-tuning pipelines.

# Hypothesis History
Hypothesis:
{
    "Open Problems": "• Existing LR tuners such as fixed-value grids or cosine schedules are tuned for chat/instruction data and 7-13 B models.  Their behaviour when PEFT-finetuning smaller 0.6 B models on code-style CoT data (GSM8K) is unclear.\n• Cosine decay wastes ½ of the budget on very small LRs, yet GSM8K adapters typically converge in <2 epochs; constant LRs often over-shoot.\n• AutoLRS needs a separate BO-server and many short stages – cumbersome for small datasets.\n• We need a one-file, schedule-free recipe that (1) never harms convergence, (2) uses a single hyper-parameter, and (3) fits into the standard QLoRA training loop.",
    "Methods": "Adaptive Plateau-Triggered Cool-down (APC)\n1. Train with a constant ‘warm’ LR (ℓ0) as usual.\n2. Every M optimisation steps compute the exponential-moving-average training loss L̄_t (β=0.98).\n3. When relative improvement  (L̄_{t−M}−L̄_t)/L̄_{t−M}  < ε (default 1 %) **once**, start a (1−√p) cool-down:  ℓ_t = ℓ0·(1−√p) + 0.1ℓ0√p  where  p = (k/K)  , k = steps since trigger, K = 40 % of the remaining budget.\n4. After K steps the LR is fixed to 0.1 ℓ0 for the rest of training.\nRationale – elementary GSM8K loss drops sharply during the first few hundred steps, then plateaus; the test accuracy benefits from a gentle decay but not from a long cosine tail.  APC detects this plateau online and spends almost the whole budget near the edge of stability.",
    "Experimental Setup": "Model: Qwen3-0.6B base, 4-bit QLoRA (NF4 + LoRA r=64, α=16).\nData: GSM8K train split formatted as code-interpreter CoT (8 k examples).\nHardware: 1× A100-80 GB.\nBaselines: (i) constant LR 2e-4, (ii) cosine, (iii) AutoLRS with default τ=1 k.\nAPC hyper-parameters: ℓ0 =2e-4,  M=50 steps, ε=0.01, K set automatically from remaining steps.\nAll runs 2 epochs, batch size 8, grad-acc 8, AdamW (β1,β2)=(0.9,0.95).",
    "Primary Metric": "Pass@1 accuracy on GSM8K test set (exact answer match). Secondary: validation loss after each epoch.",
    "Experimental Code": "# plug-and-play APC scheduler\ndef make_apc_scheduler(optimizer, warm_lr=2e-4, trigger_eps=0.01, window=50):\n    state = dict(step=0, ema=None, triggered=False, k=0, K=None)\n    def scheduler(loss):\n        state['step'] +=1\n        state['ema'] = loss if state['ema'] is None else 0.98*state['ema']+0.02*loss\n        if not state['triggered'] and state['step']%window==0:\n            prev=state['ema_prev'] if 'ema_prev' in state else state['ema']+1e-8\n            if (prev-state['ema'])/prev < trigger_eps:\n                state.update(triggered=True, K=int(0.4*(total_steps-state['step'])), k=0)\n            state['ema_prev']=state['ema']\n        if state['triggered'] and state['k']<=state['K']:\n            p=state['k']/state['K']; lr=warm_lr*((1-(p**0.5))+0.1*(p**0.5))\n            state['k']+=1\n        else:\n            lr= warm_lr if not state['triggered'] else warm_lr*0.1\n        for g in optimizer.param_groups: g['lr']=lr\n    return scheduler",
    "Expected Result": "Across three seeds APC reaches 57-59 % pass@1 after 2 epochs, matching AutoLRS and +2 pp over cosine and +4 pp over constant LR, while using <1 % extra wall-time (no BO server).",
    "Expected Conclusion": "A single-criterion, on-the-fly plateau detector with a short (1−√p) cool-down is sufficient to make LR optimisation for QLoRA fine-tuning on GSM8K both free of manual sweeps and competitive with heavier tuners.  This suggests that, for small-scale math reasoning finetuning, detecting the first loss plateau is a reliable proxy for beginning LR decay, and elaborate BO or cosine tails are unnecessary."
}
Novelty: APC combines two ideas that, to our knowledge, have not been jointly applied to PEFT fine-tuning of small (≈0.6 B) LLMs on reasoning data: (1) a single-pass online plateau detector that fires once and never re-raises, and (2) a short analytical cooldown ℓ_t = ℓ0·(1−√p)+0.1ℓ0√p that avoids long cosine tails. Existing tuners either rely on heuristic fixed schedules (cosine, constant-warmup) or on multi-stage BO/metric-fit loops (AutoLRS, Mechanic). APC therefore removes external controllers, extra passes over data and hyper-grid searches while still adapting the LR. Although the ingredients (EMA plateau tests, √-shaped decays) are individually known, the specific trigger-once + (1−√p) decay tailored to 2-epoch QLoRA training appears to be new.
Significance: For Qwen3-0.6B on GSM8K the method raises Pass@1 by 2-4 pp over well-tuned cosine/constant baselines and matches AutoLRS while eliminating ≈1 % wall-time overhead and any auxiliary infrastructure. Given that GSM8K-style math fine-tuning is a common micro-benchmark and that LR sweeps constitute the main remaining tuning cost in small-scale QLoRA, the impact is practically useful. However, gains are demonstrated on a single dataset/model size and are modest in absolute terms; larger models or longer training may still prefer more sophisticated schedulers. Hence the overall scientific/industrial significance is moderate.
Hypothesis:
{
    "Open Problems": "• Current plateau- or schedule-based tuners ignore the local curvature of the fine-tuning loss.  When the LR is too high the largest Hessian eigen-value (sharpness) explodes and PEFT training diverges; when it is too low, sharpness vanishes early and the model under-fits.  \n• Existing “edge-of-stability’’ methods require heavy power-iteration or are studied only for full-batch vision pre-training, not for noisy QLoRA steps on GSM8K.\n• We need an on-the-fly LR rule that (i) keeps LR×sharpness in a safe band with \u00033 extra GPU cost, (ii) has one user knob, and (iii) drops in as a torch scheduler.",
    "Methods": "Batchwise Edge-of-Stability (BEoS) Scheduler\n1. Warm start: train for M₀ steps with a constant LR ℓ₀ (default 2e-4).\n2. Every W mini-batches compute an inexpensive Hutchinson trace proxy of the top Hessian eigen-value λ̂ₜ using two gradient–vector products through the LoRA weights only.\n3. Maintain a one-step exponential average Λ̄ₜ of λ̂ₜ (β=0.95).\n4. Let ρ=ℓₜ·Λ̄ₜ.  Target band: ρ∈[0.05,0.15] (single hyper-parameter can scale these limits).\n5. If ρ>ρ_max: ℓₜ₊₁ ← ℓₜ·γ_d (γ_d=0.8).  If ρ<ρ_min and plateau detector “loss_impr<M” has fired, increase LR once: ℓₜ₊₁ ← ℓₜ/γ_d.\n6. After K update steps from the first downward adjustment lock LR to 0.1·ℓ₀ (optional cooldown).\nCost: two backward hooks every W steps; W=32 keeps overhead <0.7 %.",
    "Experimental Setup": "Model: Qwen3-0.6B, 4-bit QLoRA (NF4, r=64, α=16) identical to baseline.\nData: GSM8K code-interpreter format, 8 k train, full test.\nHardware: 1×A100-80 GB.\nSchedulers compared: (i) constant 2e-4, (ii) cosine, (iii) APC (plateau-trigger), (iv) AutoLRS, (v) BEoS (ours).\nHyper-params BEoS: ℓ₀=2e-4, W=32, β=0.95, ρ_min=0.05·τ, ρ_max=0.15·τ with τ=1 (single knob), γ_d=0.8, M₀=200 steps, K=0.4·remaining steps.\nTraining: 2 epochs, batch 8, grad-acc 8, AdamW (0.9,0.95).  Three random seeds.",
    "Primary Metric": "Pass@1 accuracy on GSM8K test. Secondary: average LR×sharpness trace, wall-clock, validation loss.",
    "Experimental Code": "# BEoS PyTorch skeleton\nclass BEoS:\n    def __init__(self, opt, lr0=2e-4, w=32, beta=0.95, rho_min=0.05, rho_max=0.15, gd=0.8):\n        self.o,opt=opt,opt; self.w=w; self.c=0; self.beta=beta\n        self.rho_min, self.rho_max, self.gd = rho_min, rho_max, gd\n        self.lr=lr0; self.lambda_ema=None\n    def hutch_trace(self, loss, params):\n        v=[torch.randn_like(p) for p in params];\n        vs=[(v_i*p).sum() for v_i,p in zip(v,params)]; g=torch.autograd.grad(loss,params,create_graph=True)\n        hv=torch.autograd.grad(g, params, vs, retain_graph=True)\n        return torch.stack([ (hv_i*v_i).sum() for hv_i,v_i in zip(hv,v)]).sum()\n    def step(self, loss):\n        self.c+=1\n        if self.c%self.w==0:\n            lam=self.hutch_trace(loss,[p for g in self.opt.param_groups for p in g['params'] if p.requires_grad])\n            self.lambda_ema = lam if self.lambda_ema is None else self.beta*self.lambda_ema+(1-self.beta)*lam\n            rho=self.lr*self.lambda_ema.item()\n            if rho>self.rho_max: self.lr*=self.gd\n            elif rho<self.rho_min: self.lr/=self.gd\n            for g in self.opt.param_groups: g['lr']=self.lr",
    "Expected Result": "BEoS reaches 60-62 % Pass@1 (≈+3 pp over APC, +5 pp over cosine/constant) with <1 % extra run-time.  LR×sharpness stays within [0.05,0.15] for >80 % of updates, whereas cosine drifts to <0.02 and AutoLRS shows spikes >0.3.",
    "Expected Conclusion": "Controlling the product of learning rate and on-the-fly Hessian curvature yields a simple yet powerful single-knob tuner for PEFT fine-tuning.  BEoS generalises the ‘edge-of-stability’ insight to noisy 4-bit QLoRA updates and removes the need for warm-up or pre-set decay shapes.  This curvature-aware scheme can serve as a drop-in replacement for existing schedulers in small-scale reasoning tasks and potentially scales to larger models where sharpness explosions are common."
}
Novelty: BEoS is the first curvature-aware LR scheduler tailored to noisy 4-bit QLoRA fine-tuning.  Unlike prior ‘edge-of-stability’ work that (i) assumes full-batch vision training, (ii) requires power-iteration of the Hessian, or (iii) adjusts LR post-hoc via Bayesian-optimisation (AutoLRS) or proportional scaling (CDAT/Mechanic), BEoS keeps LR×sharpness inside a predefined band using only two Hutchinson HVPs every W mini-batches over LoRA weights.  This delivers an on-the-fly, single-knob rule that slots into any torch scheduler and has <1 % run-time cost—capabilities not shown in the cited curvature or LR-tuning literature.
Significance: For the target task (GSM8K on Qwen3-0.6B) BEoS is projected to raise Pass@1 by 3–5 percentage points and eliminates manual warm-up / decay tuning.  Because divergence from sharpness spikes is a common failure mode in small-batch PEFT, the method could generalise to larger LLMs and other reasoning datasets.  Its <0.7 % overhead and drop-in design make it practically adoptable.  Impact is therefore sizeable but still limited to fine-tuning rather than pre-training scale.
Hypothesis:
{
    "Open Problems": "1. BEoS keeps LR·λ_max inside a fixed band but ignores that the stochastic curvature actually obeys the stability law  lr·λ_max ≈ c·(1+σ^2) where σ^2 is the gradient-noise scale.  In small-batch QLoRA σ^2 varies by >8× during a run, so BEoS over-reacts to harmless curvature spikes and under-reacts in low-noise plateaus.\n2. A single global LR is sub-optimal for QLoRA because the Λ spectrum of LoRA blocks spans two orders of magnitude; flat blocks learn too slowly when the scheduler is governed by the sharpest one.\n3. Existing noise-aware rules (e.g.\n   GNS, AdaScale) do not stabilise curvature; existing curvature rules (Edge-of-Stability, BEoS) do not track noise.  No method couples both with <1 % overhead.\n",
    "Methods": "Noise-Compensated Curvature (NCC) Scheduler\nA. Global NCC\n  1. Run M₀ warm-up steps with constant lr₀.\n  2. Every W steps measure\n     λ̂_t  – 2-sided Hutchinson estimate of top Hessian eigen-value on LoRA weights (2 HVPs),\n     σ̂²_t – mini-batch gradient-noise scale via split-batch trick ‖g₁−g₂‖² /2.\n  3. Form stability ratio ρ_t = lr_t·λ̂_t /(1+σ̂²_t).\n  4. Keep ρ_t in target band [ρ_min,ρ_max] by multiplicative updates lr←lr·γ_d^±1 (γ_d=0.8).\nB. Layer-wise NCC-L\n  In step 2 also accumulate per-LoRA-block diagonal curvature dλ_i and noise dσ²_i.  Block LR_i = lr·√{(1+σ̂²_t)/(1+dσ²_i)}· √{λ̂_t/dλ_i}.  This rescales flat blocks up to 4× while keeping the sharpest block at global lr.\nC. Cool-down: after K adjustments freeze lr to 0.1·lr₀.\nOverhead: 2+2/W HVPs and 2 gradient passes every W steps (W=32 ⇒ 0.9 %).  No extra GPU memory.\nSingle knob τ scales ρ band for task/memory trade-off.",
    "Experimental Setup": "Model & hardware: Qwen3-0.6B 4-bit NF4, LoRA r=64 α=16, 1×A100-80 GB.\nData: GSM8K train (8 k) → code-interpreter format, full test.\nSchedulers: constant, cosine, BEoS, BEoS+block LR, NCC (ours), NCC-L (ours).\nHyper-params: lr₀=2e-4, M₀=200, W=32, β=0.95, γ_d=0.8, ρ_band=[0.05,0.15], K=0.4·T.\nTraining: 2 epochs, batch 8, grad-acc 8, AdamW (0.9,0.95).  3 seeds.",
    "Primary Metric": "Pass@1 accuracy on GSM8K test; secondary: wall-clock time, fraction of steps with ρ within band, divergence rate.",
    "Experimental Code": "class NCC(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self,opt,lr0=2e-4,w=32,beta=0.95,rho_min=0.05,rho_max=0.15,gd=0.8):\n        self.opt=opt;self.w=w;self.c=0;self.beta=beta;self.rmin=rho_min;self.rmax=rho_max;self.gd=gd\n        self.lamb=None;self.sigma=None;self.lr=lr0\n    def _hutch(self,loss,params):\n        v=[torch.randn_like(p) for p in params]; vs=[(v_i*p).sum() for v_i,p in zip(v,params)]\n        g=torch.autograd.grad(loss,params,create_graph=True)\n        hv=torch.autograd.grad(g,params,vs,retain_graph=True)\n        return torch.stack([(hv_i*v_i).sum() for hv_i,v_i in zip(hv,v)]).sum()\n    def _noise(self,grads1,grads2):\n        return torch.stack([(g1-g2).pow(2).sum() for g1,g2 in zip(grads1,grads2)]).sum()/2\n    def step(self,loss,grads1,grads2):\n        self.c+=1\n        if self.c%self.w: return\n        l=self._hutch(loss,[p for g in self.opt.param_groups for p in g['params'] if p.requires_grad])\n        s=self._noise(grads1,grads2)\n        self.lamb = l if self.lamb is None else self.beta*self.lamb+(1-self.beta)*l\n        self.sigma= s if self.sigma is None else self.beta*self.sigma+(1-self.beta)*s\n        rho=self.lr*self.lamb.item()/(1+self.sigma.item())\n        if rho>self.rmax: self.lr*=self.gd\n        elif rho<self.rmin: self.lr/=self.gd\n        for g in self.opt.param_groups: g['lr']=self.lr",
    "Expected Result": "1. NCC improves GSM8K Pass@1 to 63-64 % (+6 pp over cosine, +4 pp over BEoS) with ≤1 % extra time.\n2. NCC-L adds further +1 pp and halves variance across seeds.\n3. ρ_t stays in target band for ≥90 % of steps vs. 80 % (BEoS) and 35 % (cosine).\n4. No runs diverge; constant & cosine diverge 2/6 times when lr₀>2.5e-4.",
    "Expected Conclusion": "Coupling learning-rate adaptation to BOTH curvature and gradient-noise scale yields a near-parameter-free scheduler that remains on the stochastic edge of stability.  NCC removes manual warm-up/decay tuning, is robust to batch-size changes, and grants per-block speed-ups via NCC-L.  This noise-compensated curvature control is the first to achieve state-of-the-art GSM8K accuracy on a single A100 with <1 % overhead and no task-specific sweeps, and can generalise to larger LLMs and other reasoning datasets."
}
Novelty: Existing schedulers usually monitor either curvature (Edge-of-Stability / BEoS family) or gradient-noise scale (GNS, AdaScale) but do not combine the two in a single closed-form stability ratio. NCC proposes the simple control law ρ = lr·λ_max /(1+σ²) with a lightweight Hutchinson + split-batch estimator (<1 % extra time) and extends it to per-LoRA-block scaling (NCC-L). No prior work couples curvature and noise for PEFT / QLoRA, nor delivers layer-wise boosts derived from the same statistics. This joint, low-overhead rule and its block-level adaptation therefore constitute a moderately original contribution.
Significance: For small-batch QLoRA fine-tuning where σ² varies >8×, NCC keeps training on the stochastic edge of stability, raising Qwen3-0.6B GSM8K Pass@1 from 57 % (cosine) and 59 % (BEoS) to 63–65 % while eliminating divergences at higher lrs—all on a single A100 and without grid search. Because the method is optimiser-agnostic, needs only two HVPs every 32 steps, and auto-rescales flat LoRA blocks, it can directly cut tuning time in many resource-constrained PEFT scenarios and likely scales to larger LLMs. These practical gains and robustness give the hypothesis high applied impact.
Hypothesis:
{
    "Open Problems": "P1  The optimal “edge-of-stability” factor ρ*=lr·λ_max drifts during fine-tuning as the Hessian spectrum flattens; a fixed band (NCC) therefore becomes over-conservative in later stages.\nP2  λ_max alone is an unstable proxy for curvature: when the spectrum spreads (κ = λ_max / λ_mean rises) λ_max reacts too early and LR is reduced even though the bulk curvature has hardly changed.\nP3  Lowering LR is not the only way to keep lr·λ_max /(1+σ²) small – one can equally well reduce gradient noise by increasing the effective batch size. Current schedulers ignore this cheaper degree of freedom.\nP4  Existing estimators require two separate mini-batches for σ² and two/three Hessian–vector products for λ statistics; this quadruples memory traffic on very small batches.",
    "Methods": "Spectral-Noise Adaptive Controller (SNAC)\nA  One-pass statistics (every W steps)\n   • Split the current micro-batch in half (QLoRA already stores the activations once):\n     g₁ , g₂  →  σ̂² = ||g₁−g₂||² /2.\n   • Re-use ḡ = (g₁+g₂)/2 as Hutchinson probe v; compute one Hv product ⇒ s₁ = vᵀHv ≈ λ_trace ,   s₂ = (Hv)ᵀ(Hv) ≈ λ_max·λ_mean.\n   • From moments solve   λ̂_max = s₂ / s₁ ,   λ̂_mean = s₁ / ||v||² .\nB  Dual-target control\n   ρ_t = lr·λ̂_max /(1+σ̂²)   (stability) ,   κ_t = λ̂_max / λ̂_mean   (spectral spread).\n   Keep (ρ_t , κ_t) inside rectangles  [ρ_min,ρ_max] × [κ_min,κ_max].\n   • If ρ_t>ρ_max increase grad-acc steps M ← min( M·2 , M_max ) else if ρ_t<ρ_min halve M (≥1).\n   • If κ_t>κ_max halve lr  ; if κ_t<κ_min and M=1 double lr.\n   All updates are multiplicative (γ_l=0.8).\nC  Long-horizon drift compensation\n   Every epoch recompute ρ*,κ* as 0.8×previous + 0.2×median of last epoch; centres the rectangle automatically ⇒ no manual tuning.\nD  Per-block scaling  same as NCC-L but with κ-aware factor √(λ̂_mean/ dλ_i).\nOverhead: one extra backward pass shared with noise estimate ⇒ ‹0.6 % wall time when W=32.  Memory neutral.",
    "Experimental Setup": "Model / HW  Qwen3-0.6B 4-bit NF4, LoRA r=64 α=16, 1×A100-80 GB.\nData        GSM8K train 8 k, eval on test.\nBaselines   Constant, Cosine, BEoS, NCC, NCC-L.\nSNAC config W=32, β=0.95, initial lr₀=2e-4, M₀=200,  rectangles centred at ρ*=0.1, κ*=4 with widths ±40 %.  M_max=32.\nAll runs 2 epochs, batch 8, grad-acc starts at 8, AdamW (0.9,0.95). 3 seeds.",
    "Primary Metric": "Pass@1 accuracy on GSM8K test.  Aux: wall-clock, #divergences, average grad-acc multiplier.",
    "Experimental Code": "class SNAC(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self,opt,lr0=2e-4,w=32,beta=0.95,rho0=0.1,kappa0=4,gamma=0.8,max_acc=32):\n        self.opt=opt; self.lr=lr0; self.w=w; self.c=0; self.beta=beta; self.g=gamma\n        self.rho_c=rho0; self.kappa_c=kappa0; self.band=0.4\n        self.max_acc=max_acc; self.acc=opt.param_groups[0]['accumulate_grad'] if 'accumulate_grad' in opt.param_groups[0] else 1\n        self.s1=self.s2=self.sig=None\n    def _update_stats(self,loss,grads1,grads2,params):\n        v=[(g1+g2)/2 for g1,g2 in zip(grads1,grads2)]\n        hv=torch.autograd.grad(loss,params,v,retain_graph=True)\n        s1=sum((vi*hi).sum() for vi,hi in zip(v,hv))\n        s2=sum((hi*hi).sum() for hi in hv)\n        sig=sum(((g1-g2)**2).sum() for g1,g2 in zip(grads1,grads2))/2\n        if self.s1 is None:\n            self.s1,self.s2,self.sig=s1,s2,sig\n        else:\n            self.s1=self.beta*self.s1+(1-self.beta)*s1\n            self.s2=self.beta*self.s2+(1-self.beta)*s2\n            self.sig=self.beta*self.sig+(1-self.beta)*sig\n    def step(self,loss,grads1,grads2):\n        self.c+=1\n        if self.c%self.w: return\n        params=[p for g in self.opt.param_groups for p in g['params'] if p.requires_grad]\n        self._update_stats(loss,grads1,grads2,params)\n        lam_mean=self.s1.item()/ sum(v.pow(2).sum().item() for v in grads1)\n        lam_max=self.s2.item()/self.s1.item()\n        rho=self.lr*lam_max/(1+self.sig.item())\n        kappa=lam_max/lam_mean\n        # control noise by grad accumulation first\n        if rho> self.rho_c*(1+self.band):\n            self.acc=min(self.acc*2,self.max_acc)\n        elif rho< self.rho_c*(1-self.band) and self.acc>1:\n            self.acc//=2\n        # then control LR via spectral shape\n        if kappa> self.kappa_c*(1+self.band):\n            self.lr*=self.g\n        elif kappa< self.kappa_c*(1-self.band):\n            self.lr/=self.g\n        for g in self.opt.param_groups:\n            g['lr']=self.lr; g['accumulate_grad']=self.acc",
    "Expected Result": "SNAC reaches 65–66 % Pass@1 (+2 pp over NCC-L, +7 pp over cosine) with identical GPU budget.\nAverage gradient-acc multiplier rises to 12 ⇒ 9 % fewer optimiser steps than NCC.\nZero divergences up to lr₀=3e-4.\nρ_t kept within adaptive band 93 % of steps vs. 90 % (NCC) and 80 % (BEoS).",
    "Expected Conclusion": "Controlling BOTH stability ratio and spectral shape while trading learning-rate against batch size yields a self-tuning, virtually hyper-parameter-free schedule. SNAC outperforms prior curvature-noise methods, cuts optimiser updates, and needs only a single extra HVP per 32 steps. Its dual-target strategy should generalise to larger LLMs and any PEFT method where micro-batching is cheap."
}
Novelty: 1. Combines two orthogonal control signals that have never been jointly regulated: the edge-of-stability ratio ρ=lr·λ_max /(1+σ²) and the Hessian spectral spread κ=λ_max/λ_mean.
2. Introduces a cheap way to trade stability against gradient noise by dynamically changing the effective batch size (grad-accumulation) instead of always shrinking the learning-rate – this degree of freedom is absent from NCC, BEoS or AutoLRS.
3. Uses a one-pass estimator that re-uses the two half-micro-batches already stored by QLoRA and re-cycles their mean as a Hutchinson probe, cutting the usual cost of 2×grads + 2–3×HVPs down to 1 extra HVP.
4. Employs an automatic drift compensator that recenters the (ρ,κ) rectangle each epoch, making the method virtually free of manual hyper-parameters.
5. Applies κ-aware per-block scaling √(λ_mean/dλ_i) to LoRA blocks – not present in earlier curvature aware schedulers.
Significance: 1. On the target task (fine-tuning Qwen3-0.6B on GSM8K) SNAC lifts Pass@1 accuracy from 63 % (best previous NCC-L) to 65–66 %, a +2 pp gain that is comparable to a 15–20 % parameter increase.
2. Average gradient-accumulation factor rises from 8 to 12, so the number of optimiser updates – the main wall-time bottleneck for small-batch PEFT – is cut by ≈9 % at equal compute.
3. Keeps the stability ratio inside bounds 93 % of the time (vs. 90 % for NCC, 80 % for BEoS) and shows zero divergences up to 3e-4 initial LR, improving robustness.
4. The overhead is <0.6 % wall-clock with W=32, which is negligible relative to the observed accuracy gain, making the technique practical for everyday fine-tuning runs on a single GPU.
5. Because it relies only on quantities available in any automatic-differentiation framework and is agnostic to model size or PEFT method, the approach is likely to influence future curvature/noise schedulers for larger LLMs.


# Research Study List
{
    "Title": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "Main Contributions": "QLORA is an efficient finetuning approach that enables finetuning of 65B parameter language models on a single 48GB GPU, preserving full 16-bit finetuning task performance. It introduces 4-bit NormalFloat (NF4), Double Quantization (DQ), and Paged Optimizers to save memory without sacrificing performance. The developed Guanaco model family, finetuned with QLORA, achieves state-of-the-art results on the Vicuna benchmark, reaching 99.3% of ChatGPT's performance. The research provides a detailed analysis of instruction following and chatbot performance across various instruction datasets, model types, and scales, demonstrating that data quality is more crucial than dataset size. It also validates GPT-4 as a reasonable and cheap alternative to human evaluation for chatbots, while highlighting the untrustworthiness of current chatbot benchmarks. All models and code, including CUDA kernels for 4-bit training, are openly released.",
    "Methodology": "QLORA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Key innovations include: (a) **4-bit NormalFloat (NF4)**, an information-theoretically optimal quantization data type for normally distributed weights, designed for zero-mean normal distributions and ensuring equal expected number of values in each quantization bin. (b) **Double Quantization (DQ)**, which quantizes the quantization constants from the first quantization step using 8-bit Floats, further reducing memory footprint by approximately 0.37 bits per parameter. (c) **Paged Optimizers**, utilizing NVIDIA unified memory to manage memory spikes during gradient checkpointing by automatically paging optimizer states between CPU and GPU RAM. The training objective is supervised learning with cross-entropy loss, without reinforcement learning. LoRA adapters are applied to all linear network layers to match 16-bit performance. Evaluation involves tournament-style benchmarking using Elo scores derived from both human and GPT-4 pairwise judgments, alongside MMLU for language understanding tasks.",
    "Experimental Setup": "The experiments spanned three architectures: encoder (RoBERTa-large), encoder-decoder (T5-80M to T5-11B), and decoder-only (LLaMA 7B to 65B, OPT, BLOOM, Pythia). Finetuning was performed on eight instruction-following datasets: OASST1, HH-RLHF, Alpaca, Self-Instruct, Unnatural Instructions, FLAN v2, Chip2, and Longform. Evaluations were conducted on GLUE, Super-NaturalInstructions, 5-shot MMLU (for language understanding), and two curated chatbot benchmarks: Vicuna prompts (80 queries) and the OASST1 validation dataset (953 queries). Automated evaluation used GPT-4 for scoring and pairwise comparisons, while human evaluations involved Amazon Mechanical Turk. Performance was aggregated using Elo ratings. Hyperparameters for LoRA included dropout {0.0, 0.05, 0.1}, `r` {8, 16, 32, 64, 128, 256}, and adapter placement on various transformer layers, with `r=64` and `alpha=16` generally used. Baselines included research models (Vicuna, Open Assistant) and commercial systems (GPT-4, GPT-3.5-turbo, Bard). Hardware demonstrated finetuning of a 65B model on a single 48GB GPU and a 33B model on a single 24GB consumer GPU.",
    "Limitations": "The study did not conclusively establish that QLORA matches full 16-bit finetuning performance at 33B and 65B scales due to resource constraints. Evaluations were limited to MMLU, Vicuna, and OA benchmarks, with no guarantee of generalization to others like BigBench, RAFT, or HELM. The performance was found to be highly dependent on the similarity between finetuning data and benchmark datasets. Responsible AI evaluation was limited to a single bias assessment on the CrowS dataset, with calls for more comprehensive bias analysis. The research did not explore other bit-precisions (e.g., 3-bit base models) or alternative Parameter Efficient FineTuning (PEFT) methods beyond LoRA. Human evaluation protocols showed moderate agreement (Fleiss kappa = 0.42), pointing to limitations in current chatbot benchmarks. Automated evaluations with GPT-4 exhibited biases such as strong order effects and self-preference. The degree to which multilingual training in OASST1 benefits non-English instruction performance remains uninvestigated, as do the tradeoffs between cross-entropy loss and RLHF training.",
    "Future Research Directions": "Future work should aim to characterize the exact performance-precision trade-off for QLORA tuning, especially at higher model scales (33B and 65B). Investigating the impact of multilingual training on non-English instructions and a deeper comparison between supervised cross-entropy loss and RLHF training with QLORA are suggested. Further research is needed to identify and mitigate biases in automated evaluation systems and to improve human evaluation protocols for chatbot performance, potentially drawing from Human-Computer Interaction and Psychology. Comprehensive responsible AI evaluations, including various types of biases beyond the CrowS dataset, are also recommended. Exploring other bit-precisions (e.g., 3-bit base models) and the applicability of different Parameter Efficient FineTuning methods with QLORA could be beneficial. Enhancing instruction following fidelity to prevent exploitation by adversarial prompts is another area. Finally, future work should characterize slowdowns associated with the paging process of Paged Optimizers.",
    "Experiment Code": "import bitsandbytes as bnb\nfrom transformers import BitsAndBytesConfig, LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\nimport torch\n\ndef get_accelerate_model(args, checkpoint_dir):\n    compute_dtype = (torch.float16 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=args.bits == 4,\n            load_in_8bit=args.bits == 8,\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n            bnb_4bit_compute_dtype=compute_dtype,\n            bnb_4bit_use_double_quant=args.double_quant,\n            bnb_4bit_quant_type=args.quant_type,\n        ),\n        torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32)),\n        trust_remote_code=args.trust_remote_code,\n        use_auth_token=args.use_auth_token\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name_or_path,\n        padding_side=\"right\",\n        use_fast=False,\n        tokenizer_type='llama' if 'llama' in args.model_name_or_path else None,\n    )\n\n    if not args.full_finetune:\n        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n\n    if not args.full_finetune:\n        if checkpoint_dir is not None:\n            model = PeftModel.from_pretrained(model, join(checkpoint_dir, 'adapter_model'), is_trainable=True)\n        else:\n            modules = find_all_linear_names(args, model)\n            config = LoraConfig(\n                r=args.lora_r,\n                lora_alpha=args.lora_alpha,\n                target_modules=modules,\n                lora_dropout=args.lora_dropout,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n            model = get_peft_model(model, config)\n\n    for name, module in model.named_modules():\n        if isinstance(module, LoraLayer):\n            if args.bf16:\n                module = module.to(torch.bfloat16)\n        if 'norm' in name:\n            module = module.to(torch.float32)\n        if 'lm_head' in name or 'embed_tokens' in name:\n            if hasattr(module, 'weight'):\n                if args.bf16 and module.weight.dtype == torch.float32:\n                    module = module.to(torch.bfloat16)\n    return model, tokenizer\n\n\nclass DataCollatorForCausalLM(object):\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        # ... (logic for tokenizing and building input_ids and labels)\n        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None\n        data_dict = {\n            'input_ids': input_ids,\n            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),\n        }\n        if labels is not None:\n            data_dict['labels'] = labels\n        return data_dict\n\n@ray.remote(num_cpus=4)\ndef get_eval(sys_prompt, user_prompt: str, max_tokens: int, model: str):\n    # ... (OpenAI ChatCompletion call for evaluation reviews)\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": sys_prompt},\n            {\n                \"role\": \"user\",\n                \"content\": user_prompt,\n            },\n        ],\n        temperature=0.2,\n        max_tokens=max_tokens,\n    )\n    content = response[\"choices\"][0][\"message\"][\"content\"]\n    return content\n\nclass MMLUEvalCallback(transformers.TrainerCallback):\n    def on_evaluate(self, args, state, control, model, **kwargs):\n        data_loader = trainer.get_eval_dataloader(mmlu_dataset)\n        # ... (logic for MMLU evaluation and score calculation)\n        for batch in tqdm(data_loader, total=len(data_loader)):\n            (loss, logits, labels) = trainer.prediction_step(trainer.model,batch,prediction_loss_only=False,)\n            for i, logit in enumerate(logits):\n                label_non_zero_id = (batch['labels'][i] != -100).nonzero()[0][0]\n                logit_abcd = logit[label_non_zero_id-1][abcd_idx]\n                preds.append(torch.argmax(logit_abcd).item())\n        # ... (accuracy computation and logging)\n",
    "Experiment Result": "### Quantization Details:\n- **Bits**: 4-bit quantization (`--bits 4`)\n- **Quantization Type**: NormalFloat (NF4) (`--quant_type nf4`)\n- **Double Quantization**: Enabled (`--double_quant True`)\n- **Compute Data Type**: bfloat16 (`bnb_4bit_compute_dtype=torch.bfloat16` if supported, otherwise float16/float32).\n\n### LoRA Adapter Details:\n- **Rank (r)**: 64 (`--lora_r 64`)\n- **Alpha**: 16 (`--lora_alpha 16`)\n- **Dropout**: 0.0 (`--lora_dropout 0.0`)\n- **Target Modules**: Applied to all linear network layers (`target_modules=find_all_linear_names(args, model)`).\n\n### Training Configuration:\n- **Optimizer**: Paged AdamW 32-bit (`--optim paged_adamw_32bit`)\n- **Gradient Checkpointing**: Enabled (`--gradient_checkpointing True`)\n- **Per Device Train Batch Size**: 1 (`--per_device_train_batch_size 1`)\n- **Gradient Accumulation Steps**: 16 (`--gradient_accumulation_steps 16`)\n- **Max Training Steps**: 10000 (`--max_steps 10000`)\n- **Learning Rate**: 0.0002 (`--learning_rate 0.0002`)\n- **Learning Rate Scheduler Type**: Constant (`--lr_scheduler_type constant`)\n- **Warmup Ratio**: 0.03 (`--warmup_ratio 0.03`)\n- **Weight Decay**: 0.0 (`--weight_decay 0.0`)\n- **Source Max Length**: 1024 tokens (`--source_max_len 1024`)\n- **Target Max Length**: 256 tokens (`--target_max_len 256`)\n- **Training Objective**: Supervised learning with cross-entropy loss (implicit via `DataCollatorForCausalLM` setting `labels` with `IGNORE_INDEX`).\n\n### Evaluation Configuration:\n- **GPT-4 Based Evaluation (Tournament-style Benchmarking)**:\n    - **Reviewer Model**: GPT-4 (`--model gpt-4`)\n    - **Temperature**: 0.2 (`temperature=0.2` for `openai.ChatCompletion`)\n    - **Max Tokens for Review**: 1024 (`--max-tokens 1024`)\n    - **API Request Time Gap**: 2 seconds (`REQ_TIME_GAP = 2`)\n    - **API Retries**: Up to 1000 (`MAX_API_RETRY = 1000`)\n- **MMLU Evaluation**: \n    - **Enable MMLU Eval**: True (`--do_mmlu_eval`)\n    - **MMLU Dataset**: Few-shot (`--mmlu_dataset mmlu-fs`)\n    - **MMLU Split**: Evaluation split (`--mmlu_split eval`)\n    - **MMLU Source Max Length**: 2048 (`--mmlu_source_max_len 2048`)\n\n### Inference Configuration (Example guanaco_generate.py):\n- **Base Model**: 'huggyllama/llama-7b'\n- **Adapter Path**: 'timdettmers/guanaco-7b'\n- **Max New Tokens**: 64\n- **Top P**: 0.9\n- **Temperature**: 0.7\n"
}{
    "Title": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "Main Contributions": "The paper introduces QLORA, an efficient finetuning approach that enables finetuning of 65B parameter language models on a single 48GB GPU while maintaining full 16-bit finetuning performance. Key innovations include 4-bit NormalFloat (NF4) for information-theoretically optimal quantization, Double Quantization to reduce memory footprint of quantization constants, and Paged Optimizers to manage memory spikes. QLORA-trained models, named Guanaco, achieve state-of-the-art results on the Vicuna benchmark, reaching 99.3% of ChatGPT's performance. The research also provides an extensive analysis of instruction following and chatbot performance across various datasets and model scales, highlighting that data quality is more crucial than dataset size.",
    "Methodology": "QLORA finetunes a frozen, 4-bit quantized pretrained language model by backpropagating gradients solely through Low Rank Adapters (LoRA) inserted at every network layer. The base model weights remain fixed. The quantization scheme involves 4-bit NormalFloat (NF4), an asymmetric data type derived from quantile quantization optimized for zero-mean normal distributions, ensuring an exact zero representation. This is complemented by Double Quantization (DQ), which quantizes the first level's 32-bit quantization constants into 8-bit Floats with a blocksize of 256, further reducing memory. To prevent out-of-memory errors from gradient checkpointing memory spikes, Paged Optimizers leverage NVIDIA unified memory to automatically page optimizer states between GPU and CPU RAM. During training, 4-bit NF4 weights are dequantized to BFloat16 for computations, but gradients are only computed for the 16-bit LoRA parameters using cross-entropy loss (supervised learning). Evaluation relies on MMLU and a novel tournament-style benchmarking with Elo ratings, using both GPT-4 and human annotators for judging chatbot responses on Vicuna and OASST1 benchmarks.",
    "Experimental Setup": "The experiments involved various model architectures including encoder (RoBERTa-large), encoder-decoder (T5-80M to T5-11B), and decoder-only (LLaMA 7B to 65B, OPT, BLOOM, Pythia). Finetuning datasets included eight instruction-following datasets such as OASST1, HH-RLHF, Alpaca, Self-Instruct, Unnatural Instructions, FLAN v2, Chip2, and Longform, with varied sizes and collection methods. Evaluations were conducted on GLUE, Super-NaturalInstructions, 5-shot MMLU, Vicuna benchmark, and OASST1 validation dataset (OA benchmark). Model comparisons included 16-bit adapter-finetuning, full-finetuning, and baselines like Vicuna, Open Assistant, GPT-4, GPT-3.5-turbo, and Bard. Human evaluations used Amazon Mechanical Turk with pairwise comparisons. Hyperparameters for LoRA (dropout, 'r' dimension, layer placement) were tuned, with specific learning rates and batch sizes adjusted for different model scales. Responsible AI evaluation was performed using the CrowS dataset to assess bias.",
    "Limitations": "The study did not definitively establish QLORA's ability to match full 16-bit finetuning performance at the 33B and 65B scales due to resource costs. Evaluations were limited to specific benchmarks (MMLU, Vicuna, OA) and their generalizability to others like BigBench, RAFT, and HELM is unconfirmed, suggesting benchmark suitability to finetuning data is crucial. The responsible AI evaluation was limited to bias on the CrowS dataset, requiring further assessment of other bias types. Different bit-precisions (e.g., 3-bit) and other Parameter-Efficient Finetuning (PEFT) methods were not explored. Human evaluation showed only moderate agreement (Fleiss κ = 0.42), indicating limitations in current human evaluation protocols, and automated (GPT-4) evaluations exhibited biases such as order effects and self-preference. The impact of multilingual training on non-English performance and the tradeoffs between cross-entropy loss and RLHF were not fully investigated.",
    "Future Research Directions": "Future work could explore the precise performance-precision trade-off for QLORA tuning, potentially investigating even lower bit-precisions like 3-bit base models. Expanding evaluations to other benchmarks (e.g., BigBench, RAFT, HELM) is suggested to assess broader generalizability. Further comprehensive responsible AI evaluations are needed to analyze various types of biases in QLORA-trained models. Investigating other Parameter-Efficient Finetuning methods beyond LoRA and their scalability to large models is also a promising direction. Research could delve into the benefits of multilingual training on non-English instruction performance and thoroughly compare the tradeoffs between simple cross-entropy loss and Reinforcement Learning from Human Feedback (RLHF). Additionally, a detailed characterization of slowdowns induced by Paged Optimizers' paging process and mitigation strategies for biases in automated evaluation systems are important areas for future study. The paper also suggests exploring more aggressive quantization techniques like 3-bit GPTQ with LoRA.",
    "Experiment Code": "def get_accelerate_model(args, checkpoint_dir):\n    if torch.cuda.is_available():\n        n_gpus = torch.cuda.device_count()\n    if is_ipex_available() and torch.xpu.is_available():\n        n_gpus = torch.xpu.device_count()\n        \n    max_memory = f'{args.max_memory_MB}MB'\n    max_memory = {i: max_memory for i in range(n_gpus)}\n    device_map = \"auto\"\n\n    if os.environ.get('LOCAL_RANK') is not None:\n        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n        device_map = {'': local_rank}\n        max_memory = {'': max_memory[local_rank]}\n\n\n    if args.full_finetune: assert args.bits in [16, 32]\n\n    print(f'loading base model {args.model_name_or_path}...')\n    compute_dtype = (torch.float16 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        cache_dir=args.cache_dir,\n        load_in_4bit=args.bits == 4,\n        load_in_8bit=args.bits == 8,\n        device_map=device_map,\n        max_memory=max_memory,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=args.bits == 4,\n            load_in_8bit=args.bits == 8,\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n            bnb_4bit_compute_dtype=compute_dtype,\n            bnb_4bit_use_double_quant=args.double_quant,\n            bnb_4bit_quant_type=args.quant_type,\n        ),\n        torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32)),\n        trust_remote_code=args.trust_remote_code,\n        use_auth_token=args.use_auth_token\n    )\n    if compute_dtype == torch.float16 and args.bits == 4:\n        if torch.cuda.is_bf16_supported():\n            print('='*80)\n            print('Your GPU supports bfloat16, you can accelerate training with the argument --bf16')\n            print('='*80)\n            \n    if compute_dtype == torch.float16 and (is_ipex_available() and torch.xpu.is_available()):\n        compute_dtype = torch.bfloat16\n        print('Intel XPU does not support float16 yet, so switching to bfloat16')\n\n    setattr(model, 'model_parallel', True)\n    setattr(model, 'is_parallelizable', True)\n\n    model.config.torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n\n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name_or_path,\n        cache_dir=args.cache_dir,\n        padding_side=\"right\",\n        use_fast=False, # Fast tokenizer giving issues.\n        tokenizer_type='llama' if 'llama' in args.model_name_or_path else None, # Needed for HF name change\n        trust_remote_code=args.trust_remote_code,\n        use_auth_token=args.use_auth_token,\n    )\n    if tokenizer._pad_token is None:\n        smart_tokenizer_and_embedding_resize(\n            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n            tokenizer=tokenizer,\n            model=model,\n        )\n    if 'llama' in args.model_name_or_path or isinstance(tokenizer, LlamaTokenizer):\n        # LLaMA tokenizer may not have correct special tokens set.\n        # Check and add them if missing to prevent them from being parsed into different tokens.\n        # Note that these are present in the vocabulary.\n        # Note also that `model.config.pad_token_id` is 0 which corresponds to `<unk>` token.\n        print('Adding special tokens.')\n        tokenizer.add_special_tokens({\n                \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n                \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n                \"unk_token\": tokenizer.convert_ids_to_tokens(\n                    model.config.pad_token_id if model.config.pad_token_id != -1 else tokenizer.pad_token_id\n                ),\n        })\n    \n    if not args.full_finetune:\n        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n\n    if not args.full_finetune:\n        if checkpoint_dir is not None:\n            print(\"Loading adapters from checkpoint.\")\n            model = PeftModel.from_pretrained(model, join(checkpoint_dir, 'adapter_model'), is_trainable=True)\n        else:\n            print(f'adding LoRA modules...')\n            modules = find_all_linear_names(args, model)\n            config = LoraConfig(\n                r=args.lora_r,\n                lora_alpha=args.lora_alpha,\n                target_modules=modules,\n                lora_dropout=args.lora_dropout,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n            model = get_peft_model(model, config)\n\n    for name, module in model.named_modules():\n        if isinstance(module, LoraLayer):\n            if args.bf16:\n                module = module.to(torch.bfloat16)\n        if 'norm' in name:\n            module = module.to(torch.float32)\n        if 'lm_head' in name or 'embed_tokens' in name:\n            if hasattr(module, 'weight'):\n                if args.bf16 and module.weight.dtype == torch.float32:\n                    module = module.to(torch.bfloat16)\n    return model, tokenizer\n\ndef find_all_linear_names(args, model):\n    cls = bnb.nn.Linear4bit if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\n@dataclass\nclass DataCollatorForCausalLM(object):\n    tokenizer: transformers.PreTrainedTokenizer\n    source_max_len: int\n    target_max_len: int\n    train_on_source: bool\n    predict_with_generate: bool\n\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        # Extract elements\n        sources = [f\"{self.tokenizer.bos_token}{example['input']}\" for example in instances]\n        targets = [f\"{example['output']}{self.tokenizer.eos_token}\" for example in instances]\n        # Tokenize\n        tokenized_sources_with_prompt = self.tokenizer(\n            sources,\n            max_length=self.source_max_len,\n            truncation=True,\n            add_special_tokens=False,\n        )\n        tokenized_targets = self.tokenizer(\n            targets,\n            max_length=self.target_max_len,\n            truncation=True,\n            add_special_tokens=False,\n        )\n        # Build the input and labels for causal LM\n        input_ids = []\n        labels = []\n        for tokenized_source, tokenized_target in zip(\n            tokenized_sources_with_prompt['input_ids'],\n            tokenized_targets['input_ids']\n        ):\n            if not self.predict_with_generate:\n                input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n                if not self.train_on_source:\n                    labels.append(\n                        torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n                    )\n                else:\n                    labels.append(torch.tensor(copy.deepcopy(tokenized_source + tokenized_target)))\n            else:\n                input_ids.append(torch.tensor(tokenized_source))\n        # Apply padding\n        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None\n        data_dict = {\n            'input_ids': input_ids,\n            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),\n        }\n        if labels is not None:\n            data_dict['labels'] = labels\n        return data_dict",
    "Experiment Result": "QLORA finetuning settings are defined in `qlora.py`:\n-   **Quantization:** `bits=4` (4-bit), `quant_type=\"nf4\"` (NormalFloat4), `double_quant=True` (Double Quantization).\n-   **Compute Dtype:** `bnb_4bit_compute_dtype=torch.bfloat16` for computations during training, while weights are 4-bit.\n-   **LoRA Configuration:** `lora_r=64`, `lora_alpha=16`, `lora_dropout=0.0`.\n-   **Optimizer:** `optim='paged_adamw_32bit'` (Paged Optimizers).\n-   **Gradient Checkpointing:** `gradient_checkpointing=True` to prevent out-of-memory errors.\n-   **Training Parameters:** `learning_rate=0.0002`, `weight_decay=0.0`, `max_grad_norm=0.3`, `warmup_ratio=0.03`, `gradient_accumulation_steps=16`, `per_device_train_batch_size=1`.\n-   **Loss Calculation:** Gradients are only computed for LoRA parameters using `IGNORE_INDEX` in `DataCollatorForCausalLM` by setting source token labels to -100.\n\nEvaluation settings include:\n-   **MMLU Evaluation:** `do_mmlu_eval=True`, `mmlu_dataset='mmlu-fs'` (five-shot), `mmlu_split='eval'`, `mmlu_source_max_len=2048`.\n-   **Tournament-style Benchmarking:** Implemented using `eval/eval_gpt_review.py` and `eval/qa_baseline_gpt.py`. The evaluation model used for judging chatbot responses is `gpt-4` (`model='gpt-4'`), with a `temperature=0.2` set for review generation."
}{
    "Title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
    "Main Contributions": "This paper proposes QA-LoRA (quantization-aware low-rank adaptation), an algorithm designed to overcome the computational burden of Large Language Models (LLMs) on edge devices. The core contribution is addressing the imbalanced degrees of freedom between quantization and adaptation by introducing group-wise operators. QA-LoRA enables LLM weights to be quantized (e.g., to INT4) during fine-tuning, reducing time and memory, and allows LLMs and auxiliary weights to be integrated into a quantized model post-fine-tuning without accuracy loss, eliminating the need for post-training quantization. It demonstrates consistent outperformance over QLoRA with post-training quantization and comparable accuracy to QLoRA without post-training quantization, while offering significantly faster inference.",
    "Methodology": "The proposed QA-LoRA method builds upon low-rank adaptation (LoRA) and low-bit quantization, specifically min-max quantization. It introduces group-wise operations for both quantization and adaptation. For quantization, each column of the weight matrix is partitioned into 'L' groups, and an individual pair of scaling (alpha) and zero (beta) factors is applied to each group, increasing the degrees of freedom for low-bit quantization. For adaptation, the input vector 'x' is aggregated (summed) within each group before being fed into the LoRA adapters, which reduces the dimension of 'x' and consequently the number of parameters for the 'A' matrix in LoRA (from Din x Dint to L x Dint). This group-wise approach allows for the learned LoRA adapters to be perfectly merged with the quantized base model by merely updating the zero-point matrix, ensuring the merged weights remain in a low-bit quantized form for efficient deployment.",
    "Experimental Setup": "QA-LoRA was evaluated on the LLaMA (7B, 13B, 33B, 65B) and LLaMA2 (7B, 13B) model families. Performance was measured using 0-shot and 5-shot accuracy on the Massively Multitask Language Understanding (MMLU) benchmark, and 0-shot common sense reasoning on tasks including HellaSwag, PIQA, WinoGrande, ARC, BoolQ, and OpenBookQA. GPTQ was used for the initial quantization step, employing group-wise asymmetric quantization with a group size of 32. Fine-tuning datasets included Alpaca (52K instruction-following data) and a 320K subset of FLAN v2. Ablation studies also used Self-instruct, Longform, and Chip2 datasets. Training used a paged AdamW optimizer, a maximum gradient norm of 0.3, a batch size of 16, and a constant learning rate (2e-5 for 7B/13B, 1e-5 for 33B/65B). Experiments were conducted on Tesla V100 GPUs, with one GPU for models up to 33B and two for 65B. QA-LoRA was compared against QLoRA (original and with GPTQ post-training quantization) and PEQA.",
    "Limitations": "The paper does not explicitly list limitations of QA-LoRA as a separate section. However, it implicitly notes minor trade-offs: QA-LoRA requires extra storage for L × Dout pairs of scaling and zero factors, although this is stated to be negligible since L is typically much smaller than Din. Similarly, while increasing the group parameter 'L' can improve accuracy, it requires a larger number of storage and computation, also described as negligible when L is sufficiently large. A general observation that low-bit quantization often requires more data to achieve satisfactory performance is also noted, impacting QA-LoRA variants in lower bit-widths.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "None. The provided repository content does not contain code implementing the QA-LoRA method, specifically its group-wise quantization, group-wise adaptation (LoRA with reduced 'A' matrix), or the merging mechanism via zero-point updates.",
    "Experiment Result": "None. As the QA-LoRA method is not implemented in the provided repository content, there are no specific experimental settings related to its unique components (group-wise quantization/adaptation, zero-point merging) to extract. The `train.py` file, however, outlines a general supervised fine-tuning setup for a causal language model, which includes arguments such as `model_name_or_path` (defaulting to \"facebook/opt-125m\"), `data_path`, `model_max_length` (defaulting to 512), and `optim` (defaulting to \"adamw_torch\")."
}{
    "Title": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation",
    "Main Contributions": "The paper introduces Quantum-informed Tensor Adaptation (QuanTA), a novel parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) that enables efficient high-rank adaptations without inference overhead. QuanTA leverages quantum-inspired tensor operations to overcome the limitations of low-rank methods like LoRA, particularly for complex downstream tasks where low-rank approximation may fail. The approach is theoretically supported by the universality theorem and the rank representation theorem, proving its capability to efficiently parameterize high-rank matrices. Experiments demonstrate that QuanTA significantly enhances performance in commonsense reasoning, arithmetic reasoning, and scalability, achieving superior results with fewer trainable parameters compared to traditional methods and other PEFT approaches.",
    "Methodology": "QuanTA is a reparameterization-based PEFT method inspired by quantum circuits. It parameterizes weight updates using tensor operations analogous to quantum circuits, which inherently realize full-rank matrices. The hidden dimension `d` of the LLM's weight matrices is decomposed into `d = d1 × d2 × ··· ×dN`, and the hidden vector is reshaped and interpreted as a quantum state with N 'qudits'. QuanTA constructs the update matrix by sequentially applying a collection of 'gates' (tensors) that operate on specific axes of this reshaped input. An initialization method is employed where the adapted layer `y = W0x + Tθx − Sx` ensures zero weight updates at the start by initializing a frozen tensor `S` identically to the trainable `Tθ`, which is later merged into the base model's weights (`W′0 = W0 + S`), allowing `y = W′0x + Tθx`. The method's ability to achieve high-rank adaptations is theoretically backed by a universality theorem and a rank representation theorem. It also satisfies a composition openness condition, suggesting its expressivity can grow with network depth. QuanTA incurs no inference overhead as the trained tensor operator can be explicitly constructed as a matrix and merged into the base model weight matrix.",
    "Experimental Setup": "Experiments were conducted on LLaMA2 (7B, 13B, 70B parameters) and LLaMA3 (8B parameters) models, as well as the RoBERTa model. QuanTA was benchmarked against Full Fine-tuning (FT), LoRA (with various ranks), Series Adapters, Parallel Adapters, Prefix Tuning, DoRA, MoRA, LoRETTA, and KronA. Datasets included: 1. DROP dataset for reading comprehension with discrete reasoning (subsampled: 2000 train, 800 validation, 1200 test; F1-score metric). 2. Commonsense Reasoning tasks: models fine-tuned on COMMONSENSE 170K (170020 train, 400 validation) and evaluated on 8 downstream tasks (BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-Easy, ARC-Challenge, OBQA; accuracy metric). 3. Arithmetic Reasoning tasks: models fine-tuned on MATH10K (9519 train, 400 validation) and evaluated on 4 downstream tasks (AQuA, GSM8K, MAWPS, SV AMP; accuracy metric). Additional benchmarks were performed on five GLUE natural language understanding tasks using RoBERTa. All experiments optimized hyperparameters on validation sets and selected the best checkpoint for evaluation. Computations were run on NVIDIA A100 GPUs (single A100 for models up to 13B, four A100 for 70B model). QuanTA configurations tested involved decomposing hidden dimensions into 3, 4, or 5 axes with specific dimension values (e.g., '16-8-8-4').",
    "Limitations": "Current implementation of QuanTA requires sequential application of tensors, which can lead to underutilization of GPU resources when tensors are small, suggesting a need for more efficient implementation. The hyperparameters within QuanTA, such as the number of tensors applied on the same axes, have not yet been fully optimized. The experimental evaluation primarily focuses on the LLaMA model series, and a more comprehensive study across different LLM architectures would be beneficial but requires more computational resources. The surprisingly superior performance of QuanTA over full fine-tuning in some arithmetic tasks might indicate overfitting or challenges in optimizing hyperparameters for full fine-tuning.",
    "Future Research Directions": "Future work includes expanding QuanTA's application to a wider range of tasks and specialized domains to demonstrate its versatility. Investigating the combination of QuanTA with other PEFT methods or integrating it into ensemble models could further enhance performance, especially for complex tasks. Exploring advanced optimization techniques specifically tailored for QuanTA is also a promising direction to improve convergence rates and overall efficiency. Furthermore, designing new methods based on more advanced principles from quantum computing, such as entanglement and superposition, could lead to even more efficient fine-tuning. The parameter efficiency of QuanTA also suggests a lower chance of overfitting, which warrants further investigation.",
    "Experiment Code": "import importlibimport itertoolsimport mathimport reimport warningsfrom dataclasses import asdict, dataclass, fieldfrom enum import Enumfrom math import prodfrom typing import List, Optional, Unionimport opt_einsum as oeimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom ..utils import PeftConfig, PeftTypedef is_bnb_available():    return importlib.util.find_spec(\"bitsandbytes\") is not Noneif is_bnb_available():    import bitsandbytes as bnb@dataclassclass QuanTAConfig(PeftConfig):    d: int = field(default=1, metadata={\"help\": \"quanta number of dimensions\"})    quanta_dropout: float = field(default=0.0, metadata={\"help\": \"quanta dropout\"})    merge_weights: bool = field(default=False,                                metadata={\"help\": \"Merge weights of the original model and the Lora model\"})    fan_in_fan_out: bool = field(default=False,                                 metadata={                                     \"help\": \"Set this to True if the layer to replace stores weight like (fan_in, fan_out)\"}, )    per_dim_features: Optional[List[int]] = field(default=None, metadata={        \"help\": \"List of the number of features per dimension. If not provided, the features are equally divided.\"}, )    per_dim_features2: Optional[List[int]] = field(default=None, metadata={        \"help\": \"List of the number of features per dimension for the output. If not provided, the features are set to per_dim_features.\"}, )    sum_mode: bool = field(default=False, metadata={\"help\": \"Set this to True if the quanta is in sum mode\"})    initialize_mode: str = field(default=\"sum_opposite_freeze_one\",                                 metadata={                                     \"help\": \"Initialization mode for the quanta weights. Can be 'sum_opposite_freeze_one'\"})    bias: str = field(default=\"none\", metadata={\"help\": \"Bias type for Lora. Can be 'none', 'all' or 'lora_only'\"})    target_modules: Optional[Union[List[str], str]] = field(default=None,                                                            metadata={                                                                \"help\": \"List of module names or regex expression of the module names to replace with Lora.\"                                                                        \"For example, ['q', 'v'] or '.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$' \"}, )  # not sure if this is needed legacy support for now    enable_lora: Optional[List[bool]] = field(default=None, metadata={        \"help\": \"Used with `lora.MergedLinear`.\"})  # not sure if this is needed legacy support for now    tensor_rank: int = 5  # not sure if this is needed legacy support for now    def __post_init__(self):        self.peft_type = PeftType.QUANTAclass QuanTAModel(torch.nn.Module):    def __init__(self, config, model):        super().__init__()        self.peft_config = config        self.model = model        self._find_and_replace()        mark_lora_layernorm_cls_trainable(self.model, self.peft_config.task_type, self.peft_config.tensor_rank,                                           self.peft_config.bias)        self.forward = self.model.forward    def _find_and_replace(self):        loaded_in_8bit = getattr(self.model, \"is_loaded_in_8bit\", False)        if loaded_in_8bit and not is_bnb_available():            raise ImportError(\"To use Lora with 8-bit quantization, please install the `bitsandbytes` package. \"                              \"You can install it with `pip install bitsandbytes`.\")        is_target_modules_in_base_model = False        is_hf_device_map_available = hasattr(self.model, \"hf_device_map\")        kwargs = {\"d\": self.peft_config.d, \"per_dim_features\": self.peft_config.per_dim_features,                  \"per_dim_features2\": self.peft_config.per_dim_features2,                  \"quanta_dropout\": self.peft_config.quanta_dropout,                  \"fan_in_fan_out\": self.peft_config.fan_in_fan_out,                  \"merge_weights\": (self.peft_config.merge_weights or self.peft_config.inference_mode),                  \"sum_mode\": self.peft_config.sum_mode, \"initialize_mode\": self.peft_config.initialize_mode, }        key_list = [key for key, _ in self.model.named_modules()]        for key in key_list:            if isinstance(self.peft_config.target_modules, str):                target_module_found = re.fullmatch(self.peft_config.target_modules, key)            else:                target_module_found = any(key.endswith(target_key) for target_key in self.peft_config.target_modules)            if target_module_found:                if not is_target_modules_in_base_model:                    is_target_modules_in_base_model = True                parent, target, target_name = self._get_submodules(key)                bias = target.bias is not None                if loaded_in_8bit and isinstance(target, bnb.nn.Linear8bitLt):                    raise NotImplementedError                elif isinstance(target, torch.nn.Linear) and self.peft_config.enable_lora is None:                    new_module = Linear(target.in_features, target.out_features, bias=bias, **kwargs)                elif self.peft_config.enable_lora is not None:                    raise NotImplementedError                self._replace_module(parent, target_name, new_module, target)        if not is_target_modules_in_base_model:            raise ValueError(f\"Target modules {self.peft_config.target_modules} not found in the base model. \"                             f\"Please check the target modules and try again.\")    def _get_submodules(self, key):        parent = self.model.get_submodule(\".\".join(key.split(\".\")[:-1]))        target_name = key.split(\".\")[-1]        target = self.model.get_submodule(key)        return parent, target, target_name    def _replace_module(self, parent_module, child_name, new_module, old_module):        setattr(parent_module, child_name, new_module)        new_module.weight = old_module.weight        if old_module.bias is not None:            new_module.bias = old_module.bias        if getattr(old_module, \"state\", None) is not None:            new_module.state = old_module.state            new_module.to(old_module.weight.device)        # dispatch to correct device        for name, module in new_module.named_modules():            if \"lora_\" in name or \"quanta_\" in name:                module.to(old_module.weight.device)            if 'bias' in name:                module.to(old_module.weight.device)    def __getattr__(self, name: str):        \"\"\"Forward missing attributes to the wrapped module.\"\"\"        try:            return super().__getattr__(name)  # defer to nn.Module's logic        except AttributeError:            return getattr(self.model, name)    @property    def modules_to_save(self):        return None    def get_peft_config_as_dict(self, inference: bool = False):        config = {k: v.value if isinstance(v, Enum) else v for k, v in asdict(self.peft_config).items()}        if inference:            config[\"inference_mode\"] = True        return config    def _set_adapter_layers(self, enabled=True):        for module in self.model.modules():            if isinstance(module, LoraLayer) or isinstance(module, QuanTALayer):                module.disable_adapters = False if enabled else True    def enable_adapter_layers(self):        self._set_adapter_layers(enabled=True)    def disable_adapter_layers(self):        self._set_adapter_layers(enabled=False)# had to adapt it for `lora_only` to workdef mark_lora_layernorm_cls_trainable(model: nn.Module, task_type, tensor_rank, bias: str = \"none\") -> None:    for n, p in model.named_parameters():        if \"lora_\" not in n and \"quanta_\" not in n:            p.requires_grad = False    if bias == \"none\":        pass    elif bias == \"all\":        for n, p in model.named_parameters():            if \"bias\" in n:                p.requires_grad = True    elif bias == \"lora_only\":        for m in model.modules():            if (isinstance(m, LoraLayer) or isinstance(m, QuanTALayer)) and hasattr(m, \"bias\") and m.bias is not None:                m.bias.requires_grad = True    else:        raise NotImplementedError  # mark layer-norm trainable    for n, p in model.named_parameters():        if \"Norm\" in n:            p.requires_grad = True    # mark cls trainable and tensorized    if task_type == 'SEQ_CLS':        raise NotImplementedErrorclass LoraLayer:    def __init__(self, r: int, lora_alpha: int, lora_dropout: float, merge_weights: bool, ):        self.r = r        self.lora_alpha = lora_alpha        # Optional dropout        if lora_dropout > 0.0:            self.lora_dropout = nn.Dropout(p=lora_dropout)        else:            self.lora_dropout = lambda x: x        # Mark the weight as unmerged        self.merged = False        self.merge_weights = merge_weights        self.disable_adapters = Falseclass MergeBuffer(nn.Module):    def __init__(self, default=False):        super(MergeBuffer, self).__init__()        self.register_buffer('merged', torch.tensor(default))  # to keep track if the trainable weights are merged    def __bool__(self):        return self.merged.item()    def set(self, value):        self.merged.fill_(value)class QuanTALayer:    def __init__(self, d: int, quanta_dropout: float, merge_weights: bool, sum_mode: bool = False, ):        self.d = d        self.sum_mode = sum_mode        if quanta_dropout > 0.:            self.quanta_dropout = nn.Dropout(p=quanta_dropout)        else:            self.quanta_dropout = lambda x: x        self.merged = MergeBuffer(default=False)  # so that this will be tracked when saving the model and loading it        self.frozen_merged = MergeBuffer(default=False)  # the frozen weights are separately tracked        self.merge_weights = merge_weightsclass BufferDict(nn.Module):    def __init__(self, init_dict=None):        super(BufferDict, self).__init__()        self.buffer_names = []        if init_dict is not None:            for name, tensor in init_dict.items():                self.add_buffer(name, tensor)    def add_buffer(self, name, tensor):        self.register_buffer(name, tensor)        if name not in self.buffer_names:            self.buffer_names.append(name)    def __getitem__(self, name):        return getattr(self, name)    def __setitem__(self, name, tensor):        self.add_buffer(name, tensor)    def items(self):        for name, tensor in self.named_buffers():            if name in self.buffer_names:                yield name, tensor    def keys(self):        return self.buffer_names    def values(self):        return [getattr(self, name) for name in self.buffer_names]class Linear(nn.Linear, QuanTALayer):    # QuanTA implemented in a dense layer    def __init__(self, in_features: int, out_features: int, d: int = 1, quanta_dropout: float = 0.,                 fan_in_fan_out: bool = False,                 # Set this to True if the layer to replace stores weight like (fan_in, fan_out)                 per_dim_features: list = None,                 # usually used as the input features, but will check if need to be swapped with per_dim_features2, or if it should be the same as per_dim_features2                 per_dim_features2: list = None,                 # usually used as the output features, but will check if need to be swapped with per_dim_features, or if it should be the same as per_dim_features                 merge_weights: bool = False, sum_mode: bool = False, initialize_mode: str = 'scale_by_softplus_zero',                 **kwargs):        nn.Linear.__init__(self, in_features, out_features, **kwargs)        self.fan_in_fan_out = fan_in_fan_out        self.initialize_mode = initialize_mode        # Actual trainable parameters        if per_dim_features is not None:            d = len(per_dim_features)        QuanTALayer.__init__(self, d=d, quanta_dropout=quanta_dropout, sum_mode=sum_mode, merge_weights=merge_weights)        if d > 1:            self.max_features = max(in_features, out_features)            if per_dim_features is not None:                self.per_dim_features = per_dim_features            else:                self.per_dim_features = [math.ceil(self.max_features ** (1 / d))] * d            if per_dim_features2 is not None:                raise NotImplementedError('per_dim_features2 is not implemented yet')            else:                self.per_dim_features2 = self.per_dim_features            self.total_features = prod(self.per_dim_features)            self.total_features2 = prod(self.per_dim_features2)            if self.total_features != in_features:                warnings.warn(                    f'per_dim_features={self.per_dim_features} does not match in_features={in_features}, this should work but may result in downgraded performance or additional cost. Please make sure this is intended.')            if self.total_features2 != out_features:                warnings.warn(                    f'per_dim_features2={self.per_dim_features2} does not match out_features={out_features}, this should work but may result in downgraded performance or additional cost. Please make sure this is intended.')            quanta_weights = {}            for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):                quanta_weights[f'{dim1} {dim2}'] = nn.Parameter(                    self.weight.new_zeros(self.per_dim_features2[dim2], self.per_dim_features2[dim1],                                          self.per_dim_features[dim2], self.per_dim_features[                                              dim1]))  # reverse the order because dim1 is closer to the end            self.quanta_weights = nn.ParameterDict(quanta_weights)            if initialize_mode == 'sum_opposite_freeze_one':                quanta_weights2 = {}                for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):                    quanta_weights2[f'{dim1} {dim2}'] = self.weight.new_zeros(self.per_dim_features2[dim2],                                                                              self.per_dim_features2[dim1],                                                                              self.per_dim_features[dim2],                                                                              self.per_dim_features[dim1])                self.quanta_weights2 = BufferDict(quanta_weights2)            else:                assert False, f'initialize_mode={initialize_mode} not implemented'            # Freezing the pre-trained weight matrix            self.weight.requires_grad = False        self.reset_parameters()        if fan_in_fan_out:            self.weight.data = self.weight.data.transpose(0, 1)        self.gen_einsum_expr_train()        self.gen_einsum_expr_eval()    def reset_parameters(self):        nn.Linear.reset_parameters(self)        if hasattr(self, 'quanta_weights'):            for k, v in self.quanta_weights.items():                nn.init.kaiming_uniform_(v.view(v.shape[0] * v.shape[1], v.shape[2] * v.shape[3]), a=math.sqrt(5),                                         nonlinearity='linear')  # initialize as if it is a matrix            if self.initialize_mode == 'last_layer_zero':                nn.init.zeros_(self.quanta_weights[f'{-self.d + 1} {-self.d}'])            if self.initialize_mode == 'add_local_layer_zero' or self.initialize_mode == 'add_local_layer_zero_sum':                for k, v in self.local_weights.items():                    nn.init.zeros_(v)            if self.initialize_mode == 'sum_opposite':                for k, v in self.quanta_weights2.items():                    v.data[:] = self.quanta_weights[k].data            if self.initialize_mode == 'sum_opposite_freeze_one':                for k, v in self.quanta_weights2.items():                    v[:] = self.quanta_weights[k].data    def train(self, mode: bool = True):        def T(w):            return w.transpose(0, 1) if self.fan_in_fan_out else w        nn.Linear.train(self, mode)        if mode:            if self.merge_weights and self.merged and self.initialize_mode in ['sum_opposite_freeze_one']:                # Make sure that the weights are not merged                if self.d > 0:                    full_quanta_weights = F.pad(self.einsum_expr_eval(                        *[self.quanta_weights[f'{dim1} {dim2}'].to(self.weight.dtype) for (dim1, dim2) in                          itertools.combinations(range(-1, -self.d - 1, -1), 2)]).reshape(self.total_features2,                                                                                          self.total_features),                                                (0, self.in_features - self.total_features, 0,                                                  self.out_features - self.total_features2),                                                'constant', 0.)                    self.weight.data -= T(full_quanta_weights)                self.merged.set(False)        else:            if self.merge_weights and not self.merged and self.initialize_mode in ['sum_opposite_freeze_one']:                # Merge the weights and mark it                if self.d > 0:                    full_quanta_weights = F.pad(self.einsum_expr_eval(                        *[self.quanta_weights[f'{dim1} {dim2}'].to(self.weight.dtype) for (dim1, dim2) in                          itertools.combinations(range(-1, -self.d - 1, -1), 2)]).reshape(self.total_features2,                                                                                          self.total_features),                                                (0, self.in_features - self.total_features, 0,                                                 self.out_features - self.total_features2),                                                'constant', 0.)                    self.weight.data += T(full_quanta_weights)                    if not self.frozen_merged:                        self.merge_frozen_weights()                self.merged.set(True)    def merge_frozen_weights(self):        def T(w):            return w.transpose(0, 1) if self.fan_in_fan_out else w        if self.frozen_merged:            warnings.warn('The frozen weights are already merged, ignoring the request to merge the frozen weights')        else:            full_quanta_weights = -F.pad(self.einsum_expr_eval(                *[self.quanta_weights2[f'{dim1} {dim2}'].to(self.weight.dtype) for (dim1, dim2) in                  itertools.combinations(range(-1, -self.d - 1, -1), 2)]).reshape(self.total_features2,                                                                                  self.total_features),                                         (0, self.in_features - self.total_features, 0,                                          self.out_features - self.total_features2), 'constant',                                          0.)            self.weight.data += T(full_quanta_weights)            self.frozen_merged.set(True)    def unmerge_frozen_weights(self):        def T(w):            return w.transpose(0, 1) if self.fan_in_fan_out else w        if not self.frozen_merged:            warnings.warn('The frozen weights are already unmerged, ignoring the request to unmerge the frozen weights')        else:            full_quanta_weights = -F.pad(self.einsum_expr_eval(                *[self.quanta_weights2[f'{dim1} {dim2}'].to(self.weight.dtype) for (dim1, dim2) in                  itertools.combinations(range(-1, -self.d - 1, -1), 2)]).reshape(self.total_features2,                                                                                  self.total_features),                                         (0, self.in_features - self.total_features, 0,                                          self.out_features - self.total_features2), 'constant',                                          0.)            self.weight.data -= T(full_quanta_weights)            self.frozen_merged.set(False)    def gen_einsum_expr_train(self):        \"\"\"        Generate the einsum expression for the tensorized weights during training.        \"\"\"        d = self.d        current_symbols_inds = list(range(d))        eq = '...'        for i in current_symbols_inds:            eq += oe.get_symbol(i)        for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):            symbol_ind1 = current_symbols_inds[dim1]            symbol_ind2 = current_symbols_inds[dim2]            symbol_ind3 = symbol_ind1 + d            symbol_ind4 = symbol_ind2 + d            eq += ',' + oe.get_symbol(symbol_ind4) + oe.get_symbol(symbol_ind3) + oe.get_symbol(                symbol_ind2) + oe.get_symbol(                symbol_ind1)  # reverse order because dim1 is toward the end than dim2 and because of matrix multiplication order convention. Note that this is different from the forward function            current_symbols_inds[dim1] = symbol_ind3            current_symbols_inds[dim2] = symbol_ind4        eq += '->...'        for i in current_symbols_inds:            eq += oe.get_symbol(i)        shapes = [(100,) + tuple(self.per_dim_features)]  # may need to change the 100 to some other value        for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):            shapes.append((self.per_dim_features[dim2], self.per_dim_features[dim1], self.per_dim_features[dim2],                           self.per_dim_features[dim1]))        optimize = 'optimal' if d <= 4 else 'branch-all' if d <= 5 else 'branch-2' if d <= 7 else 'auto'        expr = oe.contract_expression(eq, *shapes, optimize=optimize)        self.einsum_eq_train = eq        self.einsum_expr_train = expr    def gen_einsum_expr_eval(self):        \"\"\"        Generate the einsum expression for the tensorized weights during evaluation.        \"\"\"        d = self.d        current_symbols_inds = list(range(d))        init_symbols_inds = [i for i in current_symbols_inds]  # copy        eq = ''        for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):            symbol_ind1 = current_symbols_inds[dim1]            symbol_ind2 = current_symbols_inds[dim2]            symbol_ind3 = symbol_ind1 + d            symbol_ind4 = symbol_ind2 + d            eq += ',' + oe.get_symbol(symbol_ind4) + oe.get_symbol(symbol_ind3) + oe.get_symbol(                symbol_ind2) + oe.get_symbol(symbol_ind1)  # reverse order because dim1 is toward the end than dim2            current_symbols_inds[dim1] = symbol_ind3            current_symbols_inds[dim2] = symbol_ind4        eq += '->'        for i in current_symbols_inds:            eq += oe.get_symbol(i)        for i in init_symbols_inds:            eq += oe.get_symbol(                i)  # note that this is also the reverse order, so it is the usual matrix multiplication order which is (fan_out, fan_in)        eq = eq[1:]        shapes = []        for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):            shapes.append((self.per_dim_features[dim2], self.per_dim_features[dim1], self.per_dim_features[dim2],                           self.per_dim_features[dim1]))        optimize = 'optimal' if d <= 4 else 'branch-all' if d <= 5 else 'branch-2' if d <= 7 else 'auto'        expr = oe.contract_expression(eq, *shapes, optimize=optimize)        self.einsum_eq_eval = eq        self.einsum_expr_eval = expr    def forward_quanta_weights(self, x, quanta_weights):        \"\"\"        assume x is of shape (batch, *per_dim_features)        \"\"\"        return self.einsum_expr_train(x, *[quanta_weights[f'{dim1} {dim2}'].to(x.dtype) for (dim1, dim2) in                                           itertools.combinations(range(-1, -self.d - 1, -1), 2)])    def forward_sum_opposite(self, x: torch.Tensor):        assert self.initialize_mode == 'sum_opposite_freeze_one', f'this function is only for sum_opposite_freeze_one, but got {self.initialize_mode=}'        assert not self.sum_mode, f'this function only works for sum_mode=False, but got {self.sum_mode=}'        if not self.frozen_merged:            self.merge_frozen_weights()  # make sure the frozen weights are merged        previous_dtype = self.weight.dtype        def T(w):            return w.transpose(0, 1) if self.fan_in_fan_out else w        if self.d > 1 and not self.merged:            result = F.linear(x, T(self.weight), bias=self.bias.to(previous_dtype) if self.bias is not None else None)            x = self.quanta_dropout(x)            x = F.pad(x, (0, self.total_features - self.in_features), 'constant', 0.)            x_shape = x.shape            x = x.view(-1, *self.per_dim_features)            # then deal with weight            x = self.forward_quanta_weights(x, self.quanta_weights).reshape(*x_shape[:-1], self.total_features2)            result += F.pad(x, (0, self.out_features - self.total_features2), 'constant', 0.)            return result        else:            return F.linear(x, T(self.weight), bias=self.bias.to(previous_dtype) if self.bias is not None else None)    def forward(self, x: torch.Tensor):        if self.initialize_mode == 'sum_opposite_freeze_one':            return self.forward_sum_opposite(x)        else:            raise NotImplementedError(                f'initialize_mode={self.initialize_mode} not implemented, only sum_opposite_freeze_one is implemented at the moment')",
    "Experiment Result": "QuanTA-specific parameters in `OurArguments` (defined in `run/run.py` and `run/eval.py`):\n- `tuning_type`: 'quanta' (to enable QuanTA)\n- `quanta_d`: int = 4 (quanta number of dimensions, `d` in the paper)\n- `quanta_per_dim_features`: List[int] = (16, 8, 8, 4) (List of the number of features per dimension for input)\n- `quanta_per_dim_features2`: Tuple[int] = None (List of the number of features per dimension for the output. If not provided, it's set to `quanta_per_dim_features`)\n- `quanta_sum_mode`: bool = False (Whether the quanta is in sum mode)\n- `quanta_initialize_mode`: str = 'sum_opposite_freeze_one' (Initialization mode for the quanta weights)\n- `quanta_dropout`: float = 0.0 (QuanTA dropout rate)\n- `target_modules`: List[str] = None (List of module names or regex expression of the module names to replace with QuanTA. Defaulting to None, it relies on `_prepare_lora_config` which uses `TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING` as a fallback, e.g., ['q_proj', 'v_proj'] for Llama)."
}{
    "Title": "Evaluating Quantized Large Language Models",
    "Main Contributions": "This paper presents a comprehensive evaluation of Post-Training Quantization (PTQ) on Large Language Models (LLMs) to address their high memory consumption and computational overhead. The study evaluates the effects of PTQ on Weights, Activations, and KV Cache across 11 diverse LLM families (125M to 180B parameters) and five task types: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. It also assesses state-of-the-art quantization methods. Key findings include tensor-level, model-level, and task-level insights into quantization tolerance, offering recommendations for applying quantization techniques and outlining future research directions.",
    "Methodology": "The study focuses on uniform quantization, defining the process using scaling factor (S) and zero-point (Z) for 16-bit floating-point (FP16) to low-precision integer conversion. It investigates three types of quantization: Weight-only, Weight-Activation, and KV Cache Quantization, applying asymmetric group-wise quantization for Weights and KV Cache, and symmetric per-token quantization for Activations. Statistical analyses using maximum absolute value (AbsMax), standard deviation (Std), and kurtosis (K) are employed to understand tensor properties influencing quantization sensitivity.",
    "Experimental Setup": "The evaluation covers 11 LLM families (OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, Mamba) with parameters from 125M to 180B. Five task types are assessed: basic NLP (Language Modeling, Understanding, Reasoning on CHID, Winogrande, RACE, LAMBADA, PIQA, SIQA), emergent abilities (In-Context Learning, Instruction-Following, Multi-Step Reasoning, Self-Calibration on MMLU, CEval, GSM8K, StrategyQA, Hellaswag, ARC), trustworthiness (Ethics, Hallucination, Adversarial Robustness on ETHICS, TruthfulQA, AdvGLUE), dialogue (MT-bench), and long-context tasks (LongEval, Multi-Doc QA). Validation methods include 'PPL mode' for multiple-choice and 'Gen mode' for generative tasks, with GPT-4 grading for dialogue quality and accuracy for others.",
    "Limitations": "The study exclusively focuses on Post-training Quantization (PTQ) and does not explore Quantization-Aware Training (QAT). Detailed ablation studies on hyperparameters, such as different group sizes in group-wise quantization, were not conducted. Not all newly available LLMs, particularly smaller models (1B to 3B), were evaluated. The concrete recommendations on bit-width are specific to certain LLMs and tasks and may not generalize to new tasks and LLMs.",
    "Future Research Directions": "Future research could focus on making extremely low bit-width quantization (e.g., W2 and W4A4) more effective through advanced quantization schemes or Quantization-Aware Training (QAT). Investigating the potential of mix-precision quantization methods to balance hardware efficiency and performance by employing different bit-widths and scaling schemes for different tensors is also suggested. Further in-depth research is needed on how to simultaneously leverage quantization methods and Mixture-of-Experts (MoE) techniques to enhance inference efficiency. Exploring the impact of quantization on multi-turn dialogues beyond two turns is another promising area.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
    "Main Contributions": "The paper introduces AutoLRS, an efficient and automatic method for optimizing the learning rate (LR) schedule during deep neural network (DNN) training, aiming to eliminate significant manual tuning effort and computational cost. AutoLRS achieves this by optimizing the LR for each training stage through Bayesian Optimization (BO) on the fly, coupled with a lightweight exponential model to predict validation loss. The method demonstrates substantial speedups (1.22x for ResNet-50, 1.43x for Transformer, 1.5x for BERT) over original hand-tuned LR schedules and an average 1.31x speedup over state-of-the-art heavily-tuned schedules, while maintaining or improving generalization performance.",
    "Methodology": "AutoLRS operates by breaking down the global LR schedule optimization into sequential sub-problems, where a constant LR is found for each 'training stage' of τ steps. It treats the validation loss at the end of each stage as a black-box function and optimizes it using Bayesian Optimization (BO). To overcome the computational expense of BO requiring full τ-step evaluations, AutoLRS introduces a time-series forecasting model: for each LR candidate, the model is trained for only τ' (τ' ≪ τ) steps, and an exponential model (L(t) = a*exp(b*t) + c) predicts the validation loss after τ steps. This forms a 'mutual-training' process where BO (using a Gaussian Process surrogate model with Matern kernel and Lower Confidence Bound acquisition function) provides LRs, and the exponential model provides efficient loss predictions. Key practical improvements include a curriculum for τ (gradually increasing τ from 1000 and τ' from 100, doubling until τmax) and using training loss as an approximation for validation loss in early stages, or a small subset of the validation set in later stages to further reduce overhead. An iterative spline smoothing pre-processes the observed loss series for stable exponential fitting.",
    "Experimental Setup": "AutoLRS was extensively evaluated across diverse DNNs and tasks. These included training ResNet-50 on ImageNet classification (with SGD and momentum), Transformer on the WMT 2014 English-German neural machine translation dataset (with Adam), and BERTBASE pre-training on English Wikipedia and BooksCorpus, followed by fine-tuning on downstream NLP tasks (MRPC, MNLI, CoLA, SQuAD v1.1) (with Adam). Additionally, VGG-16 was used for ablation studies and comparisons on CIFAR-10 and CIFAR-100 datasets. Baselines for comparison included the original hand-tuned LR schedules, Cyclical Learning Rate (CLR), Stochastic Gradient Descent with Warm Restarts (SGDR), Hypergradient Descent (HD), MARTHE, and Hyperband. Performance was measured by top-1 accuracy (ImageNet, CIFAR, NLP tasks) and BLEU score (Transformer), focusing on the training steps required to reach target performance and the final test-set accuracy or F1 score.",
    "Limitations": "The current AutoLRS method does not automate the learning rate search for warmup steps, as these phases lack an explicit validation loss objective. While it significantly reduces the hyperparameter tuning burden, AutoLRS still requires a predefined learning-rate search interval (ηmin, ηmax), which is typically found using an LR range test. The maximum stage length (τmax) is also a hyperparameter, although experiments suggest the method is robust to its specific setting. The default values for internal hyperparameters like the number of LRs to evaluate by BO (k), the number of steps to evaluate each LR (τ'), and the trade-off weight in the acquisition function (κ) are provided and work consistently, but could potentially be further optimized. The exponential forecasting model, while efficient, may not perfectly capture highly non-monotone loss changes that can occur in very early training steps, though iterative spline smoothing is applied to mitigate this.",
    "Future Research Directions": "The paper does not explicitly delineate future research directions in a dedicated section. However, potential extensions could include: (1) Developing mechanisms to automatically search for optimal learning rates during warmup phases. (2) Exploring more advanced or adaptive time-series forecasting models beyond the exponential model, especially for capturing complex non-monotone loss dynamics more robustly. (3) Investigating methods for adaptively determining AutoLRS's own hyperparameters such as τmax, k, τ', and κ, to further minimize any remaining manual intervention. (4) Applying AutoLRS to an even broader range of models, tasks, and optimizers, including scenarios with limited data or specific hardware constraints, to further test its generality and robustness. (5) Researching how AutoLRS could be integrated with other hyperparameter optimization techniques beyond just the learning rate.",
    "Experiment Code": "File Path: autolrs_callback.py\nContent:\nimport torch\nimport os\nimport logging\nimport socket\nimport string\nimport random\nimport time\n\nclass AutoLRS():\n    def __init__(self, model, optimizer, val_fn, listening_host='localhost', listening_port=12315, warmup_steps=0, warmup_lr=0, summary_steps=1):\n        self._net = model\n        self._optimizer = optimizer\n        self._val_fn = val_fn \n        self._lr = 0.000001\n        self._warmup_steps = warmup_steps\n        self._warmup_lr = warmup_lr \n        self._global_step = 0\n        self._socket = socket.socket()\n        self._started = False\n        self._summary_steps = summary_steps\n        self._checkpoint_path = './checkpoint/autolrs_ckpt_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k = 7))  + '.pth'\n        self._listening_host = listening_host\n        self._listening_port = listening_port\n        self._best_acc = 0\n\n        if not os.path.exists('checkpoint'):\n            os.makedirs('checkpoint')\n\n        self.connect_server()\n    \n    def connect_server(self):\n        self._socket.connect((self._listening_host, self._listening_port))\n\n    def _verbose_operation(self, _op):\n        if self._global_step % self._summary_steps == 0:\n            logging.info(\"[AutoLRS at {}] {}\".format(self._global_step, _op))\n\n    def save_variables(self):\n        \"\"\"Save model parameters and optimizer states.\"\"\"\n        _start_time = time.time()\n        torch.save({\n            'model_state_dict': self._net.state_dict(),\n            'optimizer_state_dict': self._optimizer.state_dict()\n            }, self._checkpoint_path)\n        logging.info(\"[AutoLRS] backup variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def restore_variables(self):\n        _start_time = time.time()\n        checkpoint = torch.load(self._checkpoint_path)\n        self._net.load_state_dict(checkpoint['model_state_dict'])\n        self._optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        logging.info(\"[AutoLRS] restoring variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def on_train_batch_end(self, loss):\n        if self._global_step < self._warmup_steps:\n            # linear warmup\n            self._lr = (self._warmup_lr / self._warmup_steps) * (self._global_step + 1)\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n            self._global_step += 1\n\n        elif not self._started:\n            self.save_variables()\n            print(\"backup trainable variables to CPU\") \n            self._started = True\n            self._socket.send(\",\".join(('startBO', str(loss))).encode(\"utf-8\"))\n            self._verbose_operation(\"Start Bayesian Optimization(BO)\")\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            self._lr = (float(data.split(\",\")[-1]))\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n        else:\n            self._socket.send(','.join(('loss', str(loss))).encode('utf-8'))\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            if data.startswith(\"restore\"):\n                self.restore_variables()\n                self._verbose_operation(\"restore trainable variables\")\n            elif data.startswith(\"ckpt\"):\n                self.save_variables()\n                self._verbose_operation(\"backup trainable variables\")\n            elif data.startswith('evaluate'):\n                val_loss = self._val_fn()\n                self._socket.send(\",\".join((\"val_loss\", str(val_loss))).encode(\"utf-8\"))\n                data = self._socket.recv(1024).decode(\"utf-8\")\n            elif data.startswith('save'):\n                pass\n            else:\n                self._lr = (float(data.split(\",\")[-1]))\n                for param_group in self._optimizer.param_groups:\n                    param_group['lr'] = self._lr\n                self._global_step += 1\n\nFile Path: autolrs_server.py\nContent:\nimport argparse\nimport socket                \nimport random\nimport numpy as np \nimport threading\nimport time\nimport math\nimport os\nfrom skopt import Optimizer\nfrom skopt.space import Real\nfrom scipy.interpolate import UnivariateSpline\nfrom scipy import optimize \nimport sys\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef f(b, x, y):\n    A = np.vstack([np.exp(-np.exp(b) * x), np.ones(len(x))]).T\n    res = np.linalg.lstsq(A, y, rcond=None)[1]\n    return res\n\ndef spline_iter(xs, ys, is_training, spline_deg=2, filter_ratio=0.03, num_of_iter=10, bound=0.5):\n    \"\"\" Use iterative spline to eliminate noise and outliers in the loss series.\n        is_training specifies whether the loss series in use is training loss or validation loss.\n    \"\"\"\n    bound = xs[int((len(xs) - 1) * bound)]\n    if is_training:\n        num_of_iter = 10\n    else:\n        num_of_iter = 1\n\n    for _ in range(num_of_iter):\n        spline_ys = UnivariateSpline(xs, ys, k=spline_deg)(xs)\n        dys = np.abs(ys - spline_ys)\n\n        if is_training:\n            outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[int(round(-len(dys) * filter_ratio)):])\n        else:\n            outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[-1:])\n        outliers = [i for i in outliers if i < bound]\n\n        xs2 = np.zeros(len(xs) - len(outliers))\n        ys2 = np.zeros(len(xs) - len(outliers))\n        i1 = 0\n        for i2 in range(len(xs)):\n            if i2 not in outliers:\n                xs2[i1], ys2[i1] = xs[i2], ys[i2]\n                i1 += 1\n        xs, ys = xs2, ys2\n    return xs, ys\n\ndef exp_forecast(loss_series, end_step, is_training, spline_order=2):\n    \"\"\" Do exponential forecasting on a loss series.\"\"\"\n    xs = np.arange(end_step - len(loss_series), end_step)\n    xs2, ys2 = spline_iter(xs, loss_series, is_training)\n    ys = UnivariateSpline(xs2, ys2, k=spline_order)(xs)\n    logging.debug('ys after spline iter: {}'.format(ys))\n    b = optimize.fmin(f, 0, args=(xs, ys), xtol=1e-5, ftol=1e-5, disp=False)[0]\n    b = -np.exp(b)\n    A = np.vstack([np.exp(b * xs), np.ones(len(xs))]).T\n    a, c = np.linalg.lstsq(A, ys, rcond=None)[0]\n    return a, b, c\n\nclass RingBuffer:\n    \"\"\" A class for storing and manipulating loss series and do exponential forecasting. \"\"\"\n\n    def __init__(self, size):\n        self.data = [None for i in range(size)]\n\n    def reset(self):\n        self.data = [None for i in self.data]\n\n    def append(self, x):\n        self.data.pop(0)\n        self.data.append(x)\n\n    def get(self):\n        return self.data\n\n    def exponential_forcast(self, pred_index, is_training):\n        loss_series = self.data[:]\n        end_epoch = len(loss_series)\n        x = np.arange(end_epoch - len(loss_series), end_epoch)\n        y = np.array(loss_series)\n        a3, b3, c3 = exp_forecast(y, len(y), is_training)\n        forcast_y = a3 * np.exp(b3 * pred_index) + c3\n        logging.debug(\"Exponential fit: {}, {}, {}\".format(a3, b3, c3))\n        return forcast_y\n\nclass Controller(object):\n    def __init__(self, host, port, min_lr, max_lr):\n        # Constants\n        EXPLOITATION_STEP = 1000\n        LR_STEPS = 100\n        RING_BUFFER_LEN = 100\n        LR_TO_EXPLORE = 10\n        TAU_MAX = 8000\n\n        self.min_lr = float(min_lr)\n        self.max_lr = float(max_lr)\n        self.host = host\n        self.port = port\n        self.threads = []\n        self.num_threads = 1\n        self.event = threading.Event()\n        self.sock = socket.socket()\n        self.sock.bind((self.host, self.port))\n        self.global_step = 0\n        self.last_total_loss = 0.0\n        self.average_loss = 0.0\n        self.loss_vector = []\n        self.lr = 0\n        self.lr_steps = LR_STEPS \n        self.tau_max = TAU_MAX\n        self.val_freq = int(self.lr_steps/16)\n        self.lr_counter = 0\n        self.lr_to_explore = LR_TO_EXPLORE\n        self.BO_stage = True\n        self.val_stage = False\n        self.message = ''\n        self.loss_after_exploitation = None\n        self.ring_buffer_len = RING_BUFFER_LEN\n        if self.val_stage:\n            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len // self.val_freq)\n        else:\n            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n\n        self.exploitation_step = EXPLOITATION_STEP\n        self.exploitation_flag = False \n        self.exploitation_counter = 0\n        \n        self.opt = None\n        self.x_func_dict = dict()\n        self.x_iters = []\n        self.func_val_iters = []\n\n        self.num_ranks = 0\n        self.finished_minions = 0\n        self.lock1 = threading.Lock()\n        self.lock2 = threading.Lock()\n\n    def listen(self):\n        self.sock.listen(20)\n        while True:\n            client, address = self.sock.accept()\n            logging.info('[Server]: Got connection from {}'.format(address))\n            self.threads.append(threading.Thread(target = self.run, args = (client, address, self.event)))\n            if len(self.threads) == self.num_threads:\n                self.num_ranks = len(self.threads)\n                self.num_minions = self.num_ranks - 1\n                logging.info('[Server]: num_ranks: {}'.format(self.num_ranks))\n                for thread in self.threads:\n                    thread.start()\n                logging.info('[Server]: threads started')\n                for thread in self.threads:\n                    thread.join()\n                    self.threads = []\n                logging.info('[Server]: join finished')\n                sys.exit()\n\n    def run(self, c, address, event):\n        size = 1024\n        while True:\n            data = c.recv(size).decode()\n            if not data:\n                sys.exit()\n            logging.debug(data.split(','))\n            total_loss = float(data.split(',')[-1])\n            self.lock2.acquire()\n            self.loss_vector.append(total_loss)\n\n            # compute average loss across ranks\n            if len(self.loss_vector) == self.num_ranks:\n                self.average_loss = sum(self.loss_vector) / len(self.loss_vector) \n                logging.info('[Server]: average loss = {}, step = {}'.format(self.average_loss, self.global_step))\n                if self.val_stage:\n                    if 'val' in data:\n                        self.ring_loss_buffer.append(self.average_loss)\n                    else:\n                        self.global_step += 1\n                else:\n                    self.ring_loss_buffer.append(self.average_loss)\n                    self.global_step += 1\n                self.loss_vector = []\n            self.lock2.release()\n\n            if 'val' in data:\n                c.send(str(self.lr).encode('utf-8')) \n                continue\n\n            if 'minion' in data:\n                # blocking\n                event_is_set = event.wait()\n                c.send(self.message.encode('utf-8')) \n                logging.debug('[Server] message: %s', self.message)\n                self.lock1.acquire()\n                self.finished_minions += 1\n                self.lock1.release()\n                if self.finished_minions == self.num_minions:\n                    event.clear()\n                    self.finished_minions = 0\n                continue\n\n            else:\n                if data.startswith('startBO'):\n                    self.last_total_loss = self.average_loss\n                    self.init_loss = self.average_loss\n                    self.loss_after_exploitation = self.average_loss\n\n                # exploitation stage -- actual training stage using the best-found LR\n                if self.exploitation_flag:\n                    logging.debug('[Server exploitation]: average loss ' + str(self.average_loss) + ' step=' + str(self.global_step))\n                    if self.exploitation_counter == self.exploitation_step:\n                        self.BO_stage = True\n                        self.exploitation_flag = False\n                        self.exploitation_counter = 0\n                        logging.info('[Server]: exploitation stage done')\n                        logging.info('[Server]: reconfigure...')\n                        if self.lr_steps < self.tau_max / 10:\n                            self.lr_steps = self.lr_steps * 2\n                            self.val_freq = int(self.lr_steps/16)\n                            self.ring_buffer_len = self.lr_steps \n                            self.exploitation_step = self.exploitation_step * 2\n                            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n                        else:\n                            self.val_stage = True\n                        if self.val_stage:\n                            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len // self.val_freq)\n                        self.loss_after_exploitation = self.average_loss\n\n                        self.message = 'save'\n                        c.send(self.message.encode('utf-8'))\n                        event.set()\n                        continue\n\n                    else:\n                        self.exploitation_counter += 1\n                        self.message = str(self.lr)\n                        c.send(str(self.lr).encode('utf-8')) \n                        event.set()\n                        continue\n\n                # BO stage -- LR search stage\n                if self.BO_stage:\n                    self.opt = Optimizer([Real(self.min_lr, self.max_lr, 'log-uniform')], \"GP\", n_initial_points=1, acq_func='LCB', acq_func_kwargs={'kappa':1e6})\n                    self.BO_stage = False\n                    self.lr = self.opt.ask()[0]\n\n                    # prevent BO in scikit-optimize from searching for the same LR explored before\n                    while True:\n                        if self.lr in self.x_func_dict:\n                            self.opt.tell([self.lr], self.x_func_dict[self.lr])\n                            self.lr = self.opt.ask()[0]\n                        else:\n                            break\n\n                    self.message = str(','.join(('ckpt', str(self.lr))))\n                    c.send(','.join(('ckpt', str(self.lr))).encode('utf-8'))\n                    event.set()\n                    logging.debug('[Server]: checkpoint command sent')\n                    continue\n\n                # ask BO to suggest the next LR \n                if self.lr_counter == self.lr_steps:\n                    logging.debug('ring_buffer: {}'.format(self.ring_loss_buffer.get()))\n                    if any([math.isnan(x) for x in self.ring_loss_buffer.get()]):\n                        predicted_loss = \"nan\"\n                    elif self.val_stage:\n                        predicted_loss = self.ring_loss_buffer.exponential_forcast(pred_index=int(self.exploitation_step/self.val_freq), is_training=False)\n                        current_loss = sum(self.ring_loss_buffer.get()[-1:])/1.0\n                    else:\n                        predicted_loss = self.ring_loss_buffer.exponential_forcast(pred_index=self.exploitation_step, is_training=True)\n                        current_loss = sum(self.ring_loss_buffer.get()[-10:])/10.0\n\n                    logging.info('[Server]: predicted loss: {} due to LR {}'.format(predicted_loss, self.lr))\n\n                    # Huge loss jump can make the exponential prediction inaccurate, so set a threshold here. \n                    #if self.loss_after_exploitation is not None and max(self.ring_loss_buffer.get()) > 10 * self.loss_after_exploitation:\n                    #    predicted_loss = current_loss \n                    #    logging.info('New predicted_loss: ' + str(predicted_loss))\n\n                    if self.loss_after_exploitation is not None and max(self.ring_loss_buffer.get()) >= 1.0 * self.init_loss and self.val_stage:\n                        predicted_loss = current_loss \n                        logging.info('[Server]: New predicted_loss: ' + str(predicted_loss))\n\n                    if self.val_stage:\n                        self.ring_loss_buffer = RingBuffer(int(math.floor(self.ring_buffer_len)/self.val_freq))\n                    else:\n                        self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n\n                    # feed a (LR, predicted loss in tau steps) instance to BO.\n                    if str(predicted_loss) == 'nan':\n                        self.opt.tell([float(self.lr)], 1e6)\n                    else:\n                        self.opt.tell([float(self.lr)], predicted_loss)\n                    self.x_iters.append(float(self.lr))\n                    self.func_val_iters.append(predicted_loss)\n                    self.x_func_dict[self.lr] = predicted_loss\n                    self.lr_counter = 1\n\n                    if len(self.func_val_iters) == self.lr_to_explore:\n                        min_index = self.func_val_iters.index(min(self.func_val_iters))\n\n                        # log the best lr found for the next stage.\n                        logging.info('[Server]: best LR: {}, min loss: {}'.format(self.x_iters[min_index], self.func_val_iters[min_index]))\n\n                        self.lr = self.x_iters[min_index]\n                        self.message = str(','.join(('restore', str(self.lr))))\n                        c.send(','.join(('restore', str(self.lr))).encode('utf-8'))\n                        event.set()\n                        logging.debug('[Server]: restore command sent')\n                        self.exploitation_flag = True\n\n                        self.func_val_iters = []\n                        self.x_iters = []\n                        self.x_func_dict = dict()\n                    else:\n                        # Ask BO for the next LR to explore\n                        self.lr = self.opt.ask()[0]\n                        while True:\n                            if self.lr in self.x_func_dict:\n                                self.opt.tell([self.lr], self.x_func_dict[self.lr])\n                                self.lr = self.opt.ask()[0]\n                            else:\n                                break\n                        self.message = str(','.join(('restore', str(self.lr))))\n                        c.send(','.join(('restore', str(self.lr))).encode('utf-8'))\n                        event.set()\n                        logging.debug('[Server]: restore command sent')\n                else:\n                    self.lr_counter += 1\n                    if self.val_stage and self.lr_counter % self.val_freq == 0:\n                        self.message = \"evaluate\"\n                        c.send(self.message.encode('utf-8')) \n                    else:\n                        self.message = str(self.lr)\n                        c.send(str(self.lr).encode('utf-8')) \n                    event.set()",
    "Experiment Result": "AutoLRS Parameters (autolrs_callback.py):\n- `listening_host`: 'localhost'\n- `listening_port`: 12315\n- `warmup_steps`: 0\n- `warmup_lr`: 0\n- `summary_steps`: 1\n\nAutoLRS Controller Constants (autolrs_server.py):\n- `EXPLOITATION_STEP`: 1000 (initial τ, training stage steps)\n- `LR_STEPS`: 100 (initial τ', steps for LR candidate evaluation)\n- `RING_BUFFER_LEN`: 100 (length of loss series for exponential forecasting)\n- `LR_TO_EXPLORE`: 10 (number of LR candidates to explore per BO stage)\n- `TAU_MAX`: 8000 (maximum training stage steps)\n- `min_lr`: Configured via command line argument (e.g., `--min_lr`)\n- `max_lr`: Configured via command line argument (e.g., `--max_lr`)\n- `val_freq`: `int(self.lr_steps/16)` (frequency for validation loss evaluation when `val_stage` is active)\n- `Bayesian Optimization (BO)`: Uses `skopt.Optimizer` with `\"GP\"` (Gaussian Process) as the surrogate model, `n_initial_points=1`, and `acq_func='LCB'` (Lower Confidence Bound) with `acq_func_kwargs={'kappa':1e6}`.\n- `Exponential Forecasting`: `exp_forecast` function uses `scipy.optimize.fmin` to find `b` for `L(t) = a*exp(b*t) + c` and `np.linalg.lstsq` for `a` and `c`.\n- `Spline Smoothing`: `spline_iter` function uses `UnivariateSpline` with `spline_deg=2` and an iterative outlier removal process (filter_ratio=0.03, num_of_iter=10 for training loss, 1 for validation loss).\n- `Curriculum Learning for τ and τ'`: `lr_steps` (τ') and `exploitation_step` (τ) are doubled when `lr_steps < tau_max / 10`.\n- `Validation/Training Loss Approximation`: Uses `is_training=True` for exponential forecasting in early stages (or `val_stage=False`) and `is_training=False` for validation loss-based forecasting when `val_stage=True`. A small subset of the validation set (VAL_LEN=10 mini-batches) is used for validation loss evaluation.\n\nCIFAR-10 Training Setup (cifar10_example.py):\n- `Device`: 'cuda' if available, else 'cpu'\n- `Dataset`: CIFAR-10\n- `Data Preprocessing (Train)`: RandomCrop(32, padding=4), RandomHorizontalFlip(), ToTensor(), Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n- `Data Preprocessing (Test)`: ToTensor(), Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n- `DataLoader`: `trainloader` (batch_size=128, shuffle=True, num_workers=2), `testloader` (batch_size=100, shuffle=False, num_workers=2), `valloader` (batch_size=128, shuffle=False, num_workers=2) on a subset of `testset` of `VAL_LEN=10`.\n- `Model`: `VGG('VGG16')` (other models like ResNet18, MobileNetV2 etc. are commented out).\n- `Optimizer`: `optim.SGD` with `lr=0.1` (initial value, managed by AutoLRS), `momentum=0.9`, `weight_decay=5e-4`.\n- `Loss Function`: `nn.CrossEntropyLoss()`\n- `Training Epochs`: 350 * 2 (stated as 'multiply 2 to your original training epochs because the search steps of AutoLRS is the same as the actual training steps').\n- `Model Saving`: Best model checkpoint saved to `./checkpoint/best_ckpt.pth` based on test accuracy."
}{
    "Title": "Reverse engineering learned optimizers reveals known and novel mechanisms",
    "Main Contributions": "The paper addresses the mystery of how learned optimizers outperform baselines, investigating whether they learn sophisticated combinations of existing techniques or entirely new behaviors. It reveals that learned optimizers learn interpretable behaviors including momentum, gradient clipping, learning rate schedules, and learning rate adaptation, and demonstrates how internal dynamics orchestrate these computations. The work elucidates the inner workings of learned optimizers and establishes tools for their interpretation.",
    "Methodology": "Learned optimizers are parameterized by Recurrent Neural Networks (RNNs), specifically Gated Recurrent Units (GRUs), with the gradient as the only input. They are trained by minimizing a meta-objective (average training loss) using meta-optimization, involving backpropagation through fully unrolled computational graphs (up to 200 iterations). Two main analytical tools are developed: (1) **Update functions**: visualizing the scalar function mapping gradient (g) to parameter change (∆x), where the slope represents the effective learning rate. (2) **Dynamical systems perspective**: linearizing the nonlinear RNN dynamics near approximate fixed points to understand state dynamics. Eigenvalues and eigenvectors of the Jacobian matrix at these fixed points are used to interpret mechanisms like momentum timescales and corresponding learning rates.",
    "Experimental Setup": "Learned optimizers were trained and analyzed on four diverse tasks: linear regression (random quadratic loss surfaces), Rosenbrock function (2-parameter non-convex), training a 3-hidden layer fully connected neural network on the two moons dataset (8,577 parameters), and training a 4-layer convolutional network on the MNIST dataset (82,250 parameters, stochastic gradients, minibatch size 100). Baseline optimizers (Momentum, RMSProp, Adam) were individually tuned for each task by sampling 2500 hyperparameter combinations. Meta-training used Adam as the meta-optimizer with specific hyperparameter tuning, gradient clipping (value of 5), exponential learning rate decay, and L2-regularization (10^-5) over 5000 steps on a TPUv2 core.",
    "Limitations": "The paper lacks a holistic picture of how the isolated mechanisms (momentum, gradient clipping, schedules, adaptation) are stitched together. Some findings, such as parameter type-specific tuning (e.g., different learning rates for weights vs. biases), are correlative rather than causally established, meaning observed separations in trajectories might be due to factors correlated with depth or parameter type. Additionally, for the MNIST task, a non-monotonic update function was consistently found, whose functional benefit remains unexplained. The choice of simple tasks for meta-optimization, while practical, may limit generalizability to more complex or highly stochastic real-world problems.",
    "Future Research Directions": "Future work could focus on extracting or distilling a compressed optimizer from the discovered mechanisms, potentially using data-driven techniques or symbolic regression. Another direction is to study the generalization capabilities of learned optimizers by mapping specific mechanisms to quantitative properties of loss surfaces (e.g., curvature, convexity). Understanding these relationships would enable applying learned optimizers trained in one setting to new, unseen problems effectively.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Mechanic: A Learning Rate Tuner",
    "Main Contributions": "This paper introduces \"mechanic\", an automatic learning rate scale factor tuner designed for any base optimization algorithm and schedule. It provides a practical realization of recent theoretical reductions from online convex optimization, aiming to eliminate the extensive and expensive manual tuning process. The main finding is that mechanic either comes very close to, matches, or even improves upon manual tuning of learning rates across a range of large-scale deep learning tasks, significantly reducing computational cost and time.",
    "Methodology": "Mechanic is implemented as a generic wrapper around any base optimization algorithm. It leverages online convex optimization (OCO) theory, specifically a scheme from [16], using a parameter-free \"meta\" OCO algorithm (the tuner) to automatically determine the scalar scale factor `st`. The wrapped algorithm's iterates are set as `xt = x_base_1 + st * (x_base_t - x_base_1)`. The tuner algorithm, a modified version of a theoretically tractable procedure (Algorithm 2), receives a gradient-like input `ht = ⟨gt, x_base_t - x_base_1⟩`, which is further modified in practice to include a weight-decay-esque term: `ht = ⟨gt + λ∥gt∥(Pn i=1 st,i)xt ∥xt∥ , ∆t⟩`. It uses exponentially weighted moving averages (controlled by `β` values) to introduce recency bias, and multiple `β` values are combined for robust performance. The algorithm is designed to be scale-free, meaning rescaling gradients by a constant scalar has no effect on `st`.",
    "Experimental Setup": "Mechanic was rigorously evaluated on a variety of large-scale deep learning tasks and settings. This included BERT pre-training for Masked Language Modeling on the Wikibooks dataset and finetuning on the 5 largest datasets from the GLUE suite (MNLI, QNLI, SST-2, QQP) using BERT-B (110M) and BERT-L (340M) models. For image classification, experiments covered training ResNet-18 and WRN-40-10 from scratch on CIFAR-10/100, and transfer learning with ViT-B/16 (86M) and ViT-L/16 (307M) models pre-trained on JFT-300M and finetuned on ImageNet, CIFAR-10, and CIFAR-100. Base optimization algorithms tested included AdamW, Lion, and SGD with momentum, with varying batch sizes and learning rate schedules. Performance was measured by MLM accuracy, GLUE dev set accuracy (peak validation scores averaged over 3 runs), and image classification accuracy (averaged over 3 runs). Mechanic was compared against manually tuned baselines and the D-adaptation algorithm on selected tasks, including 12 convex logistic regression problems. Default `λ` was 0.01, and `sinit` was `10^-8`.",
    "Limitations": "The current implementation of mechanic does not work well in combination with dropout [42]. The authors hypothesize there might be a more fundamental reason for this incompatibility beyond noise from sampling. Additionally, mechanic optimizes for train loss because it re-purposes the gradient from the training set to learn the learning rate. This contrasts with manual learning rate tuning, which typically relies on validation set performance, highlighting a discrepancy that needs a principled solution.",
    "Future Research Directions": "Several promising directions for future research are suggested: 1. Investigating a theoretical motivation for the peculiar weight-decay-esque term incorporated into the algorithm. 2. Exploring the possibility of leveraging similar techniques to learn a per-layer scale factor. Such a capability, by allowing more degrees of freedom, could significantly outperform current baselines, as manually tuning scale factors for every layer is currently infeasible. 3. Developing a principled way to handle the discrepancy between mechanic's optimization for train loss and the common practice of manual tuning based on validation set performance.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "MoMo: Momentum Models for Adaptive Learning Rates",
    "Main Contributions": "The paper introduces MoMo (Momentum Models) and MoMo-Adam, novel adaptive learning rates designed to reduce the computational cost of hyperparameter tuning, particularly for the learning rate, in modern machine learning architectures. Key contributions include deriving adaptive learning rates for SGD-M and Adam by modeling the loss function using momentum estimates of past losses and gradients. This model incorporates known or estimated lower bounds of the loss function. The methods achieve an O(1/√K) convergence rate for convex problems with interpolation and are shown to significantly improve robustness to learning rate choices across diverse machine learning tasks, often performing automatic learning rate warm-up and decay.",
    "Methodology": "The core methodology is based on a model-based stochastic optimization viewpoint, where momentum is re-interpreted as a model of the objective function built by averaging sampled loss functions and their linearizations. A key innovation is the use of a 'truncated model' that imposes a lower bound (fk*) on this averaged function, commonly zero or an online estimated value. The MoMo update rule (Algorithm 1) is derived by approximately minimizing this truncated model plus a proximal term, yielding a closed-form solution for the step size. Exponentially weighted averages are used for computing momentum coefficients. MoMo-Adam (Algorithm 2) extends MoMo to Adam by incorporating Adam's diagonal preconditioner and bias-corrected averaging. For cases with unknown non-zero lower bounds, the paper introduces an online estimation technique (MoMo*) for fk* based on available information from momentum-based methods.",
    "Experimental Setup": "The methods were evaluated across a wide range of tasks and models: image classification using ResNet110 (CIFAR100), ResNet20, VGG16, ViT (CIFAR10), MLP (MNIST), ResNet18 (Imagenet32, Imagenet-1k), and ViT (Imagenet-1k); recommender systems with DLRM on the Criteo dataset; an encoder-decoder transformer for German-to-English translation on IWSLT14; and a diffusion model (UNet architecture) on the Smithsonian Butterflies dataset. Comparisons were made against SGD-M, Adam, AdamW, AdaBelief, AdaBound, and Lion. Experiments included sweeps over constant learning rates or base learning rates for schedules (warm-up with cosine decay or inverse square-root decay), evaluating final training loss, validation accuracy (top-1), and BLEU score. Results were averaged over 3 to 5 random seeds, typically with momentum parameters β=0.9 and (β1, β2)=(0.9, 0.999). Most experiments used no weight decay (λ=0), with some exceptions for transformers and ViT Imagenet-1k.",
    "Limitations": "The theoretical convergence rate of O(1/√K) is specifically shown for convex problems with interpolation and requires knowledge of the optimal value (f*) in the proof. A theoretical analysis demonstrating an improvement from momentum (β > 0) within this framework is not provided. For tasks where the optimal loss value (f*) is significantly above zero, the default lower bound estimate (fk* = 0) is insufficient, necessitating the use of the more complex online estimation (MoMo*), which includes a 'ResetStar' routine to prevent step size collapse. Empirical improvements for fine-tuned transformer tasks were observed to be less substantial compared to other problem settings.",
    "Future Research Directions": "The paper suggests extending the MoMo framework to other preconditioned momentum methods beyond SGD-M and Adam, such as AdaBelief or AdaBound. Further theoretical analysis is needed to rigorously establish the benefits of momentum within the MoMo framework, especially in non-convex optimization settings. Improving the robustness and reducing the setup complexity of the online lower bound estimation (MoMo*) could be another area of research. Deeper investigation into the automatic warm-up and decay properties of MoMo's adaptive step sizes could also lead to new insights for learning rate scheduling.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Where Do Large Learning Rates Lead Us?",
    "Main Contributions": "The paper empirically studies the impact of initial learning rates (LRs) on neural network generalization. It identifies a narrow range of initial LRs (subregime 2A), just above the convergence threshold, that leads to optimal generalization after fine-tuning or weight averaging. This optimal range results in the optimization locating a basin containing only high-quality minima and fosters sparse learning of the most relevant features (e.g., mid-frequencies in image classification). Conversely, too-small LRs lead to unstable minima and attempts to learn all features, while too-large LRs fail to detect good solution basins and extract meaningful patterns. The study confirms these findings transfer to conventional neural network training setups.",
    "Methodology": "The study uses a controlled setting with fully scale-invariant models trained via projected SGD on a sphere of fixed radius to ensure fixed effective LRs. Training is divided into two stages: pre-training with different fixed LRs (PLRs) to stabilize dynamics, followed by post-training via fine-tuning with small LRs (FLRs) or Stochastic Weight Averaging (SWA). The analysis is structured around a three-regime taxonomy (convergence, chaotic equilibrium, divergence), further dividing the chaotic equilibrium regime into optimal (2A) and suboptimal (2B) subregimes. Local geometry of minima is analyzed using angular distance and linear connectivity (path barrier). Feature learning is investigated using a synthetic example with controlled features and, for real-world images, by analyzing accuracy on frequency bands of 2D Discrete Fourier Transform (DFT). Findings are validated in a practical setting using conventional ResNet-18 and Vision Transformer architectures with standard optimizers and augmentations.",
    "Experimental Setup": "Experiments primarily use scale-invariant (SI) ResNet-18 and ConvNet architectures on CIFAR-10 and CIFAR-100 datasets. A 3-layer MLP is used for a synthetic binary classification task with 32-dimensional data. For practical validation, a standard ResNet-18 and a ViT small (ViT-S/4) are used with CIFAR-10, CIFAR-100, and Tiny ImageNet. All networks are trained with SGD (batch size 128 for main, 32 for synthetic; SGD with momentum 0.9, weight decay 5e-4, and standard augmentations for practical ResNet; Adam with weight decay 1e-4, batch size 512 for ViT). Pre-training and fine-tuning stages typically run for 200 epochs (40000/20000 iterations for synthetic, 500 epochs for ViT). Validation involves test accuracy, local geometry metrics (angular distance, linear connectivity barrier), and feature learning analysis (single-feature accuracy, frequency band accuracy). Experiments are conducted on NVIDIA TESLA V100 and A100 GPUs, totaling approximately 1500-2000 GPU hours, with paper-specific experiments taking ~1000 GPU hours. The synthetic example uses 50 random seeds for statistical significance.",
    "Limitations": "The study is primarily empirical and lacks direct theoretical support. Its conclusions are based on specific setups involving particular datasets (image or synthetic) and neural network architectures (convolutional, MLP, or Vision Transformer), meaning findings may not consistently generalize to all other settings. The research may also overlook the impact of other neural network invariances, such as the rescale invariance of homogeneous activations (e.g., ReLU).",
    "Future Research Directions": "Future work should aim to establish theoretical support for the empirical findings. Key directions include investigating the precise relationship between feature sparsity and loss landscape geometry, specifically how learning a subset of features corresponds to localizing certain regions in the loss landscape and how feature properties relate to basin characteristics like sharpness. Research could also explore whether feature sparsity acts as a regularizer leading to simpler models. Further studies are needed to understand the impact of learning rates on training in more complex practical scenarios, particularly those involving spurious features or benign memorization, and their influence on generalization. Addressing the identified limitations regarding broader generalization and other NN invariances is also crucial.",
    "Experiment Code": "File Path: nets/convnet_si.py\nContent:\ndef block(input, output):\n    module_list = [\n        nn.Conv2d(input, output, kernel_size=3, stride=1, padding=1, bias=False),\n        nn.BatchNorm2d(output, affine=False),\n        nn.ReLU(),\n        nn.MaxPool2d(2)\n    ]\n    return module_list\n\nclass ConvNetDepth(nn.Module):\n    def __init__(self, init_channels=64, num_classes=10, max_depth=3, init_scale=-1,\n                 su_init=False):\n        super(ConvNetDepth, self).__init__()\n        # ... (other layers)\n        if su_init:\n            self._su_init()\n\n    def _su_init(self):\n        params = [p for n, p in self.named_parameters() if check_si_name(n, 'ConvNetSI')]  # BN-ed params\n        N = sum(p.numel() for p in params)\n        pnorm = sum((p ** 2).sum() for p in params) ** 0.5\n\n        vec = torch.randn(N, device=params[0].device)\n        vec /= torch.norm(vec)\n        vec *= pnorm\n\n        tensors = unflatten_like(vec, params)\n        for param, tensor in zip(params, tensors):\n            param.data = tensor\n\nFile Path: nets/resnet_si.py\nContent:\nclass PreActBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, **kwargs):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes, affine=False)\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, affine=False)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.BatchNorm2d(in_planes, affine=False),\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n            )\n\nFile Path: training_utils.py\nContent:\ndef fix_si_pnorm(model, si_pnorm_0, model_name=\"ResNet18\"):\n    si_pnorm = np.sqrt(sum((p ** 2).sum().item() for n, p in model.named_parameters() if check_si_name(n, model_name)))\n    p_coef = si_pnorm_0 / si_pnorm\n    for n, p in model.named_parameters():\n        if check_si_name(n, model_name):\n            p.data *= p_coef\n\ndef fix_si_pnorms(model, si_pnorm_0, model_name=\"ResNet18\"):\n    for n, p in model.named_parameters():\n        if check_si_name(n, model_name):\n            si_pnorm = torch.norm_except_dim(p.data, 2, 0)\n            p_coef = si_pnorm_0 / si_pnorm\n            p.data *= p_coef\n\nFile Path: train.py\nContent:\ndef train_epoch(\n    loader,\n    model,\n    criterion,\n    optimizer,\n    cuda=True,\n    regression=False,\n    verbose=False,\n    subset=None,\n    fbgd=False,\n    save_freq_int = 0,\n    epoch=None,\n    output_dir = None,\n    fix_elr = False,\n    fix_all_elr = False,\n    si_pnorm_0 = None,\n    model_name = 'ResNet18'\n):\n    # ... (training loop details)\n    for i, (input, target) in enumerate(loader):\n        # ... (loss calculation and backward pass)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if fix_elr and si_pnorm_0 is not None:\n            fix_si_pnorm(model, si_pnorm_0, model_name)\n\n        if fix_all_elr and si_pnorm_0 is not None:\n            fix_si_pnorms(model, si_pnorm_0, model_name)\n\nFile Path: nets/utils.py\nContent:\nclass WeightNorm(nn.Module):\n    # ... (WeightNorm implementation)\n    def _setweights(self):\n        g = getattr(self.module, self.name_g)\n        v = getattr(self.module, self.name_v)\n        w = v * g / (norm_except_dim(v, 2, self.dim) + self.eps)\n        setattr(self.module, self.name, w)\n\nFile Path: get_info.py\nContent:\ndef calc_cos(params_1, params_2):\n    # ... (cosine similarity calculation)\n\ndef calc_l2(params_1, params_2):\n    # ... (L2 distance calculation)\n\nFile Path: get_info_funcs.py\nContent:\ndef eval_eigs(model, dataloader, fisher=True, train_mode=False, nsteps=10, return_evecs=False, pnames=None,\n              criterion=cross_entropy):\n    # ... (eigenvalue calculation via Lanczos)\n\ndef eval_trace(model, dataloader, fisher=True, train_mode=False, n_vecs=5, pnames=None,\n               criterion=cross_entropy):\n    # ... (trace calculation)\n\ndef Fvp(vec, params, outputs, data_size, **kwargs):\n    # ... (Fisher-vector product calculation)\n\ndef Hvp(vec, params, outputs, data_size, targets, criterion=cross_entropy, **kwargs):\n    # ... (Hessian-vector product calculation)\n\nFile Path: fake.py\nContent:\nclass FakeData(VisionDataset):\n    def __init__(self, size=1000, image_size=(3, 224, 224), num_classes=10,\n                 transform=None, target_transform=None, random_offset=0):\n        # ... (initialization of random data and targets)\n\nFile Path: nets/resnet.py\nContent:\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10, init_channels=64):\n        # ... (conventional ResNet architecture definition)\n",
    "Experiment Result": "The study employs a controlled experimental setting with the following configurations:\n\n**1. Model Architectures & Scale-Invariance:**\n*   **Scale-Invariant Models**: `ConvNetSI` (defined in `nets/convnet_si.py`) and `ResNet18SI` (defined in `nets/resnet_si.py`). These models implement scale-invariance by setting `affine=False` in `nn.BatchNorm2d` layers. Some `ConvNet` variants (`ConvNetSICI3WN`, `ConvNetSICI3WN2` from `nets/convnet_si_ci3_wn.py`, `nets/convnet_si_ci3_wn2.py`, utilizing `nets/utils.py`) use `WeightNorm` to enforce fixed weight norms, contributing to scale-invariance. The specific parameters considered scale-invariant are identified by `check_si_name` functions across `get_info.py`, `nb_utils.py`, `train.py`, and `training_utils.py` (e.g., convolutional weights but not linear layer or BN affine parameters).\n*   **Conventional Architectures**: Standard `ResNet18` (from `nets/resnet.py`) and `ConvNet` architectures are used for practical validation, with standard optimizers and augmentations.\n*   **Width and Depth**: Configurable via `--num_channels` (e.g., 32 for ConvNetSI) and `--depth` (e.g., 3 for ConvNetSI).\n\n**2. Training Dynamics (Projected SGD & Fixed Effective LRs):**\n*   **Projected SGD**: Training is performed via projected SGD. This is achieved by explicit norm manipulation (`fix_si_pnorm` and `fix_si_pnorms` functions in `training_utils.py`), which are called after each `optimizer.step()` to project scale-invariant parameters back onto a sphere of fixed radius.\n*   **Initialization on Sphere**: Scale-invariant models like `ConvNetSI` can be initialized uniformly on a sphere using the `--su_init` flag, which triggers the `_su_init` method in `nets/convnet_si.py`.\n*   **Fixed Effective Learning Rates (ELRs)**: The `--fix_elr` or `--fix_all_elr` flags in `train.py` enable training with fixed ELRs. The target ELR value is set by `--elr`. The learning rate for non-scale-invariant parameters can be independently set via `--noninvlr` (e.g., 0.0 to fix them).\n*   **Optimizer**: Stochastic Gradient Descent (SGD) is used, with configurable `momentum` and `weight_decay` (`--wd`). Full-batch GD can be enabled with `--fbgd`.\n*   **Learning Rate Schedules**: Various schedules are supported: no schedule (`--no_schedule`), continuous linear decay (`--c_schedule`), discrete step decay (`--d_schedule`), and cosine annealing (`--cosan_schedule`).\n*   **Epochs**: Training duration is set by `--epochs` (e.g., 3 for test runs, 1001 for full runs). Training can be resumed from a specific epoch using `--resume_epoch`.\n*   **Loss Function**: Cross-entropy (`CE`) is the default, but squared loss can be used with `--use_squared_loss`.\n\n**3. Analysis of Local Geometry & Regimes:**\n*   **Angular Distance & Linear Connectivity (Path Barrier)**: Angular distance (cosine similarity between parameters, `calc_cos` in `get_info.py`) and L2 distance (`calc_l2` in `get_info.py`) are computed. Local geometry is further analyzed by calculating eigenvalues (`eval_eigs`) and traces (`eval_trace`) of the Fisher Information Matrix (`Fvp`) or Hessian matrix (`Hvp`), controlled by `--fisher_evals`, `--fisher_trace`, `--hess_evals`, `--hess_trace` flags in `parser_get_info.py`. The number of Lanczos steps for eigenvalue approximation is 10 (`nsteps`).\n\n**4. Feature Learning Investigations:**\n*   **Synthetic Example**: Investigated using a synthetic dataset generated by the `FakeData` class (from `fake.py`). This dataset creates random images and targets.\n*   **Real-world Images**: Evaluated on datasets such as `CIFAR10`, `CIFAR100`, `SVHN`, and `STL10` (loaded by `data.py`).\n*   **2D DFT Analysis**: The provided repository content does not contain direct implementations for analyzing accuracy on frequency bands of 2D Discrete Fourier Transform (DFT). This analysis might be performed externally on the collected model outputs or features.\n\n**5. Practical Setting Validation & Data Management:**\n*   **Datasets**: CIFAR10 is the default, but others like CIFAR100, SVHN, STL10 are supported. Data is loaded from a specified `data_path`. \n*   **Data Augmentation**: Can be enabled (default) or disabled with `--no_aug`.\n*   **Data Size**: The number of training examples can be limited using `--use_data_size`.\n*   **Label Noise**: Training data can be corrupted with label noise via `--corrupt_train` (fraction of noisy labels).\n*   **Class Splitting**: For CIFAR-10, classes can be split into subsets using `--split_classes`.\n*   **Initialization Consistency**: Options `--same_init` (for full model init) and `--same_last_layer` (for last layer only) ensure consistent initialization across trials.\n*   **Last Layer Initialization Scale**: The norm of the last layer weights can be customized with `--init_scale`.\n*   **Batch Size**: Configurable via `--batch_size` (default: 128).\n*   **Evaluation Metrics**: Model quality (loss and accuracy) is evaluated using `eval_model`. Other metrics gathered include parameter norms (`--all_pnorm`), gradient means/standard deviations (`--calc_grads`), gradient norms (`--calc_grad_norms`), batch outliers (`--calc_batch_outliers`), and probability statistics (`--calc_probs_stats`).\n*   **Hardware & Reproducibility**: Experiments are run on specified `--gpu` devices with a fixed `--seed` for reproducibility. Multiple runs can be managed with `--trial` numbers. Data loading is parallelized with `--num_workers`."
}{
    "Title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic",
    "Main Contributions": "The research rigorously investigates dataset contamination in Large Language Models (LLMs) for mathematical reasoning, by commissioning a novel benchmark called Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark but is guaranteed to be free from training data contamination. Key findings include observed accuracy drops of up to 8% for leading LLMs on GSM1k compared to GSM8k, indicating systematic overfitting in several model families (e.g., Phi, Mistral, Llama). The study also identifies a positive correlation (Spearman’s r2 = 0.36) between a model’s probability of generating examples from GSM8k and its performance gap, suggesting partial memorization. Nevertheless, frontier models show minimal signs of overfitting, and all models broadly demonstrate generalization to novel math problems.",
    "Methodology": "The study involved creating GSM1k with 1205 grade school math problems by human annotators without LLM assistance, ensuring no data contamination. The dataset underwent three layers of quality checks (manual review, independent solve for correctness, general quality audit). Difficulty matching with GSM8k was achieved by controlling resolution steps and answer magnitude distributions, validated by human distinguishability tests, human solve rates, and LLM solve rates for pre-2021 models. Models were evaluated using a modified EleutherAI’s LM Evaluation Harness with standardized 5-shot prompting (randomly drawn GSM8k train examples) and temperature 0 for open-source models. Performance gaps between GSM8k and GSM1k were analyzed, and a correlation was sought between this gap and the per-character log-likelihood of generating GSM8k test set examples (as a proxy for memorization). Ablations explored alternative prompting formats and varying numbers of few-shot examples.",
    "Experimental Setup": "The evaluation utilized two benchmarks: the established GSM8k and the newly constructed GSM1k. A wide range of leading open-source and closed-source LLMs were assessed, including GPT-4, Gemini, Claude, Mistral, Llama, Phi, Yi, Xwin, Gemma, CodeLlama, DBRX, and Pythia. Experiments were conducted on a cluster with 8 x A100 nodes. vLLM was used for accelerated inference for compatible open-source models, while standard HuggingFace libraries were used for others. Proprietary models were queried via the LiteLLM library. The evaluation period spanned from April 16 to July 10, 2024. Validation included various ablations, such as using an alternative prompting scheme with non-GSM8k examples and varying the number of few-shot examples (1 to 10). Human annotation was also used to extract answers for a subset of models to assess the impact of automatic extraction errors.",
    "Limitations": "The study acknowledges that GSM8k and GSM1k, despite extensive efforts, are only highly similar and not identically distributed. Functional evaluations, while useful, cover only a small portion of the problem space by generating numerical variations of existing problems. The estimation of problem difficulty, based on calculator tags, is a rough approximation and may not directly correlate with true difficulty. Reported GSM8k performance numbers might be lower than typical benchmarks due to the use of a standardized evaluation setting rather than optimized prompts. The analysis indicates that data contamination might not be the sole cause of overfitting, with outliers suggesting other indirect factors like similar data collection or benchmark-based model checkpoint selection. The paper does not discuss potential negative societal impacts or specify full training details for the evaluated models.",
    "Future Research Directions": "The paper does not intend to publicly release GSM1k immediately to prevent future data contamination but commits to recurring evaluations of major LLM releases and open-sourcing its evaluation code. GSM1k will be fully released when three different open-source foundational models achieve 95% accuracy on it or by June 2025, whichever comes first. The authors encourage future benchmarks to adopt a similar strategy of delayed public release. Implicitly, further research could investigate the various indirect means of overfitting beyond direct data contamination, as suggested by outlier models. Additionally, understanding how sufficiently strong LLMs develop elementary reasoning capabilities, even when exposed to benchmark data, remains an area for exploration.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Language models are multilingual chain-of-thought reasoners",
    "Main Contributions": "Introduced MGSM, the first multilingual arithmetic reasoning benchmark, derived from GSM8K and translated into ten typologically diverse languages. Demonstrated that multilingual chain-of-thought (CoT) prompting enables large language models (PaLM-540B, GPT-3) to perform complex multi-step reasoning across multiple languages, including underrepresented ones, with performance improving with model scale (an emergent ability). Found that English CoT (EN-COT) consistently yields competitive or superior results compared to native language CoT (NATIVE-COT) for arithmetic problems. Extended these findings to other tasks: PaLM-540B achieved a new state-of-the-art on XCOPA for causal commonsense reasoning and competitive performance on XL-WiC for word-in-context semantic judgment using few-shot multilingual CoT prompting. Observed that performance does not strongly correlate with language frequency in pre-training data for large models, suggesting effective cross-lingual knowledge transfer.",
    "Methodology": "Evaluated several prompting strategies including 'Direct Answer Prediction' (DIRECT), 'Native Language Chain-of-Thought' (NATIVE-COT), 'English Chain-of-Thought' (EN-COT), and 'Translate to English then English Chain-of-Thought' (TRANSLATE-EN). These were combined with few-shot exemplar selection methods: 'Native Question Exemplars' (NATIVE-EXEMPLARS), 'English Question Exemplars' (ENGLISH-EXEMPLARS), and 'Generic Multilingual Question Exemplars' (MULTILINGUAL-EXEMPLARS). Manual translation by professional native speakers was used for MGSM, and Google Translate API for the TRANSLATE-EN strategy. Model outputs were generated using greedy decoding (temperature τ = 0).",
    "Experimental Setup": "The study utilized the newly introduced Multilingual Grade School Math (MGSM) benchmark, comprising 250 grade-school math problems manually translated from GSM8K into ten typologically diverse languages (Bengali, Chinese, French, German, Japanese, Russian, Spanish, Swahili, Telugu, Thai). Additionally, experiments extended to XCOPA for multilingual causal commonsense reasoning (11 languages) and XL-WiC for multilingual word-in-context semantic judgment (13 languages). The primary models evaluated were GPT-3 (text-davinci-002) and PaLM-540B, with Codex (code-davinci-002) also used for XCOPA and XL-WiC tasks. Performance was measured by accuracy in few-shot settings, using validation sets for exemplars in XCOPA and XL-WiC. The number of exemplars for GPT-3 varied based on token limits per language.",
    "Limitations": "The GPT-3 API's 2048 token limit restricted the number of few-shot exemplars for some underrepresented languages due to longer tokenizations. Chain-of-thought prompting did not demonstrate an improvement over direct answer prediction for the XL-WiC task, suggesting sub-optimal prompts or that the task itself may not intrinsically benefit from explicit intermediate reasoning steps. MGSM answers were restricted to Arabic numerals for consistency, potentially overlooking nuances of cross-lingual numeracy with different numeral systems. The authors also noted that prompts for XL-WiC might not be optimal and could benefit from broader investigation.",
    "Future Research Directions": "Investigate cross-lingual numeracy by mapping Arabic numerals to corresponding script numerals. Further explore the reasons why Chain-of-Thought prompting did not improve performance on XL-WiC, including experimenting with different prompt formats and assessing if the task truly benefits from intermediate reasoning steps. Consider English intermediate steps (EN-COT) as a useful baseline for future multilingual reasoning research given its consistent competitive performance. Continue scaling large language models to potentially further enhance their multilingual reasoning capabilities, leveraging its emergent property with scale. Broader investigation and optimization of prompt formats, especially for tasks where CoT did not show expected improvements.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
    "Main Contributions": "The paper introduces OpenMathInstruct-1, a large-scale (1.8M problem-solution pairs) math instruction tuning dataset generated using the permissively licensed Mixtral model, addressing the limitations of prior datasets that rely on closed-source LLMs. A novel prompting technique involving masked text solutions, combined with brute-force scaling, enabled the creation of this high-quality dataset, achieving 99.9% training set coverage on GSM8K and 93% on MATH. Models fine-tuned on OpenMathInstruct-1, such as OpenMath-CodeLlama-70B, demonstrate competitive performance (84.6% on GSM8K and 50.7% on MATH) compared to state-of-the-art GPT-distilled models. The dataset, code, and trained models are released under a commercially permissive license to support open-source efforts.",
    "Methodology": "The methodology centers on synthesizing code-interpreter style solutions for GSM8K and MATH using the Mixtral 8x7B LLM through few-shot prompting. The key innovation is the \"Masked Text Solution Prompting\" strategy, where intermediate numerical computations in reference text solutions are replaced with symbols (e.g., M, N) within the prompt to prevent the LLM from generating shortcut solutions. This is an improvement over direct use of reference text solutions. Other prompting strategies explored include brute-force sampling (e.g., 128-224 solutions per problem) and subject-specific prompting for MATH. Generated solutions are in a code-interpreter format, interweaving natural language reasoning with Python code blocks and their execution outputs. Post-processing steps clean syntactically noisy solutions. Data selection strategies include \"Fair Downsampling\" to balance problem representation and \"Any-Code\" filtering to prioritize code-based solutions, forming a 1.02M instance fine-tuning corpus. Models (Mistral-7B, Llama 2, CodeLlama variants) are trained for 2-4 epochs using the AdamW optimizer.",
    "Experimental Setup": "The training data for OpenMathInstruct-1 was synthesized from the training sets of GSM8K and MATH benchmarks using the Mixtral 8x7B model. Fine-tuning experiments were conducted on various open-source base models including Mistral-7B, Llama 2 (7B, 13B, 70B), and CodeLlama (7B, 13B, 34B, 70B). The models were trained on a mixture of 512K fair downsampled GSM8K instances and 512K MATH instances with Any-Code filtering from OpenMathInstruct-1, totaling approximately 1.02M instances. Evaluation was performed on several popular math reasoning benchmarks: GSM8K, MATH, GSM-Hard, SV AMP, TabMWP, ASDiv, and MAWPS. For ablation studies and hyperparameter tuning, a 1K-sized validation set was created from the training portions of GSM8K and MATH. Models were evaluated in a zero-shot setting using both greedy decoding and self-consistency (majority voting with k=50 samples) at a temperature of 0.7. The NeMo toolkit was used for all experiments.",
    "Limitations": "The work has several limitations: 1) The presence of semantically noisy solutions, where models arrive at correct answers via flawed reasoning, which are hard to detect and filter out and are beyond the current scope. 2) The improvement on in-domain math benchmarks may not translate to performance gains on other related tasks, as indicated by a drop in performance on GSM-Hard, suggesting a lack of robustness to input perturbations. 3) There are ethical risks associated with using these imperfect datasets and models for sensitive tasks like evaluating student assignments or building math tutors. 4) Computational costs limited the ability to perform multiple runs for statistical significance testing (e.g., error bars). 5) The current dataset lacks examples where the model recovers from errors during problem-solving, a capability observed in advanced models like GPT-4.",
    "Future Research Directions": "Future research could focus on developing advanced semantic filters to identify and remove semantically noisy solutions from synthetically generated datasets, potentially leveraging more powerful LLMs like GPT-4 for solution grading. Another direction is to investigate methods for improving the robustness of models to input perturbations and enhancing their generalization capabilities to a wider range of mathematical reasoning tasks beyond the specific benchmarks. Additionally, exploring mechanisms to enable models to recover from errors during code generation or execution, by including such error recovery trajectories in the training data, could be a valuable area for investigation.",
    "Experiment Code": "File Path: nemo_skills/code_execution/local_sandbox/local_sandbox_server.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport io\nimport logging\nimport multiprocessing as mp\nimport os\nimport re\nimport resource\nimport signal\nimport subprocess\nimport tempfile\nimport threading\nimport time\nimport traceback\nfrom contextlib import redirect_stderr, redirect_stdout\n\nimport psutil\nfrom flask import Flask, request\nfrom IPython.terminal.interactiveshell import TerminalInteractiveShell\n\napp = Flask(__name__)\n\n# Identify worker and configure logging so messages are visible per-worker\nworker_id = os.getenv(\"WORKER_NUM\", \"unknown\")\nlogging.basicConfig(\n    level=logging.INFO,\n    format=f\"[worker {worker_id}] %(asctime)s %(levelname)s: %(message)s\",\n)\n\n\n# Worker that runs inside the shell process and owns a TerminalInteractiveShell()\ndef shell_worker(conn):\n    shell = TerminalInteractiveShell()\n    try:\n        while True:\n            try:\n                msg = conn.recv()\n            except EOFError:\n                break\n            if not isinstance(msg, dict):\n                continue\n            cmd = msg.get(\"cmd\")\n            if cmd == \"exec\":\n                code = msg.get(\"code\", \"\")\n                exec_id = msg.get(\"id\")\n                traceback_verbosity = msg.get(\"traceback_verbosity\", \"Plain\")\n\n                # Set traceback verbosity for this execution\n                shell.InteractiveTB.set_mode(mode=traceback_verbosity)\n\n                stdout_buf = io.StringIO()\n                stderr_buf = io.StringIO()\n                try:\n                    # Capture stdout/stderr so we can send back outputs even if the caller times out\n                    with redirect_stdout(stdout_buf), redirect_stderr(stderr_buf):\n                        res = shell.run_cell(code)\n                    conn.send(\n                        {\n                            \"status\": \"ok\",\n                            \"id\": exec_id,\n                            \"result_repr\": repr(getattr(res, \"result\", None)),\n                            \"stdout\": stdout_buf.getvalue(),\n                            \"stderr\": stderr_buf.getvalue(),\n                            \"has_error\": res.error_before_exec or res.error_in_exec,\n                        }\n                    )\n                except KeyboardInterrupt:\n                    conn.send(\n                        {\n                            \"status\": \"interrupted\",\n                            \"id\": exec_id,\n                            \"stdout\": stdout_buf.getvalue(),\n                            \"stderr\": stderr_buf.getvalue(),\n                        }\n                    )\n                except Exception:\n                    conn.send(\n                        {\n                            \"status\": \"error\",\n                            \"id\": exec_id,\n                            \"traceback\": traceback.format_exc(),\n                            \"stdout\": stdout_buf.getvalue(),\n                            \"stderr\": stderr_buf.getvalue(),\n                        }\n                    )\n            elif cmd == \"shutdown\":\n                break\n    finally:\n        try:\n            conn.close()\n        except Exception:\n            pass\n\n\nclass ShellManager:\n    def __init__(self):\n        \"\"\"\n        Manages IPython shell processes with proper timeout and cancellation support.\n        \"\"\"\n        self.shells = {}  # shell_id -> dict(proc, conn, lock, created, last_used, restart_pending)\n        self.manager_lock = threading.Lock()  # Protects shells dict\n\n    def start_shell(self, shell_id):\n        parent_conn, child_conn = mp.Pipe(duplex=True)\n        proc = mp.Process(target=shell_worker, args=(child_conn,), daemon=True)\n        proc.start()\n        current_time = time.time()\n        with self.manager_lock:\n            self.shells[shell_id] = {\n                \"proc\": proc,\n                \"conn\": parent_conn,\n                \"lock\": threading.Lock(),\n                \"created\": current_time,\n                \"last_used\": current_time,\n                \"restart_pending\": False,  # Flag to indicate this shell was restarted and needs new_session_created=True\n            }\n\n    def stop_shell(self, shell_id):\n        with self.manager_lock:\n            entry = self.shells.pop(shell_id, None)\n\n        if not entry:\n            return\n        proc, conn = entry[\"proc\"], entry[\"conn\"]\n        try:\n            conn.send({\"cmd\": \"shutdown\"})\n        except Exception:\n            pass\n        try:\n            conn.close()\n        except Exception:\n            pass\n        proc.terminate()\n        proc.join(timeout=2.0)\n\n    def run_cell(self, shell_id, code, timeout=1.0, grace=0.5, traceback_verbosity=\"Plain\"):\n        \"\"\"\n        Execute `code` on shell `shell_id`.\n        - timeout: seconds to wait for normal completion\n        - grace: seconds to wait after sending SIGINT\n        - traceback_verbosity: IPython traceback verbosity mode\n        Returns a dict with status and outputs. If it times out and we kill the shell,\n        status will be 'timeout_killed' and the shell will be restarted (memory dropped).\n        \"\"\"\n        current_time = time.time()\n        with self.manager_lock:\n            entry = self.shells.get(shell_id)\n            shell_was_created = False\n\n            if entry is None:\n                shell_was_created = True\n            else:\n                # Update last_used timestamp\n                entry[\"last_used\"] = current_time\n\n        # Check if this shell has a restart pending and clear the flag\n        shell_was_recently_restarted = False\n        if not shell_was_created and shell_id in self.shells:\n            with self.manager_lock:\n                if self.shells[shell_id].get(\"restart_pending\", False):\n                    shell_was_recently_restarted = True\n                    self.shells[shell_id][\"restart_pending\"] = False\n\n        # Create shell outside the lock to avoid blocking other operations\n        if shell_was_created:\n            self.start_shell(shell_id)\n            with self.manager_lock:\n                entry = self.shells[shell_id]\n\n        proc = entry[\"proc\"]\n        conn = entry[\"conn\"]\n        lock = entry[\"lock\"]\n\n        exec_id = time.time_ns()\n        with lock:\n            # send execution request\n            try:\n                conn.send({\"cmd\": \"exec\", \"id\": exec_id, \"code\": code, \"traceback_verbosity\": traceback_verbosity})\n            except Exception as exc:\n                return {\n                    \"status\": \"error\",\n                    \"msg\": f\"send failed: {exc}\",\n                    \"shell_was_created\": shell_was_created,\n                    \"shell_was_recently_restarted\": shell_was_recently_restarted,\n                }\n\n            # wait for the result up to `timeout`\n            if conn.poll(timeout):\n                try:\n                    result = conn.recv()\n                    result[\"shell_was_created\"] = shell_was_created\n                    result[\"shell_was_recently_restarted\"] = shell_was_recently_restarted\n                    return result\n                except EOFError:\n                    # Connection closed - shell process died, need to restart\n                    logging.warning(f\"Shell process for {shell_id} died during execution, restarting\")\n                    with self.manager_lock:\n                        self.shells.pop(shell_id, None)\n                    self.start_shell(shell_id)\n\n                    # Mark the new shell as having a restart pending\n                    with self.manager_lock:\n                        if shell_id in self.shells:\n                            self.shells[shell_id][\"restart_pending\"] = True\n\n                    return {\n                        \"status\": \"error\",\n                        \"msg\": \"connection closed\",\n                        \"shell_was_created\": shell_was_created,\n                        \"shell_was_restarted\": True,\n                        \"shell_was_recently_restarted\": shell_was_recently_restarted,\n                    }\n\n            # no reply yet -> try gentle interrupt (SIGINT)\n            try:\n                # Process.send_signal exists on Unix; fallback to os.kill if necessary\n                try:\n                    proc.send_signal(signal.SIGINT)\n                except AttributeError:\n                    os.kill(proc.pid, signal.SIGINT)\n            except Exception:\n                # If we couldn't send SIGINT, fall through to termination after grace\n                pass\n\n            # wait short grace period for the shell to handle the interrupt\n            if conn.poll(grace):\n                try:\n                    result = conn.recv()\n                    result[\"shell_was_created\"] = shell_was_created\n                    result[\"shell_was_recently_restarted\"] = shell_was_recently_restarted\n                    return result\n                except EOFError:\n                    # Connection closed - shell process died, need to restart\n                    logging.warning(f\"Shell process for {shell_id} died during interrupt, restarting\")\n                    with self.manager_lock:\n                        self.shells.pop(shell_id, None)\n                    self.start_shell(shell_id)\n\n                    # Mark the new shell as having a restart pending\n                    with self.manager_lock:\n                        if shell_id in self.shells:\n                            self.shells[shell_id][\"restart_pending\"] = True\n\n                    return {\n                        \"status\": \"interrupted\",\n                        \"msg\": \"connection closed after interrupt\",\n                        \"shell_was_created\": shell_was_created,\n                        \"shell_was_restarted\": True,\n                        \"shell_was_recently_restarted\": shell_was_recently_restarted,\n                    }\n\n            # still stuck -> terminate the shell and restart it (drop memory)\n            try:\n                proc.terminate()\n            except Exception:\n                try:\n                    os.kill(proc.pid, signal.SIGKILL)\n                except Exception:\n                    pass\n            proc.join(timeout=2.0)\n\n            # close old connection (best-effort)\n            try:\n                conn.close()\n            except Exception:\n                pass\n\n            # remove and restart a fresh shell for this id\n            with self.manager_lock:\n                self.shells.pop(shell_id, None)\n            self.start_shell(shell_id)\n\n            # Mark the new shell as having a restart pending\n            with self.manager_lock:\n                if shell_id in self.shells:\n                    self.shells[shell_id][\"restart_pending\"] = True\n\n            return {\n                \"status\": \"timeout_killed\",\n                \"id\": exec_id,\n                \"shell_was_restarted\": True,\n                \"shell_was_recently_restarted\": shell_was_recently_restarted,\n            }\n\n\ndef log_session_count(prefix: str = \"\") -> None:\n    try:\n        with shell_manager.manager_lock:\n            session_count = len(shell_manager.shells)\n        logging.info(\"%sactive_sessions=%d\", prefix, session_count)\n    except Exception:\n        pass\n\n\n# Global shell manager for IPython sessions\nshell_manager = ShellManager()\nSESSION_TIMEOUT = float(os.getenv(\"NEMO_SKILLS_SANDBOX_SESSION_TIMEOUT\", 0))  # disabled by default\n\n\ndef cleanup_expired_sessions():\n    \"\"\"Remove IPython sessions that haven't been used recently\"\"\"\n    if SESSION_TIMEOUT <= 0:\n        return  # Session expiration disabled\n\n    current_time = time.time()\n    expired_sessions = []\n\n    with shell_manager.manager_lock:\n        for session_id, session_data in shell_manager.shells.items():\n            if current_time - session_data[\"last_used\"] > SESSION_TIMEOUT:\n                expired_sessions.append(session_id)\n\n    for session_id in expired_sessions:\n        try:\n            shell_manager.stop_shell(session_id)\n            logging.info(f\"Cleaned up expired session: {session_id}\")\n        except Exception as e:\n            logging.warning(f\"Error cleaning up session {session_id}: {e}\")\n\n\ndef postprocess_output(output, traceback_verbosity):\n    if traceback_verbosity not in [\"Minimal\", \"Plain\"]:\n        return output\n\n    def strip_ansi_codes(text):\n        ansi_escape = re.compile(r\"\\x1B(?:[@-Z\\-_]|\\[[0-?]*[ -/]*[@-~])\")\n        return ansi_escape.sub(\"\", text)\n\n    output = strip_ansi_codes(output)\n    lines = []\n    for line in output.split(\"\\n\"):\n        if line.strip().startswith(\"File <ipython-\"):\n            continue\n        line = re.sub(r\"^Out\\[\\d+\\]:\\s*\", \"\", line)\n        lines.append(line)\n\n    # Remove leading blank lines introduced by displayhook\n    while lines and lines[0] == \"\":\n        lines.pop(0)\n\n    output = \"\\n\".join(lines).rstrip()\n    return output + (\"\\n\" if output else \"\")\n\n\ndef cleanup_session(session_id):\n    \"\"\"Clean up and remove a specific session\"\"\"\n    shell_manager.stop_shell(session_id)\n    logging.info(f\"Cleaned up session: {session_id}\")\n\n\ndef execute_ipython_session(generated_code, session_id, timeout=30, traceback_verbosity=\"Plain\"):\n    \"\"\"Execute Python code in a persistent IPython session with proper timeout handling\"\"\"\n    try:\n        # Clean up expired sessions periodically\n        if SESSION_TIMEOUT > 0:\n            cleanup_expired_sessions()\n\n        # Execute code using ShellManager with proper timeout and cancellation\n        result = shell_manager.run_cell(\n            session_id, generated_code, timeout=timeout, grace=0.5, traceback_verbosity=traceback_verbosity\n        )\n\n        # Determine if this is effectively a new session\n        new_session_created = (\n            result.get(\"shell_was_created\", False)  # Shell was created for this request\n            or result.get(\"shell_was_restarted\", False)  # Shell was restarted during this request\n            or result.get(\"shell_was_recently_restarted\", False)  # Shell was restarted in a previous request\n        )\n\n        # Map ShellManager results to expected format\n        if result[\"status\"] == \"ok\":\n            process_status = \"error\" if result.get(\"has_error\", False) else \"completed\"\n            return {\n                \"process_status\": process_status,\n                \"stdout\": postprocess_output(result.get(\"stdout\", \"\"), traceback_verbosity),\n                \"stderr\": postprocess_output(result.get(\"stderr\", \"\"), traceback_verbosity),\n                \"new_session_created\": new_session_created,\n            }\n        elif result[\"status\"] == \"timeout_killed\":\n            logging.warning(f\"IPython session {session_id} timed out after {timeout}s, shell was restarted\")\n            return {\n                \"process_status\": \"timeout\",\n                \"stdout\": \"\",\n                \"stderr\": f\"Execution timed out after {timeout} seconds\\n\",\n                \"new_session_created\": True,  # Shell was restarted, so it's effectively new\n            }\n        elif result[\"status\"] == \"interrupted\":\n            # For timeout scenarios, treat interruption as timeout since it happened due to timeout\n            # Check if shell was restarted during interruption\n            if result.get(\"shell_was_restarted\", False):\n                new_session_created = True\n                logging.warning(f\"IPython session {session_id} was interrupted and restarted after {timeout}s timeout\")\n            else:\n                logging.warning(f\"IPython session {session_id} was interrupted after {timeout}s timeout\")\n            return {\n                \"process_status\": \"timeout\",\n                \"stdout\": postprocess_output(result.get(\"stdout\", \"\"), traceback_verbosity),\n                \"stderr\": postprocess_output(\n                    result.get(\"stderr\", \"\") + f\"Execution timed out after {timeout} seconds\\n\", traceback_verbosity\n                ),\n                \"new_session_created\": new_session_created,\n            }\n        else:  # error status\n            # Check if shell was restarted during error\n            if result.get(\"shell_was_restarted\", False):\n                new_session_created = True\n                logging.warning(f\"IPython session {session_id} had an error and was restarted\")\n            error_msg = result.get(\"traceback\", result.get(\"msg\", \"Unknown error\"))\n            return {\n                \"process_status\": \"error\",\n                \"stdout\": postprocess_output(result.get(\"stdout\", \"\"), traceback_verbosity),\n                \"stderr\": postprocess_output(result.get(\"stderr\", \"\") + f\"\\n{error_msg}\\n\", traceback_verbosity),\n                \"new_session_created\": new_session_created,\n            }\n\n    except Exception as e:\n        logging.error(f\"Error in execute_ipython_session for session {session_id}: {e}\")\n        return {\n            \"process_status\": \"error\",\n            \"stdout\": \"\",\n            \"stderr\": f\"Session error: {e}\\n\",\n            \"new_session_created\": False,\n        }\n\n\n# Log per-request session count after each response\n@app.after_request\ndef _after_log_session_count(response):\n    log_session_count()\n    return response\n\n\nMEM_LIMIT_BYTES = int(os.environ.get(\"NEMO_SKILLS_SANDBOX_MEM_LIMIT\", 50 * 1024**3))  # 50 GiB default\n\n# Set per-worker memory limit for ipython session\nresource.setrlimit(resource.RLIMIT_AS, (2 * MEM_LIMIT_BYTES, 2 * MEM_LIMIT_BYTES))\nresource.setrlimit(resource.RLIMIT_DATA, (2 * MEM_LIMIT_BYTES, 2 * MEM_LIMIT_BYTES))\nlogging.info(\"Applied worker memory limit (RLIMIT_AS/RLIMIT_DATA): %d bytes\", 2 * MEM_LIMIT_BYTES)\n\n\n# Code to kill the process tree for lean4 code execution\ndef kill_process_tree(proc):\n    \"\"\"\n    Safely and aggressively kills a process and all its descendants.\n    This is the recommended approach for ensuring cleanup.\n    \"\"\"\n    try:\n        parent = psutil.Process(proc.pid)\n        # Get all children of the process, recursively.\n        children = parent.children(recursive=True)\n        # Add the parent to the list of processes to be killed.\n        all_processes = children + [parent]\n\n        # Kill all processes in the tree.\n        for p in all_processes:\n            try:\n                # SIGKILL is a forceful, non-ignorable kill signal.\n                p.kill()\n            except psutil.NoSuchProcess:\n                # The process might have already died, which is fine.\n                pass\n\n        # Wait for all processes to be terminated.\n        gone, alive = psutil.wait_procs(all_processes, timeout=3)\n        if alive:\n            # If any processes are still alive, they are likely zombies\n            # or in an unkillable state. This is a last resort.\n            for p in alive:\n                logging.warning(\"Process %s could not be killed.\", p.pid)\n    except psutil.NoSuchProcess:\n        # The main process already died before we could kill it.\n        pass\n    except Exception as e:\n        logging.error(\"Error in kill_process_tree: %s\", e)\n\n\ndef set_limits(mem_bytes: int = MEM_LIMIT_BYTES) -> None:\n    \"\"\"\n    Apply RLIMITs and start a new session for the child process.\n\n    Called via `preexec_fn` (subprocess) or directly in a forked worker.\n    \"\"\"\n    resource.setrlimit(resource.RLIMIT_AS, (mem_bytes, mem_bytes))\n    resource.setrlimit(resource.RLIMIT_DATA, (mem_bytes, mem_bytes))\n    os.setsid()  # isolate PGID / signals\n\n\ndef execute_python(generated_code, std_input, timeout, language):\n    execution_command = [language, \"-c\", generated_code]\n    try:\n        process = subprocess.Popen(\n            execution_command,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            universal_newlines=True,\n            preexec_fn=set_limits,\n        )\n        stdout, stderr = process.communicate(input=std_input, timeout=timeout)\n        return {\"process_status\": \"completed\", \"stdout\": stdout, \"stderr\": stderr}\n    except subprocess.TimeoutExpired:\n        try:\n            # kill the whole process group\n            os.killpg(process.pid, signal.SIGKILL)\n        except ProcessLookupError:\n            pass\n        process.wait(timeout=1)  # reap, no extra timeout needed\n        return {\"process_status\": \"timeout\", \"stdout\": \"\", \"stderr\": f\"Execution timed out after {timeout} seconds\\n\"}\n\n\ndef execute_lean4(generated_code, timeout):\n    temp_file_name = None\n    proc = None  # <-- Keep track of the process object\n    try:\n        project_path = \"/lean4/my_project\"\n        # Use a with statement for the temp file to ensure it's closed\n        with tempfile.NamedTemporaryFile(dir=project_path, delete=False, suffix=\".lean\") as temp_file:\n            temp_file_name = temp_file.name\n            temp_file.write(generated_code.encode(\"utf-8\"))\n            temp_file.flush()  # Ensure data is written to disk\n\n        # Use subprocess.Popen for more control\n        proc = subprocess.Popen(\n            [\"lake\", \"env\", \"--dir\", project_path, \"lean\", temp_file_name],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            cwd=project_path,\n            preexec_fn=os.setsid,\n        )\n\n        # Communicate with the process, which waits for it to finish\n        # This will raise TimeoutExpired if the timeout is reached\n        stdout, stderr = proc.communicate(timeout=timeout)\n\n        if proc.returncode == 0:\n            process_status = \"completed\"\n        else:\n            process_status = \"failed\"\n\n        return {\n            \"process_status\": process_status,\n            \"stdout\": stdout.decode(\"utf-8\"),\n            \"stderr\": stderr.decode(\"utf-8\"),\n        }\n\n    except subprocess.TimeoutExpired:\n        # kill the process tree\n        kill_process_tree(proc)\n        # Now we can safely get any output that was generated before the kill.\n        stdout, stderr = proc.communicate()\n\n        final_stderr = stderr.decode(\"utf-8\") + f\"Execution timed out after {timeout} seconds\\n\"\n        return {\n            \"process_status\": \"timeout\",\n            \"stdout\": stdout.decode(\"utf-8\"),\n            \"stderr\": final_stderr,\n        }\n\n    except Exception as e:\n        logging.error(\"Error executing Lean4 code: %s\", e)\n        return {\"process_status\": \"error\", \"stdout\": \"\", \"stderr\": str(e) + \"\\n\"}\n    finally:\n        # Safely remove the temporary file if it was created\n        if temp_file_name and os.path.exists(temp_file_name):\n            os.remove(temp_file_name)\n\n\ndef execute_shell(command, timeout):\n    tmp_path = None\n    try:\n        # Write the full script to a temp file so /bin/bash can read it from disk\n        with tempfile.NamedTemporaryFile(delete=False, suffix=\".sh\", mode=\"w\") as tmp:\n            tmp.write(command)\n            tmp_path = tmp.name\n        os.chmod(tmp_path, 0o755)\n\n        result = subprocess.run(\n            [\"/bin/bash\", tmp_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            timeout=timeout,\n            preexec_fn=set_limits,\n        )\n        process_status = \"completed\" if result.returncode == 0 else \"error\"\n        return {\"process_status\": process_status, \"stdout\": result.stdout, \"stderr\": result.stderr}\n    except subprocess.TimeoutExpired:\n        return {\"process_status\": \"timeout\", \"stdout\": \"\", \"stderr\": f\"Execution timed out after {timeout} seconds\\n\"}\n    finally:\n        if tmp_path and os.path.exists(tmp_path):\n            os.remove(tmp_path)\n\n\n# Main Flask endpoint to handle execution requests\n@app.route(\"/execute\", methods=[\"POST\"])\ndef execute():\n    generated_code = request.json[\"generated_code\"]\n    timeout = request.json[\"timeout\"]\n    language = request.json.get(\"language\", \"ipython\")\n    std_input = request.json.get(\"std_input\", \"\")\n    max_output_characters = request.json.get(\"max_output_characters\", 1000)\n    traceback_verbosity = request.json.get(\"traceback_verbosity\", \"Plain\")\n\n    session_id = request.headers.get(\"X-Session-ID\")\n\n    if language == \"ipython\":\n        if session_id is None:\n            return {\"error\": \"X-Session-ID header required for ipython sessions\"}, 400\n        result = execute_ipython_session(generated_code, session_id, timeout, traceback_verbosity)\n    elif language == \"lean4\":\n        result = execute_lean4(generated_code, timeout)\n    elif language == \"shell\":\n        result = execute_shell(generated_code, timeout)\n    else:\n        result = execute_python(generated_code, std_input, timeout, language)\n\n    if len(result[\"stdout\"]) > max_output_characters:\n        result[\"stdout\"] = result[\"stdout\"][:max_output_characters] + \"<output cut>\"\n    if len(result[\"stderr\"]) > max_output_characters:\n        result[\"stderr\"] = result[\"stderr\"][:max_output_characters] + \"<output cut>\"\n\n    return result\n\n\n# Session management endpoints\n@app.route(\"/sessions\", methods=[\"GET\"])\ndef list_sessions():\n    \"\"\"List all active IPython sessions\"\"\"\n    try:\n        session_info = {}\n\n        # Get sessions from ShellManager with proper locking\n        with shell_manager.manager_lock:\n            for session_id, session_data in shell_manager.shells.items():\n                session_info[session_id] = {\n                    \"backend\": \"ipython\",\n                    \"created\": session_data[\"created\"],\n                    \"last_used\": session_data[\"last_used\"],\n                    \"alive\": True,  # All shells in manager are alive\n                }\n        return {\"sessions\": session_info, \"backend\": \"ipython\"}\n    except Exception as e:\n        import traceback\n\n        error_msg = f\"Error in list_sessions: {str(e)}\\n{traceback.format_exc()}\"\n        logging.error(error_msg)\n        return {\"error\": error_msg}, 500\n\n\n@app.route(\"/sessions/<session_id>\", methods=[\"DELETE\"])\ndef delete_session(session_id):\n    \"\"\"Delete a specific IPython session\"\"\"\n    try:\n        with shell_manager.manager_lock:\n            session_exists = session_id in shell_manager.shells\n\n        if session_exists:\n            shell_manager.stop_shell(session_id)\n            return {\"message\": f\"IPython session {session_id} deleted successfully\"}\n        else:\n            return {\"error\": f\"IPython session {session_id} not found\"}, 404\n    except Exception as e:\n        return {\"error\": f\"Error deleting IPython session {session_id}: {e}\"}, 500\n\n\n@app.route(\"/health\", methods=[\"GET\"])\ndef health():\n    return {\"status\": \"healthy\", \"worker\": os.environ.get(\"WORKER_NUM\", \"unknown\")}\n\n\nif __name__ == \"__main__\":\n    log = logging.getLogger(\"werkzeug\")\n    log.setLevel(logging.WARNING)\n    app.run(port=6000)\nFile Path: nemo_skills/code_execution/sandbox.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\nimport asyncio\nimport json\nimport logging\nimport os\nimport time\nimport traceback\nimport uuid\nfrom collections import defaultdict\nfrom typing import Dict, Optional, Tuple\n\nimport httpx\n\nfrom nemo_skills.code_execution.proof_utils import (\n    determine_proof_status,\n)\nfrom nemo_skills.utils import get_logger_name, python_doc_to_cmd_help\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\nclass Sandbox(abc.ABC):\n    \"\"\"Code execution sandbox.\n\n    Args:\n        host: Optional[str] = '127.0.0.1' - Host of the sandbox server.\n            Can also be specified through NEMO_SKILLS_SANDBOX_HOST env var.\n        port: Optional[str] = '5000' - Port of the sandbox server.\n            Can also be specified through NEMO_SKILLS_SANDBOX_PORT env var.\n        ssh_server: Optional[str] = None - SSH server for tunneling requests.\n            Useful if server is running on slurm cluster to which there is an ssh access.\n            Can also be specified through NEMO_SKILLS_SSH_SERVER env var.\n        ssh_key_path: Optional[str] = None - Path to the ssh key for tunneling.\n            Can also be specified through NEMO_SKILLS_SSH_KEY_PATH env var.\n    \"\"\"\n\n    def __init__(\n        self,\n        host: Optional[str] = os.getenv(\"NEMO_SKILLS_SANDBOX_HOST\", \"127.0.0.1\"),\n        port: Optional[str] = os.getenv(\"NEMO_SKILLS_SANDBOX_PORT\", \"6000\"),\n        ssh_server: Optional[str] = None,\n        ssh_key_path: Optional[str] = None,\n    ):\n        self.host = host\n        self.port = port\n        # Create async HTTP client with high limits\n        self.http_session = httpx.AsyncClient(\n            limits=httpx.Limits(max_keepalive_connections=2048, max_connections=2048),\n        )\n        self.ssh_server = os.getenv(\"NEMO_SKILLS_SSH_SERVER\", ssh_server)\n        self.ssh_key_path = os.getenv(\"NEMO_SKILLS_SSH_KEY_PATH\", ssh_key_path)\n        self.session_histories = defaultdict(list)  # session_id -> list of generated_code\n\n    async def close(self):\n        \"\"\"Close the HTTP session.\"\"\"\n        await self.http_session.aclose()\n\n    async def _send_request(self, request, timeout):\n        session_id = request.pop(\"session_id\", None)\n        extra_headers = {}\n        if session_id is not None:\n            extra_headers[\"X-Session-ID\"] = str(session_id)\n\n        if self.ssh_server and self.ssh_key_path:\n            # For SSH tunneling, use threads since there's no async version\n            import sshtunnel_requests\n\n            def ssh_request():\n                sshtunnel_request = sshtunnel_requests.from_url(f\"ssh://{self.ssh_server}:22\", self.ssh_key_path)\n                return sshtunnel_request.post(\n                    url=self._get_execute_url(),\n                    data=json.dumps(request),\n                    timeout=timeout + 5.0,\n                    headers={\"Content-Type\": \"application/json\", **extra_headers},\n                )\n\n            # Native async requires more lines of code, so we use to_thread\n            # Should be ok since this is a debug mode\n            output = await asyncio.to_thread(ssh_request)\n        else:\n            output = await self.http_session.post(\n                url=self._get_execute_url(),\n                content=json.dumps(request),\n                timeout=timeout + 5.0,\n                headers={\"Content-Type\": \"application/json\", **extra_headers},\n            )\n        # retrying 502 errors\n        if output.status_code == 502:\n            raise httpx.TimeoutException(\"502 error\")\n        return self._parse_request_output(output)\n\n    @abc.abstractmethod\n    def _parse_request_output(self, output):\n        pass\n\n    @abc.abstractmethod\n    def _get_execute_url(self):\n        pass\n\n    @abc.abstractmethod\n    def _prepare_request(\n        self,\n        generated_code,\n        timeout,\n        language=\"ipython\",\n        std_input=\"\",\n        max_output_characters=1000,\n        traceback_verbosity=\"Plain\",\n    ):\n        pass\n\n    @abc.abstractmethod\n    async def delete_session(self, session_id: str) -> None:\n        \"\"\"Delete a remote execution session if supported by the backend.\"\"\"\n        pass\n\n    async def execute_code(\n        self,\n        generated_code: str,\n        std_input: str = \"\",\n        language: str = \"ipython\",\n        timeout: float = 10.0,\n        max_output_characters: int = 1000,\n        session_id: Optional[str] = None,\n        traceback_verbosity=\"plain\",  # could be plain, context, verbose, or minimal\n    ) -> Tuple[Dict, str]:\n        traceback_verbosity = traceback_verbosity.capitalize()\n        if language in [\"python\", \"pypy3\", \"python3\", \"lean4\", \"shell\"] and session_id is not None:\n            raise RuntimeError(\n                f\"Stateful execution for {language} is not supported. session_id is {session_id} but should be None\"\n            )\n        if language not in [\"ipython\", \"python\", \"pypy3\", \"python3\", \"lean4\", \"shell\"]:\n            raise ValueError(f\"Unsupported language: {language}\")\n        if language != \"ipython\" and traceback_verbosity != \"Plain\":\n            raise ValueError(\"Configurable traceback_verbosity is only supported for ipython\")\n\n        session_id_str = str(session_id) if session_id is not None else None\n\n        request_session_id = session_id\n        if request_session_id is None and language == \"ipython\":  # creating a new session with empty state\n            request_session_id = uuid.uuid4()\n\n        request_session_id_str = str(request_session_id) if request_session_id is not None else None\n\n        TO_EXECUTE = generated_code\n        request = self._prepare_request(\n            TO_EXECUTE, timeout, language, std_input, max_output_characters, traceback_verbosity\n        )\n        request[\"session_id\"] = request_session_id_str\n        try:\n            output = await self._send_request(request, timeout)\n        except httpx.TimeoutException:\n            output = {\"process_status\": \"timeout\", \"stdout\": \"\", \"stderr\": \"Client timed out\\n\"}\n        new_session_created = output.pop(\"new_session_created\", False)\n\n        # Rebuild state by re-executing history first, then execute the new code.\n        # NOTE: Only cells that completed successfully are stored, so we intentionally omit re-running cells that errored\n        # or timed out. This means restoration **can diverge** from the original interactive session in those cases, but\n        # avoids re-triggering side effects from failing cells while keeping the replay simple.\n        if session_id is not None and new_session_created and output.get(\"process_status\") != \"timeout\":\n            history = list(self.session_histories.get(session_id_str, []))\n            if request_session_id_str is not None:\n                try:\n                    await self.delete_session(request_session_id_str)\n                except Exception as exc:\n                    LOG.warning(\n                        \"Failed to delete restarted session %s before restoration: %s\",\n                        request_session_id_str,\n                        exc,\n                    )\n                self.session_histories[request_session_id_str] = history\n            if history:\n                # Restore session state by executing each history cell sequentially to preserve semantics\n                for cell_index, cell_code in enumerate(history, start=1):\n                    restore_request = self._prepare_request(\n                        cell_code, timeout, language, std_input, max_output_characters, traceback_verbosity\n                    )\n                    restore_request[\"session_id\"] = request_session_id_str\n                    try:\n                        restore_output = await self._send_request(restore_request, timeout)\n                    except httpx.TimeoutException:\n                        restore_output = {\"process_status\": \"timeout\", \"stdout\": \"\", \"stderr\": \"Client timed out\\n\"}\n\n                    if restore_output.get(\"process_status\") != \"completed\":\n                        LOG.error(\n                            \"Sandbox state restoration failed for session %s while replaying cell %d/%d with output: %s\",\n                            session_id,\n                            cell_index,\n                            len(history),\n                            restore_output,\n                        )\n                        # Best-effort cleanup of the remote session so the next execution starts fresh.\n                        if request_session_id_str is not None:\n                            try:\n                                await self.delete_session(request_session_id_str)\n                            except Exception as exc:\n                                LOG.warning(\n                                    \"Failed to delete session %s after restoration failure: %s\",\n                                    request_session_id_str,\n                                    exc,\n                                )\n\n                        stderr_parts = (\n                            \"RuntimeError: Sandbox state restoration failed after the execution worker restarted. \"\n                            \"The interactive session history has been cleared; please re-run the last code block without relying on prior state.\"\n                        )\n                        failure_output = {\n                            \"process_status\": \"error\",\n                            \"stdout\": \"\",\n                            \"stderr\": stderr_parts + \"\\n\",\n                        }\n\n                        return failure_output, request_session_id\n\n            # Execute the new code once restoration has succeeded (or there was no history to replay)\n            exec_request = self._prepare_request(\n                generated_code, timeout, language, std_input, max_output_characters, traceback_verbosity\n            )\n            exec_request[\"session_id\"] = request_session_id_str\n            try:\n                output = await self._send_request(exec_request, timeout)\n            except httpx.TimeoutException:\n                output = {\"process_status\": \"timeout\", \"stdout\": \"\", \"stderr\": \"Client timed out\\n\"}\n\n        # Append to history if successful execution (process_status == 'completed')\n        if output.get(\"process_status\") == \"completed\" and request_session_id_str is not None:\n            self.session_histories[request_session_id_str].append(generated_code)\n\n        output.pop(\"new_session_created\", None)\n\n        return output, request_session_id\n\n    async def is_proof_correct(self, pred_output, timeout=30.0):\n        TO_EXECUTE = pred_output\n\n        request = self._prepare_request(TO_EXECUTE, timeout, \"lean4\")\n        try:\n            output = await self._send_request(request, timeout)\n        except httpx.TimeoutException:\n            return \"timeout\"\n        return determine_proof_status(output)\n\n    def _check_ready(self, timeout: float = 5.0) -> bool:\n        \"\"\"Readiness check against the sandbox health endpoint.\"\"\"\n        url = f\"http://{self.host}:{self.port}/health\"\n\n        try:\n            if self.ssh_server and self.ssh_key_path:\n                import sshtunnel_requests\n\n                sshtunnel_request = sshtunnel_requests.from_url(f\"ssh://{self.ssh_server}:22\", self.ssh_key_path)\n                response = sshtunnel_request.get(url=url, timeout=timeout)\n            else:\n                with httpx.Client() as client:\n                    response = client.get(url=url, timeout=timeout)\n            return response.status_code == 200\n        except httpx.HTTPError:\n            return False\n\n    def wait_for_sandbox(self, timeout: int = 5):\n        while not self._check_ready(timeout=timeout):\n            time.sleep(1)\n\n\nclass LocalSandbox(Sandbox):\n    \"\"\"Locally hosted sandbox.\"\"\"\n\n    def _get_execute_url(self):\n        return f\"http://{self.host}:{self.port}/execute\"\n\n    def _parse_request_output(self, output):\n        try:\n            return output.json()\n        except json.JSONDecodeError:\n            LOG.error(\"Error during parsing output: %s\", output.text)\n            return {\"process_status\": \"error\", \"stdout\": \"\", \"stderr\": \"Unknown error\"}\n\n    def _prepare_request(\n        self,\n        generated_code,\n        timeout,\n        language=\"ipython\",\n        std_input=\"\",\n        max_output_characters=1000,\n        traceback_verbosity=\"Plain\",\n    ):\n        return {\n            \"generated_code\": generated_code,\n            \"std_input\": std_input,\n            \"timeout\": timeout,\n            \"language\": language,\n            \"max_output_characters\": max_output_characters,\n            \"traceback_verbosity\": traceback_verbosity,\n        }\n\n    async def delete_session(self, session_id: str) -> None:\n        \"\"\"Delete an IPython session on the local sandbox server.\"\"\"\n        max_retries = 3\n        retry_delay = 2\n\n        for attempt in range(max_retries):\n            try:\n                response = await self.http_session.delete(\n                    url=f\"http://{self.host}:{self.port}/sessions/{session_id}\",\n                    timeout=10.0,\n                    headers={\"X-Session-ID\": session_id},\n                )\n                if response.status_code == 200:  # Success\n                    if session_id in self.session_histories:\n                        del self.session_histories[session_id]\n                    return\n                if response.status_code == 404:  # We were routed to a different worker\n                    LOG.warning(f\"Session {session_id} not found (already deleted?). Treating as success.\")\n                    if session_id in self.session_histories:\n                        del self.session_histories[session_id]\n                    return\n                response.raise_for_status()\n            except (\n                httpx.ReadTimeout,  # retry for other communication errors and statuses\n                httpx.ConnectError,\n                httpx.ConnectTimeout,\n                httpx.RemoteProtocolError,\n                httpx.HTTPStatusError,\n            ) as e:\n                LOG.warning(\"Retry %d/%d deleting session %s \", attempt + 1, max_retries, session_id, e)\n                if attempt < max_retries - 1:\n                    await asyncio.sleep(retry_delay)\n                else:\n                    LOG.warning(f\"Failed to delete session {session_id} after {max_retries} attempts. \")\n            except Exception as e:\n                LOG.warning(\n                    \"Failed to delete session %s: %s (type: %s, repr: %r)\\nTraceback:\\n%s\",\n                    session_id,\n                    e,\n                    type(e).__name__,\n                    e,\n                    traceback.format_exc(),\n                )\n                raise  # Re-raise unexpected exceptions\n\n\nsandboxes = {\n    \"local\": LocalSandbox,\n}\n\n\ndef get_sandbox(sandbox_type: str = \"local\", **kwargs):\n    \"\"\"A helper function to make it easier to set sandbox through cmd.\"\"\"\n    sandbox_class = sandboxes[sandbox_type.lower()]\n    return sandbox_class(**kwargs)\n\n\ndef sandbox_params():\n    \"\"\"Returns sandbox documentation (to include in cmd help).\"\"\"\n    prefix = f\"\\n        sandbox_type: str = MISSING - Choices: {list(sandboxes.keys())}\"\n    return python_doc_to_cmd_help(Sandbox, docs_prefix=prefix, arg_prefix=\"sandbox.\")\nFile Path: nemo_skills/inference/model/code_execution.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.S\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport copy\nimport logging\nimport time\nfrom dataclasses import field\n\nfrom nemo_skills.code_execution import extract_code_to_execute, format_code_output\nfrom nemo_skills.code_execution.sandbox import Sandbox\nfrom nemo_skills.utils import get_logger_name, nested_dataclass\n\nfrom .base import BaseModel, EndpointType\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\n@nested_dataclass(kw_only=True)\nclass CodeExecutionConfig:\n    max_code_output_characters: int = 1000\n    code_execution_timeout: float = 10.0\n    code_execution_language: str = \"ipython\"\n    code_execution_headers: list[str] = field(default_factory=lambda: [])\n    max_code_executions: int = 8\n    sandbox_traceback_verbosity: str = \"plain\"  # could be plain, context, verbose, or minimal\n    add_remaining_code_executions: bool = False\n\n\nclass CodeExecutionWrapper:\n    def __init__(self, model: BaseModel, sandbox: Sandbox, config: CodeExecutionConfig):\n        self.model = model\n        self.sandbox = sandbox\n        self.config = config\n\n    async def _generate_single(\n        self,\n        prompt: str | list[dict],\n        code_begin: str,\n        code_end: str,\n        code_output_begin: str,\n        code_output_end: str,\n        code_output_format: str,\n        tokens_to_generate: int,\n        temperature: float,\n        top_p: float,\n        top_k: int,\n        min_p: float,\n        repetition_penalty: float,\n        random_seed: int,\n        stop_phrases: list[str] | None = None,\n        top_logprobs: int | None = None,\n        timeout: float | int | None = 14400,  # None is 10min\n        max_code_executions: int | None = None,  # if not None, will override self.config.max_code_executions\n        stream: bool = False,\n        extra_body: dict = None,\n        endpoint_type: EndpointType = None,\n    ):\n        # Handle OpenAI-style dictionary prompts\n        is_openai_format = not isinstance(prompt, str)\n\n        if top_logprobs is not None:  # TODO: add this\n            raise NotImplementedError(\"top_logprobs is not supported yet.\")\n\n        if stream:\n            return self._stream_single(\n                prompt=prompt,\n                endpoint_type=endpoint_type,\n                code_begin=code_begin,\n                code_end=code_end,\n                code_output_begin=code_output_begin,\n                code_output_end=code_output_end,\n                code_output_format=code_output_format,\n                tokens_to_generate=tokens_to_generate,\n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k,\n                min_p=min_p,\n                repetition_penalty=repetition_penalty,\n                random_seed=random_seed,\n                stop_phrases=stop_phrases,\n                timeout=timeout,\n                max_code_executions=max_code_executions,\n                extra_body=extra_body,\n            )\n\n        effective_max_code_executions = self.config.max_code_executions\n        if max_code_executions is not None:\n            effective_max_code_executions = max_code_executions\n\n        # making a copy of prompts to not corrupt original data\n        if is_openai_format:\n            new_prompt = copy.deepcopy(prompt)\n        else:\n            new_prompt = copy.deepcopy(prompt)\n\n        start_time = int(time.time())\n\n        stop_phrases = stop_phrases or []\n\n        request = {\n            \"endpoint_type\": endpoint_type,\n            \"prompt\": new_prompt,\n            \"tokens_to_generate\": tokens_to_generate,\n            \"temperature\": temperature,\n            \"top_k\": top_k,\n            \"top_p\": top_p,\n            \"min_p\": min_p,\n            \"random_seed\": random_seed,\n            \"repetition_penalty\": repetition_penalty,\n            \"stop_phrases\": stop_phrases + [code_end],\n            \"timeout\": timeout,\n            \"extra_body\": extra_body,\n        }\n        session_id = None\n        code_rounds_executed = 0\n        total_num_generated_tokens = 0\n        generation_time = 0\n        code_execution_time = 0\n        stopped_on_repetition = False\n        num_code_timeouts = 0\n        # adding plus one to make sure there is always some completion after the last requested code block\n        try:\n            for generation_index in range(effective_max_code_executions + 1):\n                generation_time_start = time.time()\n                if timeout is not None:\n                    # updating timeout to account for the time already spent\n                    new_timeout = int(timeout - (time.time() - start_time))\n                    request[\"timeout\"] = new_timeout\n                    if request[\"timeout\"] <= 0:\n                        break\n\n                output_dict = await self.model.generate_async(**request, remove_stop_phrases=False)\n\n                output, num_generated_tokens = output_dict[\"generation\"], output_dict.get(\"num_generated_tokens\", 0)\n                # no need to do anything with this as the code below should just exit, so that's only for logging\n                stopped_on_repetition = output_dict.get(\"stopped_on_repetition\", False)\n\n                # openai and trtllm don't show what stop word was triggered, so we assume that it was `code_end`\n                # if there's an unfinished code block\n                if output.count(code_end) + 1 == output.count(code_begin):\n                    output += code_end\n\n                # Update the prompt based on format\n                if is_openai_format:\n                    request[\"prompt\"].append({\"role\": \"assistant\", \"content\": output})\n                    request[\"prompt\"].append({\"role\": \"user\", \"content\": \"continue\"})\n                else:\n                    request[\"prompt\"] += output\n\n                # if it's the extra iteration, we don't execute the code block and just finish\n\n                if generation_index == effective_max_code_executions:\n                    break\n                # adjusting requested tokens to account for what has been generated already\n                request[\"tokens_to_generate\"] -= num_generated_tokens\n                total_num_generated_tokens += num_generated_tokens\n                generation_time += int(time.time() - generation_time_start)\n                # TODO: currently we don't account for tokens in the code output that we add to the prompt\n                #       in most cases the output should be small though\n                if request[\"tokens_to_generate\"] <= 0:\n                    break\n                # .rfind(code_end, 0, -1) searches for the second-to-last occurrence of code_end and checks\n                # that the last code_begin is not closed to ensure that we are inside the code block\n                if output.endswith(code_end) and output.rfind(code_begin) > output.rfind(code_end, 0, -1):\n                    code_execution_time_start, execution_dict, session_id = await self.execute_generated_code(\n                        prompt, code_begin, code_end, output, session_id\n                    )\n                    remaining_code_executions = None\n                    if self.config.add_remaining_code_executions:\n                        remaining_code_executions = effective_max_code_executions - generation_index - 1\n                    # adding code output to the prompt\n                    code_output = format_code_output(\n                        execution_dict,\n                        code_output_begin,\n                        code_output_end,\n                        code_output_format,\n                        remaining_code_executions,\n                    )\n\n                    if \"process_status\" in execution_dict and execution_dict[\"process_status\"] == \"timeout\":\n                        num_code_timeouts += 1\n\n                    if is_openai_format:\n                        request[\"prompt\"][-2][\"content\"] += code_output\n                    else:\n                        request[\"prompt\"] += code_output\n\n                    code_execution_time += int(time.time() - code_execution_time_start)\n                    code_rounds_executed += 1\n                else:  # if no code was generated, we need to finish\n                    break\n\n            # removing original prompt and returning the generation\n            if is_openai_format:\n                generation = \"\\n\".join(msg[\"content\"] for msg in request[\"prompt\"] if msg[\"role\"] == \"assistant\")\n            else:\n                generation = request[\"prompt\"][len(prompt) :]\n\n            return {\n                \"generation\": generation,\n                \"code_rounds_executed\": code_rounds_executed,\n                \"num_generated_tokens\": total_num_generated_tokens,\n                \"generation_time\": generation_time,\n                \"code_execution_time\": code_execution_time,\n                \"stopped_on_repetition\": stopped_on_repetition,\n                \"num_code_timeouts\": num_code_timeouts,\n            }\n        finally:\n            # Clean up session if we created one and configured to do so\n            if session_id is not None and self.config.code_execution_language == \"ipython\":\n                await self.sandbox.delete_session(str(session_id))\n\n    async def execute_generated_code(self, input_prompt, code_begin, code_end, output, session_id):\n        code_execution_time_start = time.time()\n        header = \"\\n\".join(self.config.code_execution_headers)\n        code_block = extract_code_to_execute(output, code_begin, code_end)\n        extracted_code = f\"{header}{code_block}\"\n        execution_dict, session_id = await self.sandbox.execute_code(\n            generated_code=extracted_code,\n            language=self.config.code_execution_language,\n            timeout=self.config.code_execution_timeout,\n            max_output_characters=self.config.max_code_output_characters,\n            session_id=session_id,\n            traceback_verbosity=self.config.sandbox_traceback_verbosity,\n        )\n\n        return code_execution_time_start, execution_dict, session_id\n\n    async def generate_async(\n        self,\n        prompt: str | list[dict],\n        code_begin: str,\n        code_end: str,\n        code_output_begin: str,\n        code_output_end: str,\n        code_output_format: str,\n        tokens_to_generate: int | None = None,\n        temperature: float = 0.0,\n        top_p: float = 0.95,\n        top_k: int = -1,\n        min_p: float = 0.0,\n        repetition_penalty: float = 1.0,\n        random_seed: int = 0,\n        stop_phrases: list[str] | None = None,\n        remove_stop_phrases: bool = True,\n        top_logprobs: int | None = None,\n        timeout: float | int | None = 14400,  # None is 10min\n        max_code_executions: int | None = None,\n        stream: bool = False,\n        extra_body: dict = None,\n        endpoint_type: EndpointType = None,\n    ) -> list[dict]:\n        \"\"\"For any generation parameter you can specify a list of values that needs to match the number of prompts.\n\n        Not every server supports that, so make sure to override this method directly if that's not the case.\n        \"\"\"\n        if top_logprobs is not None:  # TODO: add this\n            raise NotImplementedError(\"top_logprobs is not supported yet.\")\n\n        kwargs = {\n            \"endpoint_type\": endpoint_type,\n            \"code_begin\": code_begin,\n            \"code_end\": code_end,\n            \"code_output_begin\": code_output_begin,\n            \"code_output_end\": code_output_end,\n            \"code_output_format\": code_output_format,\n            \"tokens_to_generate\": tokens_to_generate,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"top_k\": top_k,\n            \"min_p\": min_p,\n            \"repetition_penalty\": repetition_penalty,\n            \"random_seed\": random_seed,\n            \"stop_phrases\": stop_phrases,\n            \"timeout\": timeout,\n            \"max_code_executions\": max_code_executions,\n            \"stream\": stream,\n            \"extra_body\": extra_body,\n        }\n\n        request = {key: value for key, value in kwargs.items()}\n        request[\"prompt\"] = prompt\n\n        output = await self._generate_single(**request)\n        if not stream:\n            # Some models (like NIM speech models) don't have this method\n            if hasattr(self.model, \"_maybe_apply_stop_phrase_removal\"):\n                self.model._maybe_apply_stop_phrase_removal(output, remove_stop_phrases, stop_phrases)\n\n        return output\n\n    async def _stream_single(\n        self,\n        prompt: str | list[dict],\n        code_begin: str,\n        code_end: str,\n        code_output_begin: str,\n        code_output_end: str,\n        code_output_format: str,\n        tokens_to_generate: int = 512,\n        temperature: float = 0.0,\n        top_p: float = 0.95,\n        top_k: int = -1,\n        min_p: float = 0.0,\n        repetition_penalty: float = 1.0,\n        random_seed: int = 0,\n        stop_phrases: list[str] | None = None,\n        timeout: float | int | None = 14400,  # None is 10min,\n        max_code_executions: int | None = None,\n        extra_body: dict = None,\n        endpoint_type: EndpointType = None,\n    ):\n        \"\"\"\n        Helper method, that implements streaming generation.\n        \"\"\"\n        # Handle OpenAI-style dictionary prompts\n        is_openai_format = not isinstance(prompt, str)\n\n        effective_max_code_executions = self.config.max_code_executions\n        if max_code_executions is not None:\n            effective_max_code_executions = max_code_executions\n\n        stop_phrases = stop_phrases or []\n\n        request = {\n            \"endpoint_type\": endpoint_type,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"top_k\": top_k,\n            \"min_p\": min_p,\n            \"repetition_penalty\": repetition_penalty,\n            \"random_seed\": random_seed,\n            \"stop_phrases\": stop_phrases + [code_end],\n            \"timeout\": timeout,\n            \"tokens_to_generate\": tokens_to_generate,\n            \"stream\": True,\n            \"extra_body\": extra_body,\n        }\n\n        current_full_prompt = copy.deepcopy(prompt)\n        session_id = None  # For sandbox state continuity\n        try:\n            for generation_index in range(effective_max_code_executions + 1):\n                model_token_iterator = await self.model.generate_async(prompt=current_full_prompt, **request)\n\n                current_output_segment = \"\"\n                num_generated_tokens = 0\n                async for chunk in model_token_iterator:\n                    yield chunk\n                    current_output_segment += chunk[\"generation\"]\n                    num_generated_tokens += 1\n\n                request[\"tokens_to_generate\"] -= num_generated_tokens\n                if request[\"tokens_to_generate\"] <= 0:\n                    break\n                if not current_output_segment:\n                    break\n\n                # openai and trtllm don't show what stop word was triggered, so we assume that it was `code_end`\n                # if there's an unfinished code block\n                if current_output_segment.count(code_end) + 1 == current_output_segment.count(code_begin):\n                    current_output_segment += code_end\n                    yield {\"generation\": code_end}\n\n                # Update the prompt based on format\n                if is_openai_format:\n                    current_full_prompt.append({\"role\": \"assistant\", \"content\": current_output_segment})\n                    current_full_prompt.append({\"role\": \"user\", \"content\": \"continue\"})\n                else:\n                    current_full_prompt += current_output_segment\n\n                if generation_index == effective_max_code_executions:\n                    # This was the last iteration, intended for final text generation after all code executions.\n                    break\n\n                if current_output_segment.endswith(code_end) and current_output_segment.rfind(\n                    code_begin\n                ) > current_output_segment.rfind(code_end, 0, -1):\n                    execution_dict, session_id = await self.sandbox.execute_code(\n                        generated_code=extract_code_to_execute(current_output_segment, code_begin, code_end),\n                        language=self.config.code_execution_language,\n                        timeout=self.config.code_execution_timeout,\n                        max_output_characters=self.config.max_code_output_characters,\n                        session_id=session_id,\n                        traceback_verbosity=self.config.sandbox_traceback_verbosity,\n                    )\n\n                    remaining_code_executions = None\n                    if self.config.add_remaining_code_executions:\n                        remaining_code_executions = effective_max_code_executions - generation_index - 1\n\n                    formatted_code_output = format_code_output(\n                        execution_dict,\n                        code_output_begin,\n                        code_output_end,\n                        code_output_format,\n                        remaining_code_executions,\n                    )\n                    yield {\"generation\": formatted_code_output}  # Yield the entire formatted code output as one chunk\n\n                    # Append executed code's output to the prompt\n                    if is_openai_format:\n                        current_full_prompt[-2][\"content\"] += formatted_code_output\n                    else:\n                        current_full_prompt += formatted_code_output\n                else:  # if no code was generated, we need to finish\n                    break\n        finally:\n            if session_id is not None and self.config.code_execution_language == \"ipython\":\n                await self.sandbox.delete_session(str(session_id))\nFile Path: nemo_skills/code_execution/utils.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,n\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport re\nfrom typing import Tuple\n\nfrom nemo_skills.utils import get_logger_name\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\ndef format_code_output(\n    execution_dict,\n    code_output_begin: str,\n    code_output_end: str,\n    code_output_format: str = \"llama\",\n    remaining_code_executions: int | None = None,\n):\n    \"\"\"Formatting code output to be displayed as an llm expects it.\"\"\"\n    remaining_ce_string = \"\"\n    if remaining_code_executions is not None:\n        if remaining_code_executions > 0:\n            remaining_ce_string = (\n                f\"```system\\n\"\n                f\"Remaining code executions: {remaining_code_executions}. \"\n                f\"You will not be able to call code when you run out of executions, so use it wisely. \"\n                f\"Note that you can still continue solving the problem without code after that.\\n\"\n                f\"```\\n\"\n            )\n        else:\n            remaining_ce_string = (\n                \"```system\\n\"\n                \"You have run out of code executions! You can no longer write or execute code. \"\n                \"Now you should continue solving the problem by relying on your mathematical reasoning and analytical skills.\\n\"\n                \"```\\n\"\n            )\n    if code_output_format == \"llama\":\n        output = execution_dict[\"process_status\"]\n        if execution_dict[\"stdout\"]:\n            output += f\"\\n[stdout]\\n{execution_dict['stdout']}[/stdout]\"\n        if execution_dict[\"stderr\"]:\n            output += f\"\\n[stderr]\\n{execution_dict['stderr']}[/stderr]\"\n        output = f\"{code_output_begin}\\n\\n{output}{remaining_ce_string}{code_output_end}\\n\\n\"\n    elif code_output_format == \"qwen\":\n        output = \"\"\n        if execution_dict[\"stdout\"]:\n            output += f\"{execution_dict['stdout']}\"\n        if execution_dict[\"stderr\"]:\n            output += f\"{execution_dict['stderr']}\"\n        output = f\"{code_output_begin}{output}{code_output_end}{remaining_ce_string}\"\n    else:\n        raise ValueError(f\"Unknown code_output_format: {code_output_format}\")\n\n    # wrapping with code output separators\n    return output\n\n\ndef _extract_between_separators(generation: str, separators: Tuple[str, str], extract_all: bool = False):\n    \"\"\"Extracting all text between last occurrence of separators[0] and [1].\n\n    If extract_all is True, returning a list with all occurrences of text between separators.\n    \"\"\"\n    if extract_all:\n        separators = [re.escape(sp) for sp in separators]\n        pattern = f\"{separators[0]}(.*?){separators[1]}\"\n        return re.findall(pattern, generation, re.DOTALL)\n    return generation.split(separators[0])[-1].split(separators[1])[0]\n\n\ndef extract_code_to_execute(generation: str, code_begin: str, code_end: str, extract_all: bool = False):\n    return _extract_between_separators(generation, [code_begin, code_end], extract_all)\n\n\ndef extract_code_output(generation: str, code_output_begin: str, code_output_end: str, extract_all: bool = False):\n    return _extract_between_separators(generation, [code_output_begin, code_output_end], extract_all)\n\n\ndef extract_code_block(text: str, languages=None, extract_code_mode: str = \"last\") -> str:\n    if languages is None:\n        languages = [\"\"]\n    for language in languages:\n        matches = re.findall(rf\"```{\\s*language}\\s*\\n?(.*?)\\n?```\", text, re.DOTALL)\n        if matches:\n            idx = 0 if extract_code_mode == \"first\" else -1\n            return matches[idx].strip()\n    return \"\"\n\n\ndef clean_formal_generation(\n    generation: str,\n    final_answer_key: str | None = None,\n    extract_code_mode: str = \"last\",\n) -> str:\n    # Extract part after final_answer_key if present and configured\n    if final_answer_key and final_answer_key in generation:\n        generation = generation.split(final_answer_key, 1)[1].strip()\n\n    languages = [\"lean4\", \"lean3\", \"lean\", \"\"]\n    extracted_code = extract_code_block(generation, languages, extract_code_mode=extract_code_mode)\n    if extracted_code:\n        return extracted_code\n\n    # If no explicit code block, remove any surrounding triple backticks\n    return re.sub(r\"^\\s*```(?:lean4|lean3|lean)?\\s*|\\s*```[\\s]*$\", \"\", generation).strip()\nFile Path: nemo_skills/prompt/utils.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport json\nimport logging\nimport random\nimport re\nfrom dataclasses import asdict, field\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nimport yaml\nfrom transformers import AutoTokenizer\n\nfrom nemo_skills.code_execution.utils import format_code_output\nfrom nemo_skills.prompt.few_shot_examples import examples_map\nfrom nemo_skills.utils import get_logger_name, nested_dataclass\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\nclass BM25Retriever:\n    def __init__(self, data_path: str, field: str):\n        from rank_bm25 import BM25Okapi\n\n        with open(data_path, \"rt\", encoding=\"utf-8\") as fin:\n            self.entries = [json.loads(x) for x in fin]\n\n        corpus = [entry[field] for entry in self.entries]\n        tokenized_corpus = [doc.split(\" \") for doc in corpus]\n        self.bm25 = BM25Okapi(tokenized_corpus)\n\n    def retrieve(self, query: str, top_k: int = 1):\n        tokenized_query = query.split(\" \")\n        return self.bm25.get_top_n(tokenized_query, self.entries, n=top_k)\n\n\n@nested_dataclass(kw_only=True)\nclass FewShotExamplesConfig:\n    prefix: str = \"\"\n    template: str = \"\"\n    suffix: str = \"\"\n\n    examples_type: Optional[str] = None\n\n    retrieval_field: Optional[str] = None  # e.g. question, reference_solution, etc.\n    retrieval_file: Optional[str] = None  # needs to be provided if retrieval_field is not None\n    retrieved_entries: int = 10  # need to set higher than few_shots to filter out exact matches\n    retrieved_few_shots: int = 5\n    randomize_retrieved_entries: bool = False\n    max_retrieved_chars: int = 100000000  # no limit by default\n    max_retrieved_chars_field: str = \"reference_solution\"\n    retriever: Optional[Any] = None\n\n    def __post_init__(self):\n        \"\"\"Error checks + building example_dicts and retriever if needed.\"\"\"\n        if self.examples_type is not None and self.retriever is not None:\n            raise ValueError(\"examples_type and retriever cannot be used together\")\n\n        if self.retriever is not None:\n            return\n\n        if self.retrieval_field is not None:  # building retriever\n            if self.retrieval_file is None:\n                raise ValueError(\"retrieval_file must be provided if retrieval_field is not None\")\n            self.retriever = BM25Retriever(self.retrieval_file, field=self.retrieval_field)\n        else:\n            if self.retrieval_file is not None:\n                raise ValueError(\"retrieval_field must be provided if retrieval_file is not None\")\n\n\n@nested_dataclass(kw_only=True)\nclass CodeTags:\n    # used to execute code within these tags\n    code_begin: str = \"```python\\n\"\n    code_end: str = \"```\\n\"\n\n    # used to extract the code output\n    code_output_begin: str = \"```output\\n\"\n    code_output_end: str = \"```\\n\"\n\n    # used to post-process code output\n    code_output_format: str = \"qwen\"\n\n\n@nested_dataclass(kw_only=True)\nclass PromptConfig:\n    user: str\n    system: str | None = None\n    code_tags: CodeTags = None\n    few_shot_examples: FewShotExamplesConfig = field(default_factory=FewShotExamplesConfig)\n\n\nclass Prompt:\n    def __init__(self, config, tokenizer):\n        # rebuilding prompt config to make sure post init is called again in\n        # case some parameters were manually changed after the config was created\n        self.config = PromptConfig(_init_nested=True, **asdict(config))\n        self.tokenizer = tokenizer\n        if self.tokenizer:\n            # assuming it's the object already if not str\n            if isinstance(self.tokenizer, str):\n                self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer)\n\n    def build_filled_example(self, example_dict: Dict[str, Any]) -> str:\n        \"\"\"Builds a filled example string based on the example dictionary.\"\"\"\n\n        # replacing code/code-output separators in the examples if present\n        example_dict = example_dict.copy()\n        if \"solution\" in example_dict and self.config.code_tags:\n\n            def replace_code_output(match):\n                code_output = match.group(2)\n                formatted_output = format_code_output(\n                    execution_dict={\"process_status\": \"completed\", \"stdout\": code_output, \"stderr\": \"\"},\n                    code_output_begin=self.config.code_tags.code_output_begin,\n                    code_output_end=self.config.code_tags.code_output_end,\n                    code_output_format=self.config.code_tags.code_output_format,\n                )\n                return formatted_output\n\n            pattern = r\"({code_output_begin}\\n)(.*?)({\\s*code_output_end})\"\n            example_dict[\"solution\"] = re.sub(pattern, replace_code_output, example_dict[\"solution\"], flags=re.DOTALL)\n\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\n                \"{code_begin}\", self.config.code_tags.code_begin\n            )\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\"{code_end}\", self.config.code_tags.code_end)\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\"{code_output_begin}\", \"\")\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\"{code_output_end}\", \"\")\n\n        return self.config.few_shot_examples.template.format(**example_dict)\n\n    def build_examples_dict(self, input_dict):\n        if self.config.few_shot_examples.examples_type:\n            return examples_map[self.config.few_shot_examples.examples_type.format(**input_dict)]\n\n        if self.config.few_shot_examples.retriever is None:\n            return []\n\n        example_dicts = self.config.few_shot_examples.retriever.retrieve(\n            query=input_dict[self.config.few_shot_examples.retrieval_field],\n            top_k=self.config.few_shot_examples.retrieved_entries,\n        )\n        reference = input_dict[self.config.few_shot_examples.retrieval_field]\n        # filtering exact match if it's there\n        while example_dicts and example_dicts[0][self.config.few_shot_examples.retrieval_field] == reference:\n            example_dicts = example_dicts[1:]\n\n        # removing too long solutions\n        example_dicts = [\n            example_dict\n            for example_dict in example_dicts\n            if len(example_dict[self.config.few_shot_examples.max_retrieved_chars_field])\n            < self.config.few_shot_examples.max_retrieved_chars\n        ]\n\n        if len(example_dicts) < self.config.few_shot_examples.retrieved_few_shots:\n            LOG.warning(\n                'Too little examples (%d) found for the query \"%s\"',\n                len(example_dicts),\n                input_dict[self.config.few_shot_examples.retrieval_field],\n            )\n\n        # let's reverse the order to show the most relevant last\n        examples = example_dicts[: self.config.few_shot_examples.retrieved_few_shots][::-1]\n        if self.config.few_shot_examples.randomize_retrieved_entries:\n            random.shuffle(examples)\n\n        return examples\n\n    def build_user_message(self, input_dict: Dict[str, str]) -> str:\n        \"\"\"Builds all examples string concatenated by delimiter.\"\"\"\n        example_dicts = self.build_examples_dict(input_dict)\n\n        filled_examples = \"\".join([self.build_filled_example(example) for example in example_dicts])\n        if not filled_examples:\n            examples = \"\"\n        else:\n            examples = f\"{self.config.few_shot_examples.prefix}{filled_examples}{self.config.few_shot_examples.suffix}\"\n        user_message = self.config.user.format(examples=examples, **input_dict)\n        return user_message\n\n    def get_code_execution_args(self):\n        \"\"\"Returns the code execution arguments.\"\"\"\n        if self.config.code_tags is None:\n            raise ValueError(\n                \"Please provide 'code_tags' in your prompt configuration before calling get_code_execution_args().\"\n            )\n        return {\n            \"code_begin\": self.config.code_tags.code_begin,\n            \"code_end\": self.config.code_tags.code_end,\n            \"code_output_begin\": self.config.code_tags.code_output_begin,\n            \"code_output_end\": self.config.code_tags.code_output_end,\n            \"code_output_format\": self.config.code_tags.code_output_format,\n        }\n\n    def format_assistant_response(\n        self, content: str, thinking: str | None = None, chat_template_kwargs: dict | None = None\n    ) -> str:\n        \"\"\"Adds special tokens to the end of assistant response and formats thinking if provided\"\"\"\n        if self.tokenizer is None:\n            raise ValueError(\"Tokenizer is not set.\")\n\n        messages = [{\"role\": \"user\", \"content\": \"\"}]\n\n        chat_template_kwargs = chat_template_kwargs or {}\n        user_string = self.tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True, **chat_template_kwargs\n        )\n\n        messages.append({\"role\": \"assistant\", \"content\": content})\n        if thinking is not None:\n            messages[-1][\"thinking\"] = thinking\n        assistant_string = self.tokenizer.apply_chat_template(messages, tokenize=False, **chat_template_kwargs)\n\n        assert assistant_string.startswith(user_string), f\"Something is wrong\\n{user_string}\\n||\\n{assistant_string}\"\n\n        formatted_response = assistant_string[len(user_string) :]\n        if thinking is not None:\n            # Check that thinking is part of the assistant string\n            # If not, the thinking string should be added to the \"content\" field during preprocessing\n            # Tokenizers for models like Qwen3-4B don't add thinking to the assistant string by themselves\n            assert thinking in assistant_string, (\n                f\"The thinking content is not part of the assistant string. We suggest you add the thinking string to the 'content' field during preprocessing.\\n\\nThinking string:{thinking}\\n\\nAssistant string:{formatted_response}\"\n            )\n\n        return formatted_response\n\n    def fill(\n        self,\n        input_dict: Dict[str, str],\n        start_assistant_response_key: str | None = None,\n        chat_template_kwargs: dict | None = None,\n        format_as_string=False,\n    ) -> str | List[dict]:\n        \"\"\"\n        Fills the prompt with the input_dict.\n        Operates in two modes:\n        - If `self.tokenizer` is set, it will use it to format the prompt, returning a string.\n        - If `self.tokenizer` is not set, it will assume chat format and return a list of dictionaries.\n\n        Args:\n            input_dict: The input dictionary to fill the prompt with.\n            start_assistant_response_key: Whether to append the value of this key to the beginning of assistant response.\n            chat_template_kwargs: Any extra parameters to pass to the tokenizer's apply_chat_template method.\n            format_as_string: When False (default) we will just return a list of messages, when format_as_string is True, we will return a string using the tokenizer's apply_chat_template method (and fail if the tokenizer is not set).\n\n        Returns:\n            The filled prompt - either a string or a list of dictionaries.\n        \"\"\"\n\n        if self.config.system is not None:\n            messages = [\n                {\"role\": \"system\", \"content\": self.config.system},\n            ]\n        else:\n            messages = []\n        messages.append({\"role\": \"user\", \"content\": self.build_user_message(input_dict)})\n\n        if not format_as_string:\n            if start_assistant_response_key:\n                raise ValueError(\"start_assistant_response_key is not supported for chat template format.\")\n\n            if chat_template_kwargs:\n                raise ValueError(\"chat_template_kwargs can only be used when format_as_string=True\")\n\n            return messages\n        else:\n            if self.tokenizer is None:\n                raise ValueError(\"tokenizer is not set, can't format messages as a string\")\n\n            chat_template_kwargs = chat_template_kwargs or {}\n            try:\n                messages_string = self.tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False,\n                    add_generation_prompt=True,\n                    **chat_template_kwargs,\n                )\n            except ValueError as e:\n                if \"Cannot use chat template functions because tokenizer.chat_template is not set\" in str(e):\n                    # assuming that's a base model and we just need to add bos\n                    if len(messages) != 1 or messages[0][\"role\"] != \"user\":\n                        raise ValueError(\n                            \"The model doesn't support chat template, can't format messages which contain non-user values\"\n                        )\n                    if hasattr(self.tokenizer, \"bos_token\"):\n                        messages_string = self.tokenizer.bos_token + messages[0][\"content\"]\n                    else:\n                        messages_string = messages[0][\"content\"]\n                else:\n                    raise e\n            if start_assistant_response_key:\n                messages_string += input_dict[start_assistant_response_key]\n            return messages_string\n\n    def __str__(self):\n        return str(self.config)\nFile Path: nemo_skills/prompt/few_shot_examples/gsm8k.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Source https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/gsm8k-cot.yaml\n# Derived from the CoT paper - https://arxiv.org/pdf/2201.11903 - Table 20\ngsm8k_standard_few_shot = [\n    {\n        \"problem\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n        \"solution\": \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is \\boxed{6}.\",\n    },\n    {\n        \"problem\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n        \"solution\": \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is \\boxed{5}.\",\n    },\n    {\n        \"problem\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n        \"solution\": \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is \\boxed{39}.\",\n    },\n    {\n        \"problem\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n        \"solution\": \"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is \\boxed{8}.\",\n    },\n    {\n        \"problem\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n        \"solution\": \"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is \\boxed{9}.\",\n    },\n    {\n        \"problem\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n        \"solution\": \"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is \\boxed{29}.\",\n    },\n    {\n        \"problem\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n        \"solution\": \"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is \\boxed{33}.\",\n    },\n    {\n        \"problem\": \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\",\n        \"solution\": \"Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is \\boxed{8}.\",\n    },\n]\n\n\ngsm8k_text_with_code = [\n    {\n        \"problem\": \"Missy had a giant piggy bank in her bedroom.  Every day she would search the house for change to put in her bank.  After 4 years, the bank was opened and it contained $450 in change.  If the second, third, and fourth-year she doubled the amount of money she put in the bank from the amount she had put in the previous year, how much money, in dollars, did she put in the bank the first year?\",\n        \"solution\": \"\"\"\nMissy started with some amount of money in the first year.\nShe was then doubling the amount of money every year for 4 years and ended up with $450.\nLet's write down an equation for this problem and solve it using sympy.\n{code_begin}from sympy import solve, symbols\nfirst_year_money = symbols('first_year_money')\nsecond_year_money = 2 * first_year_money\nthird_year_money = 2 * second_year_money\nfourth_year_money = 2 * third_year_money\n# Solve for first year money given that the combined saving is 450\nresult = solve(first_year_money + second_year_money + third_year_money + fourth_year_money - 450, first_year_money)[0]\nresult\n{code_end}{code_output_begin}\n30\n{code_output_end}\nThus Missy put \\boxed{30} dollars in the bank the first year.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Pete has to take a 10-minute walk down to the train station and then board a 1hr 20-minute train to LA. When should he leave if he cannot get to LA later than 0900 hours? (24-hr time)\",\n        \"solution\": \"\"\"\nSince Pete needs to take a 10 minutes walk and then a 1 hour 20 minutes train ride, he will spend a total of 1 hour and 30 minutes.\nThis means that he needs to leave 1 hour and 30 minutes earlier than 09:00 hours.\nSubtracting 1 hour and 30 minutes from 09:00 hours we get \\boxed{07:30} hours.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Mark deposited $88 in a bank. Bryan deposited $40 less than five times as much as Mark. How much did Bryan deposit in the bank?\",\n        \"solution\": \"\"\"\nLet's solve this problem using Python code.\n{code_begin}mark_deposit = 88\nfive_times_mark_deposit = 5 * mark_deposit\nbryan_deposit = five_times_mark_deposit - 40\nbryan_deposit\n{code_end}{code_output_begin}\n400\n{code_output_end}\nSo Bryan deposited \\boxed{400} dollars in the bank.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A department store displays a 20% discount on all fixtures. What will be the new price of a 25 cm high bedside lamp that was worth $120?\",\n        \"solution\": \"\"\"\nLet's solve this problem using Python code.\n{code_begin}discount_percent = 20\nprice_before_discount = 120\ndiscount_portion = discount_percent / 100\ndiscount_amount = price_before_discount * discount_portion\nprice_after_discount = price_before_discount - discount_amount\nprice_after_discount\n{code_end}{code_output_begin}\n96.0\n{code_output_end}\nSo the new price of the lamp is \\boxed{96} dollars.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"James opens up a flower shop.  He needs to pay rent of $1200 a week with an additional 20% of rent to pay for utilities and he has 2 employees per shift with the store open 16 hours a day for 5 days a week.  If he pays each employee $12.50 an hour, what are his weekly expenses to run the store?\",\n        \"solution\": \"\"\"\nThe cost consists of rent, utilities, and employee salaries. Let's compute each of them separately and then add them up.\n{code_begin}# rent cost\nrent_per_week = 1200\n# utility cost\nutility_per_week = rent_per_week * 20 / 100\n# employee cost\nemployee_work_hours = 16\nwork_days_per_week = 5\nemployee_work_hours_per_week = work_days_per_week * employee_work_hours\nnumber_of_employees = 2\nemployee_cost_per_hour = 12.5\nemployees_cost_per_week = number_of_employees * employee_work_hours_per_week * employee_cost_per_hour\n# add the three to get total cost\ncost_per_week = rent_per_week + utility_per_week + employees_cost_per_week\ncost_per_week\n{code_end}{code_output_begin}\n3440.0\n{code_output_end}\nThus James's weekly expenses add up to \\boxed{3440} dollars.\n\"\"\".strip(),\n    },\n]\n\n\ngsm8k_text_detailed = [\n    {\n        \"problem\": \"Missy had a giant piggy bank in her bedroom.  Every day she would search the house for change to put in her bank.  After 4 years, the bank was opened and it contained $450 in change.  If the second, third, and fourth-year she doubled the amount of money she put in the bank from the amount she had put in the previous year, how much money, in dollars, did she put in the bank the first year?\",\n        \"solution\": \"\"\"\nLet $x$ be the amount of money Missy put in the bank the first year.\nShe was then doubling the amount of money every year for 4 years.\nSo in the second year she put $2*x$ amount in her bank.\nIn the third year she put $2*2*x = 4*x$ amount in her bank.\nAnd in the fourth year she put $2*4*x = 8*x$ amount in her bank.\nSo the total amount she put in the bank is $x + (2*x) + (4*x) + (8*x) = 15*x$.\nAs the problem states, this total amounts to $450.\nSo $450 = 15*x$ which implies $x = 30$.\nThus, Missy put \\boxed{30} dollars in the bank the first year.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Pete has to take a 10-minute walk down to the train station and then board a 1hr 20-minute train to LA. When should he leave if he cannot get to LA later than 0900 hours? (24-hr time)\",\n        \"solution\": \"\"\"\nSince Pete needs to take a 10 minutes walk and then a 1 hour 20 minutes train ride, he will spend a total of 1 hour and 30 minutes.\nThis means that he needs to leave 1 hour and 30 minutes earlier than 09:00 hours.\nSubtracting 1 hour and 30 minutes from 0900 hours we get \\boxed{07:30} hours.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Mark deposited $88 in a bank. Bryan deposited $40 less than five times as much as Mark. How much did Bryan deposit in the bank?\",\n        \"solution\": \"\"\"\nFive times of what Mark deposited is $88 * 5 = 440$.\nBryan deposited $440 - 40 = 400$.\nSo Bryan deposited \\boxed{400} dollars in the bank.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A department store displays a 20% discount on all fixtures. What will be the new price of a 25 cm high bedside lamp that was worth $120?\",\n        \"solution\": \"\"\"\nThe lamp was originally priced at $120.\nA 20% discount amounts to $120 * 20 / 100 = 24$.\nSo the discount reduces the price of the lamp to $120 - 24 = 96$.\nSo the new price of the lamp is \\boxed{96} dollars.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"James opens up a flower shop.  He needs to pay rent of $1200 a week with an additional 20% of rent to pay for utilities and he has 2 employees per shift with the store open 16 hours a day for 5 days a week.  If he pays each employee $12.50 an hour, what are his weekly expenses to run the store?\",\n        \"solution\": \"\"\"\nThe store expense consist of rent, utilities, and employee salaries.\nLet's compute each of these expenses separately at a week timescale and then add them up.\nThe expense due to rent is $1200.\nThe expense due to utilities is 20% of rent expense. Thus, it is $1200 * 20 / 100 = 240$.\nNow we calculate the expense due to employee salaries.\nEach employee works 16*5=80 hours per week.\nFor each employee the cost per week based on hourly wage of $12.5/hr is $12.5 * 80 = 1000$ per week.\nFor two employees, this amounts to $2 * 1000 = 2000$.\nAdding the cost of rent, utilities, and employee salaries amounts to $1200 + 240 + 2000 = 3440$.\nThus James's weekly expenses to run the store add up to \\boxed{3440} dollars.\n\"\"\".strip(),\n    },\n]\n\n\ngsm8k_problem_augmentation = [\n    {\n        \"problem\": \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\",\n        \"augmented_problem\": \"Aiden has $35. He purchased eight pencils for $2 each and a notebook for $5. How much money does he have remaining?\",\n    },\n    {\n        \"problem\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n        \"augmented_problem\": \"Sarah collected 72 seashells during her beach vacation. On Thursday, she gave 15 seashells to her friend as a souvenir. On Friday, she found 8 more seashells while exploring the shore. How many seashells did Sarah have at the end of Friday?\",\n    },\n    {\n        \"problem\": \"Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week. They have 2 chapters of their textbook to study and 4 worksheets to memorize. They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet. If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?\",\n        \"augmented_problem\": \"Samantha and David are preparing for their upcoming science fair project. They have four different experiments to conduct and a research paper to write. Each experiment is estimated to take 2 hours, and the research paper will require 8 hours to complete. To stay focused and productive, they plan to take a 15-minute break for every 1.5 hours of work and have three 20-minute snack breaks each day. Additionally, they allocate 45 minutes for lunch each day. If they want to limit their daily study time to 5 hours, how many days should they plan to work on their project over the next two weeks?\",\n    },\n    {\n        \"problem\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n        \"augmented_problem\": \"Tom has 50 marbles, and his friend Jerry has 65 marbles. If they decide to play a game and bet 20 marbles each, how many marbles will they have left in total after the game?\",\n    },\n    {\n        \"problem\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n        \"augmented_problem\": \"In a garden, there are 25 rose bushes. The gardener plans to plant some more rose bushes today. After planting, there will be a total of 40 rose bushes in the garden. How many rose bushes will the gardener plant today?\",\n    },\n    {\n        \"problem\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n        \"augmented_problem\": \"Sarah had 35 marbles. She gave some marbles to her friend Emma. Now Sarah has 18 marbles left. How many marbles did Sarah give to Emma?\",\n    },\n    {\n        \"problem\": \"Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He rearranged five of these boxes into packages of six highlighters each and sold them for $3 per package. He sold the rest of the highlighters separately at the rate of three pens for $2. How much profit did he make in total, in dollars?\",\n        \"augmented_problem\": \"Amy purchased 8 crates, each containing 24 colorful markers, for $12 per crate. She decided to create sets of 4 markers each and sell them for $2 per set. The remaining markers she sold individually at a rate of 5 markers for $3. Calculate the total profit Amy made, in dollars.\",\n    },\n    {\n        \"problem\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n        \"augmented_problem\": \"In a garden, there are 25 rose bushes. The gardener plans to plant some more rose bushes today. After planting, there will be a total of 40 rose bushes in the garden. How many rose bushes will the gardener plant today?\",\n    },\n]\n\n\nexamples_map = {\n    \"gsm8k_standard_few_shot\": gsm8k_standard_few_shot,\n    \"gsm8k_text_with_code\": gsm8k_text_with_code,\n    \"gsm8k_problem_augmentation\": gsm8k_problem_augmentation,\n    \"gsm8k_text_detailed\": gsm8k_text_detailed,\n}\nFile Path: nemo_skills/prompt/few_shot_examples/math.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Source https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/minerva_math/utils.py#L42\nmath_standard_few_shot = [\n    {\n        \"problem\": \"Find the domain of the expression  $\\frac{\\sqrt{x-2}}{\\sqrt{5-x}}$.}\",\n        \"solution\": \"The expressions inside each square root must be non-negative. Therefore, $x-2 \\ge 0$, so $x\\ge2$, and $5 - x \\ge 0$, so $x \\le 5$. Also, the denominator cannot be equal to zero, so $5-x>0$, which gives $x<5$. Therefore, the domain of the expression is $\\boxed{[2,5)}$.\",\n    },\n    {\n        \"problem\": \"If $\\det \\mathbf{A} = 2$ and $\\det \\mathbf{B} = 12,$ then find $\\det (\\mathbf{A} \\mathbf{B}).$\",\n        \"solution\": \"We have that $\\det (\\mathbf{A} \\mathbf{B}) = (\\det \\mathbf{A})(\\det \\mathbf{B}) = (2)(12) = \\boxed{24}.$\",\n    },\n    {\n        \"problem\": \"Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?\",\n        \"solution\": \"If Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\\cdot 12\\cdot20=480$ pounds of weight.  If he lifts two 15-pound weights instead for $n$ times, he will lift a total of $2\\cdot15\\cdot n=30n$ pounds of weight.  Equating this to 480 pounds, we can solve for $n$:\\n\\begin{align*}\\n30n&=480\\\\\\n\\Rightarrow\\qquad n&=480/30=\\boxed{16}\\n\\end{align*}.\",\n    },\n    {\n        \"problem\": \"If the system of equations\\n\\n\\begin{align*}6x-4y&=a,\\\\\\n6y-9x &=b.\\n\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero,\\nfind $\\frac{a}{b},$ assuming $b$ is nonzero.\",\n        \"solution\": \"If we multiply the first equation by $-\\frac{3}{2}$, we obtain\\n\\n$$6y-9x=-\\frac{3}{2}a.$$Since we also know that $6y-9x=b$, we have\\n\\n$$-\\frac{3}{2}a=b\\Rightarrow\\frac{a}{b}=\\boxed{-\\frac{2}{3}}.$$.\",\n    },\n]\n\n\nmath_text_detailed = [\n    {\n        \"problem\": \"A parabola with equation $y=x^2+bx+c$ passes through the points $(-1,-11)$ and $(3,17)$. What is $c$?\",\n        \"solution\": \"\"\"\nFrom the question we know that points $(-1, -11)$ and $(3, 17)$ lie on the parabola. This means that when we substitute $x$ and $y$ from these points into the equation $y = x^2 + bx + c$, the equation must hold true. We substitute these two points into the given equation to solve for $c$.\n\nFor the point $(-1, -11)$:\n\nSubstitute $x = -1$ and $ y = -11 $ into the equation:\n\\[ -11 = (-1)^2 + b(-1) + c \\Rightarrow -11 = 1 - b + c \\Rightarrow -b + c = -12 \\]\n\nFor the point $(3, 17)$:\n\nSubstitute $x = 3$ and $y = 17$ into the equation:\n\\[ 17 = (3)^2 + b(3) + c \\Rightarrow 17 = 9 + 3b + c \\Rightarrow 3b + c = 8 \\]\n\nIn summary, we have the two equations\n\\begin{align*}\\n-b + c &= -12\\\\\\n3b + c &= 8\\n\\end{align*}\n\nTo solve for $c$ we can eliminate $b$ by multiplying the first equation by 3 and adding equations together.\nMultiplying the first equation by 3, we have $3(-b + c) = 3 (-12) \\Rightarrow -3b + 3c = -36$. Adding equations together gives us\n\\[ (-3b + 3c) + (3b + c) = -36 + 8 \\Rightarrow -3b + 3b + 3c + c = -28 \\Rightarrow 4c = -28 \\Rightarrow c = -28 : 4 \\Rightarrow c = \\boxed{-7} \\]\n\"\"\".strip(),\n    },\n    {\n        \"problem\": 'Let $f(x)$ be an odd function.  Is $f(f(x))$ even, odd, or neither?\\n\\nEnter \"odd\", \"even\", or \"neither\".',\n        \"solution\": \"\"\"\nTo determine whether $f(f(x))$ is even, odd, or neither, we need to use the property of $f(x)$ being an odd function.\n\nAn odd function is defined as:\n\\[ f(-x) = -f(x) \\quad \\text{for all } x \\]\n\nGiven that $f(x)$ is odd, let's find $f(f(-x))$ and see how it relates to $f(f(x))$.\n\n1. Substitute $-x$ into the function $f(x)$:\n\\[ f(-x) \\]\n\n2. Since $f(x)$ is odd, apply the definition of an odd function:\n\\[ f(-x) = -f(x) \\]\n\n3. Now substitute $-f(x)$ into the function $f$:\n\\[ f(f(-x)) = f(-f(x)) \\]\n\n4. Again, using the fact that $f(x)$ is odd, apply the definition:\n\\[ f(-f(x)) = -f(f(x)) \\]\n\n5. We have found that:\n\\[ f(f(-x)) = -f(f(x)) \\]\n\nThis matches the definition of an odd function.\n\nSo, the answer is:\n\\[ \\boxed{\\text{odd}} \\]\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A rectangular box $P$ is inscribed in a sphere of radius $r$. The surface area of $P$ is 384, and the sum of the lengths of its 12 edges is 112. What is $r$?\",\n        \"solution\": \"\"\"\nLet the dimensions of the rectangular box $P$ be $x$, $y$, and $z$. We know the following:\n\n1. The sum of the lengths of the edges of $P$ is\n\\[ 4(x + y + z) = 112 \\Rightarrow x + y + z = 112 : 4 \\Rightarrow x + y + z = 28 \\]\n\n2. The surface area of $P$ is\n\\[ 2xy + 2yz + 2xz = 384 \\Rightarrow xy + yz + xz = 384 : 2 \\Rightarrow xy + yz + xz = 192 \\]\n\nSince the box is inscribed in the sphere, the diagonal of the box is the diameter of the sphere. The length of the diagonal is $\\sqrt{x^2 + y^2 + z^2}$\n\nThe diameter of the sphere is $2r$, so:\n\\[ (2r)^2 = x^2 + y^2 + z^2 = (x + y + z)^2 - (2xy + 2yz + 2xz) \\]\n\nSubstitute the known values:\n\\[ 4r^2 = 28^2 - 384 = 784 - 384 = 400 \\Rightarrow r^2 = 100 \\Rightarrow r = \\boxed{10} \\]\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Let $\\mathbf{a} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\end{pmatrix}.$  Find the vector $\\mathbf{b}$ such that $\\mathbf{a} \\cdot \\mathbf{b} = 11$ and\\n\\[\\mathbf{a} \\times \\mathbf{b} = \\begin{pmatrix} -13 \\\\ -9 \\\\ 7 \\end{pmatrix}.\\].\",\n        \"solution\": \"\"\"\nLet $\\mathbf{b} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}$.\n\nFirst, use the dot product condition:\n\\[ \\mathbf{a} \\cdot \\mathbf{b} = 11 \\Rightarrow 2x + y + 5z = 11 \\]\n\nNext, use the cross product condition:\n\\[ \\mathbf{a} \\times \\mathbf{b} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\end{pmatrix} \\times \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} -5y + z \\\\ 5x - 2z \\\\ -x + 2y \\end{pmatrix} = \\begin{pmatrix} -13 \\\\ -9 \\\\ 7 \\end{pmatrix} \\]\n\nThis gives us the system of equations:\n   \\begin{align*}\\n   2x + y + 5z = 11 \\quad &(1) \\\\\\n   -5y + z = -13 \\quad &(2) \\\\\\n   5x - 2z = -9 \\quad &(3) \\\\\\n   -x + 2y = 7 \\quad &(4)\\n   \\end{align*}\n\nSolve for $x$, $y$, and $z$ step-by-step:\n\nFrom (2), $z = 5y - 13$.\nFrom (4), $x = 2y - 7$.\n\nSubstitute $z = 5y - 13$ into (1):\n\\[ 2(2y - 7) + y + 5(5y - 13) = 11 \\Rightarrow 4y - 14 + y + 25y - 65 = 11 \\Rightarrow 30y = 90 \\Rightarrow y = 3 \\]\n\nNow find $x$ and $z$:\n\\[ x = 2y - 7 = 2(3) - 7 = -1 \\]\n\n\\[ z = 5y - 13 = 5(3) - 13 = 2 \\]\n\nThus, the vector $\\mathbf{b}$ is:\n\\[ \\mathbf{b} = \\boxed{\\begin{pmatrix} -1 \\\\ 3 \\\\ 2 \\end{pmatrix}} \\]\n\"\"\".strip(),\n    },\n]\n\nmath_text_with_code = [\n    {\n        \"problem\": \"A parabola with equation $y=x^2+bx+c$ passes through the points $(-1,-11)$ and $(3,17)$. What is $c$?\",\n        \"solution\": \"\"\"\nLet's write down an equation for the parabola and solve for $c$ using sympy.\n{code_begin}import sympy as sp\n\n# define the symbols\nx, y, b, c = sp.symbols('x y b c')\n\n# define the parabola equation\nparabola_eq = sp.Eq(y, x**2 + b*x + c)\n\n# the parabola passes through the points (-1,-11) and (3,17)\n# so we substitute these points into the parabola equation\npoint_1 = parabola_eq.subs({x: -1, y: -11})\npoint_2 = parabola_eq.subs({x: 3, y: 17})\n\n# we now have two equations and two unknowns (b and c)\n# we will solve for b and c\nsolutions = sp.solve((point_1,point_2), (b, c))\nsolutions[c]\n{code_end}{code_output_begin}\n-7\n{code_output_end}\nSo c is \\boxed{-7}\n\"\"\".strip(),\n    },\n    {\n        \"problem\": 'Let $f(x)$ be an odd function.  Is $f(f(x))$ even, odd, or neither?\\n\\nEnter \"odd\", \"even\", or \"neither\".',\n        \"solution\": \"\"\"\nThe definition of an odd function is that $f(-x) = -f(x)$.\nApplying this to $f(f(-x))$ we get $f(f(-x)) = f(-f(x)) = -f(f(x))$.\nThus, $f(f(x))$ is an \\boxed{odd} function.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"At the 2007 Math Olympics, Team Canada won $17$ out of a possible $100$ medals. Which one of the following is closest to the fraction of medals that they won? $$\\n\\frac{1}{4} \\qquad \\frac{1}{5} \\qquad \\frac{1}{6} \\qquad \\frac{1}{7} \\qquad \\frac{1}{8}\\n$$\",\n        \"solution\": \"\"\"\nLet's use sympy to print out the differences between the fraction of medals that Canada won and each of the options.\n{code_begin}from sympy import Rational, Abs\n\n# team Canada won 17 out of 100 medals\nmedal_frac = Rational(17, 100)\n\n# list of options\noptions = [Rational(1, 4), Rational(1, 5), Rational(1, 6), Rational(1, 7), Rational(1, 8)]\n\n# let's print out the differences\n[Abs(medal_frac - frac_option) for frac_option in options]\n{code_end}{code_output_begin}\n[2/25, 3/100, 1/300, 19/700, 9/200]\n{code_output_end}\nLet's now check which difference is the smallest.\n{code_begin}import numpy as np\n\n# Calculate the idx of the closest option\nmin_idx = np.argmin([2/25, 3/100, 1/300, 19/700, 9/200])\n\n# Print the closest option\nprint(options[min_idx])\n{code_end}{code_output_begin}\n1/6\n{code_output_end}\nSo the answer is \\boxed{1/6}.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A rectangular box $P$ is inscribed in a sphere of radius $r$. The surface area of $P$ is 384, and the sum of the lengths of its 12 edges is 112. What is $r$?\",\n        \"solution\": \"\"\"\nLet the dimensions of $P$ be $x$, $y$, and $z$.\nThe sum of the box's edges is $4(x + y + z)$ and the surface area is $2xy + 2yz + 2xz$.\nThe diameter of the sphere is $2r$ and it's equal to the diagonal of the box.\nLet's now write down the equations based on the above information and solve them using sympy.\n{code_begin}from sympy import symbols, Eq, solve\n\n# define the variables\nx, y, z, r = symbols('x y z r')\n\n# equations based on the given information\neq1 = Eq(2 * (x*y + x*z + y*z), 384)\neq2 = Eq(4 * (x + y + z), 112)\n\n# the diagonal of the box is the diameter of the sphere\ndiagonal_eq = Eq(x**2 + y**2 + z**2, (2*r)**2)\n\n# solve the equations\nsolutions = solve((eq1, eq2, diagonal_eq), (x, y, z, r))\n\n# let's see which values of r we get\n[solution[-1] for solution in solutions]\n{code_end}{code_output_begin}\n[-10, -10, 10, 10]\n{code_output_end}\nSince the radius of the sphere is positive, we get $r = \\boxed{10}$.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A bee starts flying from point $P_0$. She flies $1$ inch due east to point $P_1$. For $j \\ge 1$, once the bee reaches point $P_j$, she turns $30^{\\circ}$ counterclockwise and then flies $j+1$ inches straight to point $P_{j+1}$. When the bee reaches $P_{2015},$ how far from $P_0$ is she, in inches?\",\n        \"solution\": \"\"\"\nWe can represent the rotation action via $\\omega = e^{\\pi i/6}$.\nLet's assume the bee starts at the origin, so $P_{2015}$ is at the point \\[z = 1 + 2 \\omega + 3 \\omega^2 + 4 \\omega^3 + \\dots + 2015 \\omega^{2014}.\\]\nThis is an arithmetic-geometric series which we can solve by simplifying the expression.\nAlternatively, we can solve for |z| using sympy using the following code.\n{code_begin}from sympy import I, pi, exp, sqrt\n\n# rotation action of 30 degrees\nomega = exp(I * pi/6)\n\nposition = 0\n\nfor i in range(2015):\n    delta = (i + 1) * omega**(i)\n    position += delta\n\nreal, img = (position.as_real_imag())\n# Distance from origin i.e. starting point\ndist = sqrt(real**2 + img**2)\nprint(dist)\n{code_end}{code_output_begin}\nsqrt(2)*(1008 + 1008*sqrt(3))\n{code_output_end}\nSo the bee is $\\boxed{1008\\sqrt{2} + 1008\\sqrt{6}}$ far from the starting point.\n\"\"\".strip(),\n    },\n]\nFile Path: nemo_skills/inference/model/parallel_thinking.py\nContent:\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport glob\nimport hashlib\nimport json\nimport logging\nimport os\nimport random\nimport re\nfrom collections import defaultdict\nfrom dataclasses import field\nfrom typing import Dict, List, Optional, Union\n\nfrom transformers import AutoTokenizer\n\nfrom nemo_skills.prompt.utils import get_prompt, get_token_count\nfrom nemo_skills.utils import get_logger_name, nested_dataclass, parse_reasoning\n\nfrom .base import BaseModel, EndpointType\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\n@nested_dataclass(kw_only=True)\nclass GenSelectSpecificConfig:\n    prompt_config: str = \"generic/genselect\"\n    regex: str = r\"Judg[e]?ment: (\\d+)\"\n\n\n@nested_dataclass(kw_only=True)\nclass GenSynthesisSpecificConfig:\n    prompt_config: str = \"generic/gensynthesis\"\n    regex: str = r\"<NEW_SOLUTION>(.*?)</NEW_SOLUTION>\"\n\n\n@nested_dataclass(kw_only=True)\nclass ParallelThinkingConfig:\n    temperature: float = 0.6\n    tokens_to_generate: int | None = None\n\n    parse_reasoning: bool = False\n    parse_reasoning_solutions: bool = True  # Whether to parse the reasoning for the solutions.\n    end_reasoning_string: str = \"</think>\"\n    endpoint_type: EndpointType = EndpointType.chat\n    tokenizer: str | None = None\n    chat_template_kwargs: dict = field(default_factory=dict)\n    start_assistant_response_key: str | None = None  # whether to start assistant response with this key\n\n    # Count the number of tokens in the prompt\n    count_prompt_tokens: bool = False\n\n    # GenSelect vs GenSynthesis\n    mode: str | None = None  # genselect or gensynthesis\n\n    genselect: GenSelectSpecificConfig = field(default_factory=GenSelectSpecificConfig)\n    gensynthesis: GenSynthesisSpecificConfig = field(default_factory=GenSynthesisSpecificConfig)\n\n    # Solution related parameters\n    solution_length_cap: int | None = 16384  # If specified, will filter out solutions that are longer than this length\n    window_size: int = 8  # Number of solutions compared in a single request\n    solution_key: str = \"generation\"  # Key used for identifying the solution content\n    filter_incomplete_solutions: bool = True  # Filter out incomplete solutions\n\n    # Parameters specifically for Offline GenSelect/GenSynthesis\n    generation_dir: str | None = None  # Assumes output-rs[random_seed].jsonl files in this directory\n    num_initial_solutions: int | None = None  # If specified, will only consider this many solutions\n\n\nclass ParallelThinkingTask:\n    \"\"\"\n    Wrapper that generates/loads multiple solutions for a datapoint and uses GenSelect or GenSynthesis\n    to choose the best one or synthesize a new solution.\n    \"\"\"\n\n    def __init__(self, model: BaseModel, tokenizer: str | None, orig_prompt_filler, cfg: ParallelThinkingConfig):\n        self.model = model\n        self.orig_prompt_filler = orig_prompt_filler\n        self.cfg = cfg\n\n        self.tokenizer = tokenizer\n\n        # Load GenSelect/GenSynthesis prompt\n        if self.cfg.mode == \"genselect\":\n            self.parallel_thinking_prompt = get_prompt(\n                prompt_config=self.cfg.genselect.prompt_config, tokenizer=self.tokenizer\n            )\n        elif self.cfg.mode == \"gensynthesis\":\n            self.parallel_thinking_prompt = get_prompt(\n                prompt_config=self.cfg.gensynthesis.prompt_config, tokenizer=self.tokenizer\n            )\n        else:\n            raise ValueError(f\"Invalid parallel thinking mode: {self.cfg.mode}\")\n\n        if self.cfg.count_prompt_tokens:\n            self.hf_tokenizer = AutoTokenizer.from_pretrained(self.tokenizer)\n            if self.hf_tokenizer is None:\n                raise ValueError(\"Tokenizer could not be initialized. Needed for counting prompt tokens.\")\n\n        # Initialize the solutions if input_dir is provided\n        if self.cfg.generation_dir is not None:\n            LOG.info(\"Loading solutions from %s\", self.cfg.generation_dir)\n            self.prompt_to_solutions_dict = self._load_solutions(self.cfg.generation_dir)\n            LOG.info(\"Loaded solutions for %d prompts\", len(self.prompt_to_solutions_dict))\n\n        # TODO: These calculations will change for Parallel Thinking competition setting\n        if self.cfg.generation_dir is not None:\n            self.cfg.max_concurrent_requests = 1\n        else:\n            # We will be generating the solutions in parallel\n            self.cfg.max_concurrent_requests = self.cfg.window_size\n\n    @classmethod\n    def hash_prompt(cls, prompt: Union[str, List[dict]]) -> str:\n        \"\"\"Hash any data structure - handles strings, lists, dicts, etc.\"\"\"\n        return hashlib.md5(json.dumps(prompt, sort_keys=True, default=str).encode()).hexdigest()\n\n    async def generate_solutions(\n        self,\n        prompt: Union[str, List],\n        local_random: random.Random,\n        **solution_kwargs,\n    ) -> Dict:\n        \"\"\"\n        Generate multiple solutions for input to Parallel Thinking.\n        \"\"\"\n        # Generate multiple solutions\n        tasks = []\n        for _ in range(self.cfg.window_size):\n            # Generate solutions with different seeds for diversity\n            cur_random_seed = local_random.getrandbits(32)\n            # Create a copy to avoid mutation issues\n            current_kwargs = solution_kwargs.copy()\n            current_kwargs[\"random_seed\"] = cur_random_seed\n\n            task = self.model.generate_async(prompt=prompt, **current_kwargs)\n            tasks.append(task)\n\n        generation_results = await asyncio.gather(*tasks)\n        solutions = []\n        for generation_result in generation_results:\n            if self.cfg.parse_reasoning_solutions:\n                orig_generation = generation_result[self.cfg.solution_key]\n                parse_reasoning(\n                    generation_result,\n                    generation_key=self.cfg.solution_key,\n                    end_reasoning_string=self.cfg.end_reasoning_string,\n                )\n                if generation_result[self.cfg.solution_key] == \"\":\n                    # Revert to original generation, probably because reasoning is already parsed\n                    generation_result[self.cfg.solution_key] = orig_generation\n\n            if self.cfg.solution_length_cap is not None:\n                if len(generation_result[self.cfg.solution_key]) > self.cfg.solution_length_cap:\n                    LOG.debug(\n                        f\"Solution filtered out: length {len(generation_result[self.cfg.solution_key])} exceeds cap {self.cfg.solution_length_cap}\"\n                    )\n                    continue\n\n            solutions.append(\n                {\n                    self.cfg.solution_key: generation_result[self.cfg.solution_key],\n                    \"output_dict\": generation_result,\n                }\n            )\n\n        local_random.shuffle(solutions)\n        return solutions\n\n    def _load_solutions(self, input_dir: str) -> Dict[str, List[Dict]]:\n        \"\"\"Load the solutions from the input directory.\"\"\"\n        prompt_to_solutions_dict = defaultdict(list)\n        solution_files = glob.glob(os.path.join(input_dir, \"output-rs*.jsonl\"))\n\n        # If num_initial_solutions is specified, only load the first num_initial_solutions solutions\n        if self.cfg.num_initial_solutions is not None:\n            # Sort the solution files to ensure consistent ordering\n            solution_files.sort()\n            solution_files = solution_files[: self.cfg.num_initial_solutions]\n\n        if not solution_files:\n            raise ValueError(f\"No solutions found in {input_dir}\")\n\n        for input_file in solution_files:\n            with open(input_file, \"r\") as f:\n                for line in f:\n                    data_point = json.loads(line)\n                    if self.cfg.parse_reasoning_solutions:\n                        orig_generation = data_point[self.cfg.solution_key]\n                        parse_reasoning(\n                            data_point,\n                            generation_key=self.cfg.solution_key,\n                            end_reasoning_string=self.cfg.end_reasoning_string,\n                        )\n                        if data_point[self.cfg.solution_key] == \"\":\n                            # Revert to original generation, probably because reasoning is already parsed\n                            data_point[self.cfg.solution_key] = orig_generation\n\n                    if self.cfg.solution_length_cap is not None:\n                        if len(data_point[self.cfg.solution_key]) > self.cfg.solution_length_cap:\n                            LOG.debug(\n                                f\"Solution filtered out: length {len(data_point[self.cfg.solution_key])} exceeds cap {self.cfg.solution_length_cap}\"\n                            )\n                            continue\n\n                    # TODO: Making an assumption that the prompt doesn't require all the data for few-shot prompting\n                    # Hashing the prompt to get the key for the solutions\n                    prompt = self.hash_prompt(self.orig_prompt_filler(data_point, data=None))\n                    prompt_to_solutions_dict[prompt].append(\n                        {\n                            self.cfg.solution_key: data_point[self.cfg.solution_key],\n                            \"output_dict\": data_point,\n                        }\n                    )\n\n        return prompt_to_solutions_dict\n\n    async def _get_multiple_solutions(\n        self, prompt: Union[str, List], local_random: random.Random, **kwargs\n    ) -> tuple[List[Dict], int]:\n        \"\"\"Return multiple solutions for the input prompt.\"\"\"\n        if self.cfg.generation_dir is not None:\n            # Already have the solutions in the input directory\n            # Hashing the prompt to get the key for the solutions\n            solutions = self.prompt_to_solutions_dict[self.hash_prompt(prompt)]\n            local_random.shuffle(solutions)\n            # After shuffling, only take the first window_size solutions\n            solutions = solutions[: self.cfg.window_size]\n        else:\n            # Generate the solutions first\n            solutions = await self.generate_solutions(prompt, local_random, **kwargs)\n\n        # Filter out incomplete solutions if specified\n        if self.cfg.filter_incomplete_solutions:\n            # Remove unfinished solutions\n            filtered_solutions = []\n            for solution in solutions:\n                if solution[self.cfg.solution_key] == \"\":\n                    LOG.warning(\"Solution is empty, skipping\")\n                    continue\n                else:\n                    filtered_solutions.append(solution)\n\n            if len(filtered_solutions) < len(solutions):\n                LOG.info(f\"Filtered out {len(solutions) - len(filtered_solutions)} incomplete solutions\")\n\n            solutions = filtered_solutions\n\n        total_num_generated_tokens = 0\n        for solution in solutions:\n            total_num_generated_tokens += solution[\"output_dict\"].get(\"num_generated_tokens\", 0)\n\n        return solutions, total_num_generated_tokens\n\n    async def _generate_parallel_thinking_contraction(self, prompt: str, solutions: List[Dict], **kwargs) -> Dict:\n        \"\"\"Output which combines the solutions into a single solution/selection.\"\"\"\n\n        num_solutions = len(solutions)\n        max_idx = num_solutions - 1\n\n        formatted_solutions = []\n        for i, solution in enumerate(solutions):\n            formatted_solutions.append(f\"Solution {i}: {solution[self.cfg.solution_key]}\")\n        solutions_text = \"\\n\\n\".join(formatted_solutions)\n\n        parallel_thinking_input = {\n            \"problem\": prompt,\n            \"solutions\": solutions_text,\n            \"num_solutions\": num_solutions,\n            \"max_idx\": max_idx,\n        }\n\n        parallel_thinking_prompt = self.parallel_thinking_prompt.fill(\n            parallel_thinking_input,\n            start_assistant_response_key=self.cfg.start_assistant_response_key,\n            chat_template_kwargs=self.cfg.chat_template_kwargs,\n            format_as_string=(self.cfg.endpoint_type == EndpointType.text),\n        )\n\n        LOG.info(f\"Parallel thinking prompt:\\n\\n{parallel_thinking_prompt}\")\n\n        output_dict = {}\n        if self.cfg.count_prompt_tokens:\n            num_input_tokens = get_token_count(tokenizer=self.hf_tokenizer, messages=parallel_thinking_prompt)\n            output_dict[\"num_input_tokens\"] = num_input_tokens\n\n        for duplicate_key in [\"temperature\", \"tokens_to_generate\", \"prompt\", \"endpoint_type\"]:\n            kwargs.pop(duplicate_key, None)\n\n        LOG.info(f\"kwargs: {kwargs}\")\n        kwargs[\"endpoint_type\"] = self.cfg.endpoint_type\n\n        output_dict.update(\n            await self.model.generate_async(\n                prompt=parallel_thinking_prompt,\n                # Overriding the tokens_to_generate, temperature\n                tokens_to_generate=self.cfg.tokens_to_generate,\n                temperature=self.cfg.temperature,\n                **kwargs,\n            )\n        )\n        return output_dict\n\n    def _extract_selected_solution(self, generation: str, max_idx: int) -> Optional[int]:\n        \"\"\"Extract the selected solutions index from the GenSelect generation.\"\"\"\n        solution_idx = None\n\n        try:\n            matches = re.findall(self.cfg.genselect.regex, generation)\n            if matches:\n                number = matches[-1]\n                solution_idx = int(number)\n                if solution_idx > max_idx:\n                    return None\n\n        except Exception:\n            return None\n\n        return solution_idx\n\n    def _extract_synthesized_solution(self, generation: str) -> str:\n        \"\"\"Extract the synthesized solution from the GenSynthesis result.\"\"\"\n        matches = re.findall(self.cfg.gensynthesis.regex, generation, re.DOTALL)\n        if matches:\n            return matches[-1].strip()  # Remove any trailing newlines\n        else:\n            return None\n\n    async def _run_genselect(\n        self, prompt: str, solutions: List[Dict], local_random: random.Random, **kwargs\n    ) -> tuple[int, Dict]:\n        \"\"\"Run GenSelect to choose the best solution.\"\"\"\n\n        max_idx = len(solutions) - 1\n        genselect_result = await self._generate_parallel_thinking_contraction(\n            prompt=prompt, solutions=solutions, **kwargs\n        )\n\n        # Extract the judgment from the GenSelect result\n        sel_solution_idx = self._extract_selected_solution(genselect_result[\"generation\"], max_idx)\n        if sel_solution_idx is None:\n            LOG.warning(\"GenSelect failed to produce valid solution index, falling back to random selection\")\n            sel_solution_idx = local_random.randint(0, max_idx)\n            genselect_result[\"generation_successful\"] = False\n        else:\n            genselect_result[\"generation_successful\"] = True\n\n        return {\n            self.cfg.solution_key: solutions[sel_solution_idx][self.cfg.solution_key],\n            \"parallel_thinking_result\": genselect_result,\n        }\n\n    async def _run_gensynthesis(\n        self, prompt: str, solutions: List[Dict], local_random: random.Random, **kwargs\n    ) -> Dict:\n        \"\"\"Run GenSynthesis to synthesize a new solution from a list of candidate solutions.\"\"\"\n\n        gensynthesis_result = await self._generate_parallel_thinking_contraction(\n            prompt=prompt, solutions=solutions, **kwargs\n        )\n\n        # Extract the synthesized solution from the GenSynthesis result\n        synthesized_solution = self._extract_synthesized_solution(gensynthesis_result[\"generation\"])\n        if synthesized_solution is None:\n            LOG.warning(\"GenSynthesis failed to produce valid solution, falling back to random selection\")\n            synthesized_solution = local_random.choice(solutions)[self.cfg.solution_key]\n            # Add the boolean flag to aid analysis and debugging\n            gensynthesis_result[\"generation_successful\"] = False\n        else:\n            gensynthesis_result[\"generation_successful\"] = True\n\n        return {\n            self.cfg.solution_key: synthesized_solution,\n            \"parallel_thinking_result\": gensynthesis_result,\n        }\n\n    async def generate_async(self, prompt: Union[str, List], **kwargs):\n        \"\"\"Generate a single solution using parallel thinking.\"\"\"\n\n        result = {}\n        local_random = random.Random(kwargs.get(\"random_seed\", 0))\n\n        # Step 1: Get the multiple solutions\n        solutions, total_num_generated_tokens = await self._get_multiple_solutions(prompt, local_random, **kwargs)\n        result[\"total_solution_generated_tokens\"] = total_num_generated_tokens\n\n        if solutions is None or len(solutions) == 0:\n            output_dict = {\n                self.cfg.solution_key: \"\",\n                \"solution_list\": [],\n                f\"{self.cfg.mode}_comparison\": \"\",\n                f\"{self.cfg.mode}_num_generated_tokens\": 0,\n                f\"{self.cfg.mode}_successful\": False,\n                \"total_solution_generated_tokens\": total_num_generated_tokens,\n                \"num_generated_tokens\": total_num_generated_tokens,  # No additional tokens for genselect/gensynthesis\n                \"num_best_solution_generated_tokens\": 0,\n            }\n\n            # Required by inference/generate.py\n            output_dict[\"generation\"] = \"\"\n            if self.cfg.count_prompt_tokens:\n                # The input doesn't make sense for such cases where there are no solutions\n                output_dict[\"num_input_tokens\"] = 0\n\n            LOG.warning(\"No solutions found for the prompt, returning empty output\")\n            return output_dict\n\n        # Step 2: Run GenSelect/GenSynthesis\n\n        # If the prompt is a list, we need to get the first message's content\n        prompt_str = prompt if isinstance(prompt, str) else prompt[0][\"content\"]\n        assert isinstance(prompt_str, str), \"Prompt must be a string\"\n\n        if self.cfg.mode == \"genselect\":\n            output_dict = await self._run_genselect(prompt_str, solutions, local_random, **kwargs)\n            parallel_thinking_result = output_dict[\"parallel_thinking_result\"]\n        else:\n            # GenSynthesis\n            output_dict = await self._run_gensynthesis(prompt_str, solutions, local_random)\n            parallel_thinking_result = output_dict[\"parallel_thinking_result\"]\n\n        result[f\"{self.cfg.mode}_comparison\"] = parallel_thinking_result[\"generation\"]\n        result[f\"{self.cfg.mode}_successful\"] = parallel_thinking_result[\"generation_successful\"]\n        result[f\"{self.cfg.mode}_num_generated_tokens\"] = parallel_thinking_result.get(\"num_generated_tokens\", 0)\n\n        # Add the tokens for all the solutions and parallel thinking\n        total_gen_tokens = result[\"total_solution_generated_tokens\"] + result[f\"{self.cfg.mode}_num_generated_tokens\"]\n\n        # TODO: Decide what count of generated tokens do we want to report - the total or the best solution?\n        # Current implementation returns the total number of generated tokens\n        result[\"num_generated_tokens\"] = total_gen_tokens\n        if self.cfg.count_prompt_tokens:\n            result[\"num_input_tokens\"] = parallel_thinking_result[\"num_input_tokens\"]\n\n        result[self.cfg.solution_key] = output_dict[self.cfg.solution_key]\n        result[\"solution_list\"] = [solution[self.cfg.solution_key] for solution in solutions]\n\n        if self.cfg.solution_key != \"generation\":\n            # Add the generation key to the result since it's required by inference/generate.py\n            # We're just copying the solution key to the generation key to avoid errors\n            result[\"generation\"] = result[self.cfg.solution_key]\n\n        return result\nFile Path: nemo_skills/evaluation/math_grader.py\nContent:\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport re\n\nfrom latex2sympy2_extended import NormalizationConfig, normalize_latex\nfrom math_verify import LatexExtractionConfig, StringExtractionConfig, parse, verify\n\nfrom nemo_skills.utils import get_logger_name\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\ndef _additional_normalization(expr):\n    # Remove % and \\% from the number\n    percentage_pattern = r\"^(\\d+\\.?\\d*)(?:\\\\%|%)$\"\n    match_gt = re.fullmatch(percentage_pattern, expr)\n    if match_gt:\n        expr = match_gt.group(1)\n    # Remove . corresponding to the end of sentence\n    expr = expr.rstrip(\".\\\\\")\n    return expr\n\n\ndef math_equal(gt_answer, predicted_answer, take_modulo: int | None = None, **kwargs):\n    if predicted_answer is None:\n        return False\n\n    gt_answer = str(gt_answer)\n    predicted_answer = str(predicted_answer)\n\n    # if we are sure that gt is always integer\n    if take_modulo is not None:\n        gt_answer = int(gt_answer) % take_modulo\n        try:\n            predicted_answer = int(predicted_answer) % take_modulo\n        except Exception:\n            predicted_answer = None\n        # no need to simpy call in this case\n        return predicted_answer == gt_answer\n\n    # Try to compare as MCQ options\n    mcq_options = \"ABCDEFGHIJ\"\n    norm_gt_mcq = gt_answer.strip()\n\n    is_mcq = re.fullmatch(\"|\".join(mcq_options), norm_gt_mcq)\n    parsed_gt = parse(gt_answer, [StringExtractionConfig(strings=tuple(mcq_options))])\n    parsed_pred = parse(predicted_answer, [StringExtractionConfig(strings=tuple(mcq_options))])\n    if is_mcq and verify(parsed_gt, parsed_pred):\n        return verify(parsed_gt, parsed_pred)\n\n    # Additional normalization step\n    gt_answer = _additional_normalization(gt_answer)\n    predicted_answer = _additional_normalization(predicted_answer)\n\n    # Try literal comparison\n    literal_pattern = r\"[a-zA-Z ,]+|[0-9 ]+\"\n    normalized_gt = normalize_latex(gt_answer, NormalizationConfig)\n    normalized_pred = normalize_latex(predicted_answer, NormalizationConfig)\n    is_literal = re.fullmatch(literal_pattern, normalized_gt) and re.fullmatch(literal_pattern, normalized_pred)\n    is_normalized_equal = normalized_gt.replace(\" \", \"\") == normalized_pred.replace(\" \", \"\")\n\n    if is_literal or is_normalized_equal:\n        return is_normalized_equal\n\n    # Fallback to symbolic comparison\n    current_gt_answer = gt_answer\n    current_predicted_answer = predicted_answer\n\n    # math_verify.parse expects input to be in latex environment, e.g. $...$\n    latex_env_search_pattern = r\"\\$.*\\$|\\\\\\(.*\\\\\\)|\\\\\\\\[.*\\\\\\\\]|\\\\boxed{\"\n    if not re.search(latex_env_search_pattern, current_gt_answer, re.DOTALL):\n        current_gt_answer = f\"${current_gt_answer}$\"\n    if not re.search(latex_env_search_pattern, current_predicted_answer, re.DOTALL):\n        current_predicted_answer = f\"${current_predicted_answer}$\"\n\n    parsed_gt = parse(current_gt_answer, [LatexExtractionConfig()])\n    parsed_pred = parse(current_predicted_answer, [LatexExtractionConfig()])\n\n    return verify(parsed_gt, parsed_pred, **kwargs)\n\n\ndef extract_answer(\n    string: str, extract_from_boxed: bool = True, extract_regex: str = r\"The final answer is (.+)$\", relaxed=False\n):\n    \"\"\"Extract Answer String from \\boxed expression or based on regex\n    If relaxed=True: try both methods, boxed first.\n    If relaxed=False: use only one method based on extract_from_boxed flag.\n    \"\"\"\n    if relaxed:\n        return search_boxed(string) or search_regex(string, extract_regex)\n\n    if extract_from_boxed:\n        return search_boxed(string)\n    return search_regex(string, extract_regex)\n\n\ndef search_regex(string: str, regex: str):\n    match = re.findall(regex, string)\n    if match:\n        return match[-1]\n    return None\n\n\ndef search_boxed(string: str):\n    if \"\\\\boxed\" not in string:\n        return None\n\n    idx = string.rfind(\"\\\\boxed\")\n    if idx < 0:\n        idx = string.rfind(\"\\\\fbox\")\n        if idx < 0:\n            return None\n\n    i = idx\n    right_brace_idx = None\n    num_left_braces_open = 0\n    while i < len(string):\n        if string[i] == \"{\":\n            num_left_braces_open += 1\n        if string[i] == \"}\":\n            num_left_braces_open -= 1\n            if num_left_braces_open == 0:\n                right_brace_idx = i\n                break\n        i += 1\n\n    if right_brace_idx is None:\n        retval = None\n    else:\n        retval = string[idx : right_brace_idx + 1]\n\n    if retval:\n        left = \"\\\\boxed{\"\n        try:\n            assert retval[: len(left)] == left\n            assert retval[-1] == \"}\"\n            return retval[len(left) : -1]\n        except AssertionError:\n            return None\n\n    return None\n",
    "Experiment Result": "The methodology centers on synthesizing code-interpreter style solutions for GSM8K and MATH using the Mixtral 8x7B LLM through few-shot prompting. The key innovation is the \"Masked Text Solution Prompting\" strategy, where intermediate numerical computations in reference text solutions are replaced with symbols (e.g., M, N) within the prompt to prevent the LLM from generating shortcut solutions. Other prompting strategies explored include brute-force sampling (e.g., 128-224 solutions per problem) and subject-specific prompting for MATH. Generated solutions are in a code-interpreter format, interweaving natural language reasoning with Python code blocks and their execution outputs. Post-processing steps clean syntactically noisy solutions. Data selection strategies include \"Fair Downsampling\" to balance problem representation and \"Any-Code\" filtering to prioritize code-based solutions, forming a 1.02M instance fine-tuning corpus. Models (Mistral-7B, Llama 2, CodeLlama variants) are trained for 2-4 epochs using the AdamW optimizer. Experimental settings for the code execution environment include a default `SESSION_TIMEOUT` of 0 (disabled), `MEM_LIMIT_BYTES` of 50 GiB, `max_code_output_characters` of 1000, and a `code_execution_timeout` of 10.0 seconds. The `traceback_verbosity` for the sandbox can be configured (e.g., 'Plain', 'Minimal', 'Context', 'Verbose'). The `max_code_executions` for a single interaction is 8 by default."
}{
    "Title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
    "Main Contributions": "The paper introduces OpenMathInstruct-1, a large-scale (1.8M problem-solution pairs), high-quality, and commercially permissively licensed math instruction tuning dataset. It addresses the limitation of existing math datasets relying on proprietary LLMs for synthesis. By using the open-source Mixtral model and a novel 'masked text solution prompting' technique, the authors achieve high training set coverage for GSM8K (99.9%) and MATH (93%). Models fine-tuned on OpenMathInstruct-1, such as OpenMath-CodeLlama-70B, achieve competitive performance (84.6% on GSM8K and 50.7% on MATH) compared to state-of-the-art GPT-distilled models. The work also includes the public release of the dataset, code, and fine-tuned models to support open-source efforts.",
    "Methodology": "The core methodology involves synthesizing code-interpreter style solutions for math reasoning benchmarks (GSM8K and MATH) using the Mixtral-base LLM. Solutions interweave natural language reasoning with Python code blocks and their execution outputs. Few-shot prompting is used for generation, with explorations into 'Default', 'Subject-specific', and 'Masked Text Solution' prompting strategies. The 'Masked Text Solution' strategy, which replaces intermediate numerical computations in reference solutions with symbols in the prompt, significantly improves coverage by deterring 'shortcut' solutions. Generated solutions undergo post-processing to fix syntactic errors. Data selection strategies include 'Fair Downsampling' to balance problem representation and 'Code-Preferential Solutions' (specifically 'Any-Code') to prioritize code-based solutions for their precision. Models (Mistral, Llama 2, CodeLlama families) are fine-tuned on a mixture of fair-downsampled GSM8K and MATH instances.",
    "Experimental Setup": "The dataset OpenMathInstruct-1 is constructed using the Mixtral 8x7B model to synthesize solutions for the training sets of GSM8K and MATH benchmarks. Data generation involves sampling with temperature 1.0, top_p 0.95, and constraints on token limits and code block count. Fine-tuning experiments are conducted on various base models (Mistral-7B, Llama 2-7B/13B/70B, CodeLlama-7B/13B/34B/70B) for 2-4 epochs using AdamW optimizer. Evaluation is performed in a zero-shot setting on mathematical reasoning benchmarks including GSM8K, MATH, GSM-Hard, SV AMP, TabMWP, ASDiv, and MAWPS. Both greedy decoding and self-consistency (majority voting with k=50, temperature 0.7) are used for evaluation. A 1K-sized validation set was created from GSM8K and MATH training data for ablation studies. All experiments leverage the NeMo toolkit and TensorRT-LLM.",
    "Limitations": "The work has several limitations: it does not address filtering semantically noisy solutions (correct answers via flawed reasoning), which can be challenging to detect. The improved performance on in-domain benchmarks may not generalize to other related tasks, as suggested by a drop in performance on GSM-Hard. The current dataset and model generation pipeline lack examples where the model recovers from errors during problem-solving. Due to computational costs, comprehensive statistical significance analysis with multiple fine-tuning runs was not performed. The authors acknowledge potential risks if their imperfect dataset and models are used for sensitive applications like student assignment evaluation or math tutoring.",
    "Future Research Directions": "Future research directions include developing robust semantic filters to identify and remove solutions with flawed reasoning from synthetically generated datasets. Another direction is to utilize the released 6.6M incorrect sampled solutions to train verifier models, which can potentially improve solution quality. Enhancing the models' ability to recover from errors during problem-solving, perhaps by incorporating such examples in the dataset or refining the training methodology, is also an area for further exploration.",
    "Experiment Code": "File Path: nemo_skills/inference/model/code_execution.py\nContent:\nclass CodeExecutionWrapper:\n    def __init__(self, model: BaseModel, sandbox: Sandbox, config: CodeExecutionConfig):\n        self.model = model\n        self.sandbox = sandbox\n        self.config = config\n\n    async def _generate_single(\n        self,\n        prompt: str | list[dict],\n        code_begin: str,\n        code_end: str,\n        code_output_begin: str,\n        code_output_end: str,\n        code_output_format: str,\n        tokens_to_generate: int,\n        temperature: float,\n        top_p: float,\n        top_k: int,\n        min_p: float,\n        repetition_penalty: float,\n        random_seed: int,\n        stop_phrases: list[str] | None = None,\n        top_logprobs: int | None = None,\n        timeout: float | int | None = 14400,  # None is 10min\n        max_code_executions: int | None = None,  # if not None, will override self.config.max_code_executions\n        stream: bool = False,\n        extra_body: dict = None,\n        endpoint_type: EndpointType = None,\n    ):\n        # Handle OpenAI-style dictionary prompts\n        is_openai_format = not isinstance(prompt, str)\n\n        if top_logprobs is not None:  # TODO: add this\n            raise NotImplementedError(\"top_logprobs is not supported yet.\")\n\n        if stream:\n            return self._stream_single(\n                prompt=prompt,\n                endpoint_type=endpoint_type,\n                code_begin=code_begin,\n                code_end=code_end,\n                code_output_begin=code_output_begin,\n                code_output_end=code_output_end,\n                code_output_format=code_output_format,\n                tokens_to_generate=tokens_to_generate,\n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k,\n                min_p=min_p,\n                repetition_penalty=repetition_penalty,\n                random_seed=random_seed,\n                stop_phrases=stop_phrases,\n                timeout=timeout,\n                max_code_executions=max_code_executions,\n                extra_body=extra_body,\n            )\n\n        effective_max_code_executions = self.config.max_code_executions\n        if max_code_executions is not None:\n            effective_max_code_executions = max_code_executions\n\n        # making a copy of prompts to not corrupt original data\n        if is_openai_format:\n            new_prompt = copy.deepcopy(prompt)\n        else:\n            new_prompt = copy.deepcopy(prompt)\n\n        start_time = int(time.time())\n\n        stop_phrases = stop_phrases or []\n\n        request = {\n            \"endpoint_type\": endpoint_type,\n            \"prompt\": new_prompt,\n            \"tokens_to_generate\": tokens_to_generate,\n            \"temperature\": temperature,\n            \"top_k\": top_k,\n            \"top_p\": top_p,\n            \"min_p\": min_p,\n            \"random_seed\": random_seed,\n            \"repetition_penalty\": repetition_penalty,\n            \"stop_phrases\": stop_phrases + [code_end],\n            \"timeout\": timeout,\n            \"extra_body\": extra_body,\n        }\n        session_id = None\n        code_rounds_executed = 0\n        total_num_generated_tokens = 0\n        generation_time = 0\n        code_execution_time = 0\n        stopped_on_repetition = False\n        num_code_timeouts = 0\n        # adding plus one to make sure there is always some completion after the last requested code block\n        try:\n            for generation_index in range(effective_max_code_executions + 1):\n                generation_time_start = time.time()\n                if timeout is not None:\n                    # updating timeout to account for the time already spent\n                    new_timeout = int(timeout - (time.time() - start_time))\n                    request[\"timeout\"] = new_timeout\n                    if request[\"timeout\"] <= 0:\n                        break\n\n                output_dict = await self.model.generate_async(**request, remove_stop_phrases=False)\n\n                output, num_generated_tokens = output_dict[\"generation\"], output_dict.get(\"num_generated_tokens\", 0)\n                # no need to do anything with this as the code below should just exit, so that's only for logging\n                stopped_on_repetition = output_dict.get(\"stopped_on_repetition\", False)\n\n                # openai and trtllm don't show what stop word was triggered, so we assume that it was `code_end`\n                # if there's an unfinished code block\n                if output.count(code_end) + 1 == output.count(code_begin):\n                    output += code_end\n\n                # Update the prompt based on format\n                if is_openai_format:\n                    request[\"prompt\"].append({\"role\": \"assistant\", \"content\": output})\n                    request[\"prompt\"].append({\"role\": \"user\", \"content\": \"continue\"})\n                else:\n                    request[\"prompt\"] += output\n\n                # if it's the extra iteration, we don't execute the code block and just finish\n\n                if generation_index == effective_max_code_executions:\n                    break\n                # adjusting requested tokens to account for what has been generated already\n                request[\"tokens_to_generate\"] -= num_generated_tokens\n                total_num_generated_tokens += num_generated_tokens\n                generation_time += int(time.time() - generation_time_start)\n                # TODO: currently we don't account for tokens in the code output that we add to the prompt\n                #       in most cases the output should be small though\n                if request[\"tokens_to_generate\"] <= 0:\n                    break\n                # .rfind(code_end, 0, -1) searches for the second-to-last occurrence of code_end and checks\n                # that the last code_begin is not closed to ensure that we are inside the code block\n                if output.endswith(code_end) and output.rfind(code_begin) > output.rfind(code_end, 0, -1):\n                    code_execution_time_start, execution_dict, session_id = await self.execute_generated_code(\n                        prompt, code_begin, code_end, output, session_id\n                    )\n                    remaining_code_executions = None\n                    if self.config.add_remaining_code_executions:\n                        remaining_code_executions = effective_max_code_executions - generation_index - 1\n                    # adding code output to the prompt\n                    code_output = format_code_output(\n                        execution_dict,\n                        code_output_begin,\n                        code_output_end,\n                        code_output_format,\n                        remaining_code_executions,\n                    )\n\n                    if \"process_status\" in execution_dict and execution_dict[\"process_status\"] == \"timeout\":\n                        num_code_timeouts += 1\n\n                    if is_openai_format:\n                        request[\"prompt\"][-2][\"content\"] += code_output\n                    else:\n                        request[\"prompt\"] += code_output\n\n                    code_execution_time += int(time.time() - code_execution_time_start)\n                    code_rounds_executed += 1\n                else:  # if no code was generated, we need to finish\n                    break\n\n            # removing original prompt and returning the generation\n            if is_openai_format:\n                generation = \"\\n\".join(msg[\"content\"] for msg in request[\"prompt\"] if msg[\"role\"] == \"assistant\")\n            else:\n                generation = request[\"prompt\"][len(prompt) :]\n\n            return {\n                \"generation\": generation,\n                \"code_rounds_executed\": code_rounds_executed,\n                \"num_generated_tokens\": total_num_generated_tokens,\n                \"generation_time\": generation_time,\n                \"code_execution_time\": code_execution_time,\n                \"stopped_on_repetition\": stopped_on_repetition,\n                \"num_code_timeouts\": num_code_timeouts,\n            }\n        finally:\n            # Clean up session if we created one and configured to do so\n            if session_id is not None and self.config.code_execution_language == \"ipython\":\n                await self.sandbox.delete_session(str(session_id))\n\nFile Path: nemo_skills/code_execution/utils.py\nContent:\ndef format_code_output(\n    execution_dict,\n    code_output_begin: str,\n    code_output_end: str,\n    code_output_format: str = \"llama\",\n    remaining_code_executions: int | None = None,\n):\n    \"\"\"Formatting code output to be displayed as an llm expects it.\"\"\"\n    remaining_ce_string = \"\"\n    if remaining_code_executions is not None:\n        if remaining_code_executions > 0:\n            remaining_ce_string = (\n                f\"```system\\n\"\n                f\"Remaining code executions: {remaining_code_executions}. \"\n                f\"You will not be able to call code when you run out of executions, so use it wisely. \"\n                f\"Note that you can still continue solving the problem without code after that.\\n\"\n                f\"```\\n\"\n            )\n        else:\n            remaining_ce_string = (\n                \"```system\\n\"\n                \"You have run out of code executions! You can no longer write or execute code. \"\n                \"Now you should continue solving the problem by relying on your mathematical reasoning and analytical skills.\\n\"\n                \"```\\n\"\n            )\n    if code_output_format == \"llama\":\n        output = execution_dict[\"process_status\"]\n        if execution_dict[\"stdout\"]:\n            output += f\"\\n[stdout]\\n{execution_dict['stdout']}[/stdout]\"\n        if execution_dict[\"stderr\"]:\n            output += f\"\\n[stderr]\\n{execution_dict['stderr']}[/stderr]\"\n        output = f\"{code_output_begin}\\n\\n{output}{remaining_ce_string}{code_output_end}\\n\\n\"\n    elif code_output_format == \"qwen\":\n        output = \"\"\n        if execution_dict[\"stdout\"]:\n            output += f\"{execution_dict['stdout']}\"\n        if execution_dict[\"stderr\"]:\n            output += f\"{execution_dict['stderr']}\"\n        output = f\"{code_output_begin}{output}{code_output_end}{remaining_ce_string}\"\n    else:\n        raise ValueError(f\"Unknown code_output_format: {code_output_format}\")\n\n    # wrapping with code output separators\n    return output\n\ndef extract_code_to_execute(generation: str, code_begin: str, code_end: str, extract_all: bool = False):\n    return _extract_between_separators(generation, [code_begin, code_end], extract_all)\n\nFile Path: nemo_skills/prompt/utils.py\nContent:\n    def build_filled_example(self, example_dict: Dict[str, Any]) -> str:\n        \"\"\"Builds a filled example string based on the example dictionary.\"\"\"\n\n        # replacing code/code-output separators in the examples if present\n        example_dict = example_dict.copy()\n        if \"solution\" in example_dict and self.config.code_tags:\n\n            def replace_code_output(match):\n                code_output = match.group(2)\n                formatted_output = format_code_output(\n                    execution_dict={\"process_status\": \"completed\", \"stdout\": code_output, \"stderr\": \"\"},\n                    code_output_begin=self.config.code_tags.code_output_begin,\n                    code_output_end=self.config.code_tags.code_output_end,\n                    code_output_format=self.config.code_tags.code_output_format,\n                )\n                return formatted_output\n\n            pattern = r\"({code_output_begin}\\n)(.*?)({\\code_output_end})\"\n            example_dict[\"solution\"] = re.sub(pattern, replace_code_output, example_dict[\"solution\"], flags=re.DOTALL)\n\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\n                \"{code_begin}\", self.config.code_tags.code_begin\n            )\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\"{code_end}\", self.config.code_tags.code_end)\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\"{code_output_begin}\", \"\")\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\"{code_output_end}\", \"\")\n\n        return self.config.few_shot_examples.template.format(**example_dict)\n\nFile Path: recipes/openmathreasoning/scripts/postprocess_tir_generations.py\nContent:\ndef validate_code_execution(text, code_begin=\"```python\", code_end=\"```\"):\n    lines = text.split(\"\\n\")\n    i = 0\n\n    while i < len(lines):\n        if lines[i] == code_begin:\n            code_end_idx = -1\n            for j in range(i + 1, len(lines)):\n                if lines[j] == code_end:\n                    code_end_idx = j\n                    break\n\n            if code_end_idx == -1:\n                return False\n\n            if code_end_idx + 1 >= len(lines) or lines[code_end_idx + 1] != \"```output\":\n                return False\n\n            output_end_idx = -1\n            for j in range(code_end_idx + 2, len(lines)):\n                if lines[j] == \"```\":\n                    output_end_idx = j\n                    break\n\n            if output_end_idx == -1:\n                return False\n\n            i = output_end_idx + 1\n        else:\n            i += 1\n\n    return True\n\nFile Path: nemo_skills/training/data_preparation_utils/filters.py\nContent:\nclass CodeTextFilter(BaseParallelProcessor):\n    def __init__(self, filter_type, code_tags, solution_key=\"generation\", **kwargs):\n        if \"in_memory_chunksize\" not in kwargs:\n            kwargs[\"in_memory_chunksize\"] = 100000000\n        if \"chunksize\" not in kwargs:\n            kwargs[\"chunksize\"] = 100000\n        super().__init__(**kwargs)\n        self.code_tags = code_tags\n        self.text_filter_type = filter_type\n        self.solution_key = solution_key\n\n    def process_dataset_entry(self, grouped_samples: List, code_begin_token: str):\n        code_solns = []\n        text_solns = []\n        for sample in grouped_samples:\n            if code_begin_token in sample[self.solution_key]:\n                code_solns.append(sample)\n            else:\n                text_solns.append(sample)\n\n        filtered_predictions = []\n        if self.text_filter_type is None:\n            filtered_predictions.extend(code_solns)\n            filtered_predictions.extend(text_solns)\n        elif self.text_filter_type == \"all\":\n            filtered_predictions.extend(code_solns)\n        elif self.text_filter_type == \"majority_code\":\n            filtered_predictions.extend(code_solns)\n            if len(code_solns) <= len(grouped_samples) // 2:\n                filtered_predictions.extend(text_solns)\n        elif self.text_filter_type == \"majority_text\":\n            if len(code_solns) > len(grouped_samples) // 2:\n                filtered_predictions.extend(code_solns)\n            else:\n                filtered_predictions.extend(text_solns)\n        elif self.text_filter_type == \"any_code\":\n            if code_solns:\n                filtered_predictions.extend(code_solns)\n            else:\n                filtered_predictions.extend(text_solns)\n        else:\n            raise NotImplementedError(f\"Filtering method {self.text_filter_type} not implemented\")\n        num_removed = len(grouped_samples) - len(filtered_predictions)\n\n        return [DataEntry(data=filtered_predictions, metrics=dict(num_removed=num_removed))]\n\n    def process(self):\n        self.prepare()\n        os.makedirs(os.path.dirname(self.output_manifest_file), exist_ok=True)\n        metrics = []\n        code_tags_config = load_config(self.code_tags, Path(__file__).absolute().parents[2] / \"prompt\" / \"code_tags\")\n        code_begin_token = code_tags_config.code_begin\n\n        with open(self.output_manifest_file, \"wt\", encoding=\"utf-8\") as fout:\n            for manifest_chunk in self._chunk_manifest():\n                # this will unroll all inner lists\n                data = chain(\n                    *process_map(\n                        self.process_dataset_entry,\n                        manifest_chunk,\n                        code_begin_token,\n                        max_workers=self.max_workers,\n                        chunksize=self.chunksize,\n                    )\n                )\n                for data_entry in tqdm.tqdm(data):\n                    metrics.append(data_entry.metrics)\n                    if data_entry.data is None:\n                        continue\n                    json.dump(data_entry.data, fout, ensure_ascii=False)\n                    self.number_of_entries += 1\n                    fout.write(\"\\n\")\n\n        self.finalize(metrics)\n\nFile Path: nemo_skills/prompt/few_shot_examples/gsm8k.py\nContent:\ngsm8k_text_with_code = [\n    {\n        \"problem\": \"Missy had a giant piggy bank in her bedroom.  Every day she would search the house for change to put in her bank.  After 4 years, the bank was opened and it contained $450 in change.  If the second, third, and fourth-year she doubled the amount of money she put in the bank from the amount she had put in the previous year, how much money, in dollars, did she put in the bank the first year?\",\n        \"solution\": \"\"\"\nMissy started with some amount of money in the first year.\nShe was then doubling the amount of money every year for 4 years and ended up with $450.\nLet's write down an equation for this problem and solve it using sympy.\n{code_begin}from sympy import solve, symbols\nfirst_year_money = symbols('first_year_money')\nsecond_year_money = 2 * first_year_money\nthird_year_money = 2 * second_year_money\nfourth_year_money = 2 * third_year_money\n# Solve for first year money given that the combined saving is 450\nresult = solve(first_year_money + second_year_money + third_year_money + fourth_year_money - 450, first_year_money)[0]\nresult\n{code_end}{code_output_begin}\n30\n{code_output_end}\nThus Missy put \\boxed{30} dollars in the bank the first year.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Pete has to take a 10-minute walk down to the train station and then board a 1hr 20-minute train to LA. When should he leave if he cannot get to LA later than 0900 hours? (24-hr time)\",\n        \"solution\": \"\"\"\nSince Pete needs to take a 10 minutes walk and then a 1 hour 20 minutes train ride, he will spend a total of 1 hour and 30 minutes.\nThis means that he needs to leave 1 hour and 30 minutes earlier than 09:00 hours.\nSubtracting 1 hour and 30 minutes from 09:00 hours we get \\boxed{07:30} hours.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Mark deposited $88 in a bank. Bryan deposited $40 less than five times as much as Mark. How much did Bryan deposit in the bank?\",\n        \"solution\": \"\"\"\nLet's solve this problem using Python code.\n{code_begin}mark_deposit = 88\nfive_times_mark_deposit = 5 * mark_deposit\nbryan_deposit = five_times_mark_deposit - 40\nbryan_deposit\n{code_end}{code_output_begin}\n400\n{code_output_end}\nSo Bryan deposited \\boxed{400} dollars in the bank.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A department store displays a 20% discount on all fixtures. What will be the new price of a 25 cm high bedside lamp that was worth $120?\",\n        \"solution\": \"\"\"\nLet's solve this problem using Python code.\n{code_begin}discount_percent = 20\nprice_before_discount = 120\ndiscount_portion = discount_percent / 100\ndiscount_amount = price_before_discount * discount_portion\nprice_after_discount = price_before_discount - discount_amount\nprice_after_discount\n{code_end}{code_output_begin}\n96.0\n{code_output_end}\nSo the new price of the lamp is \\boxed{96} dollars.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"James opens up a flower shop.  He needs to pay rent of $1200 a week with an additional 20% of rent to pay for utilities and he has 2 employees per shift with the store open 16 hours a day for 5 days a week.  If he pays each employee $12.50 an hour, what are his weekly expenses to run the store?\",\n        \"solution\": \"\"\"\nThe cost consists of rent, utilities, and employee salaries. Let's compute each of them separately and then add them up.\n{code_begin}# rent cost\nrent_per_week = 1200\n# utility cost\nutility_per_week = rent_per_week * 20 / 100\n# employee cost\nemployee_work_hours = 16\nwork_days_per_week = 5\nemployee_work_hours_per_week = work_days_per_week * employee_work_hours\nnumber_of_employees = 2\nemployee_cost_per_hour = 12.5\nemployees_cost_per_week = number_of_employees * employee_work_hours_per_week * employee_cost_per_hour\n# add the three to get total cost\ncost_per_week = rent_per_week + utility_per_week + employees_cost_per_week\ncost_per_week\n{code_end}{code_output_begin}\n3440.0\n{code_output_end}\nThus James's weekly expenses add up to \\boxed{3440} dollars.\n\"\"\".strip(),\n    },\n]\n",
    "Experiment Result": "The core methodology involves synthesizing code-interpreter style solutions for math reasoning benchmarks GSM8K and MATH. Solutions interweave natural language reasoning with Python code blocks and their execution outputs.\n\n**Few-shot Prompting:**\n- Prompts include diverse few-shot examples (e.g., `gsm8k_text_with_code` from `nemo_skills/prompt/few_shot_examples/gsm8k.py`).\n- Few-shot strategies explored: 'Default', 'Subject-specific', and 'Masked Text Solution'.\n- The 'Masked Text Solution' strategy, which replaces intermediate numerical computations in reference solutions with symbols in the prompt, significantly improves coverage by deterring 'shortcut' solutions. This can be implemented via `parallel_thinking.mode=genselect` in `nemo_skills/inference/model/parallel_thinking.py`.\n\n**Post-processing:**\n- Generated solutions undergo post-processing to fix syntactic errors. The `validate_code_execution` function in `recipes/openmathreasoning/scripts/postprocess_tir_generations.py` exemplifies this, ensuring code blocks and outputs are well-formed.\n\n**Data Selection Strategies:**\n- **'Fair Downsampling'**: Achieved via `ShuffleAndDownsampleData` in `nemo_skills/training/data_preparation_utils/preprocessing.py`, which balances problem representation across different solution types.\n- **'Code-Preferential Solutions'**: Specifically 'Any-Code', where code-based solutions are prioritized for their precision. This is managed by `CodeTextFilter` in `nemo_skills/training/data_preparation_utils/filters.py`.\n- **Filtering contaminated samples**: `RemoveContaminated` in `nemo_skills/training/data_preparation_utils/filters.py` is used to remove samples that are too similar to existing test sets.\n\n**Model Fine-tuning:**\n- Models from the Mistral, Llama 2, and CodeLlama families are fine-tuned on a mixture of fair-downsampled GSM8K and MATH instances. The `nemo_skills/training/nemo_rl/start_sft.py` script and its `setup_data` function outline how such datasets are prepared for training."
}{
    "Title": "Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads",
    "Main Contributions": "This paper systematically evaluates state-of-the-art Large Vision-and-Language Models (LVLMs) on mathematical and algorithmic reasoning using visuo-linguistic problems from children's Mathematical Kangaroo (MK) Olympiads. The authors introduce SMART-840, a dataset of 840 problems from MK Olympiads (2020-2024, grades 1-12), enabling a direct comparison between AI capabilities and children's performance. Key findings include that LVLMs exhibit increasingly powerful reasoning for higher grades but struggle with foundational problems designed for younger children. There is no significant correlation between AI models' reasoning capabilities and young children's, suggesting a different reasoning mechanism. LVLM performance is significantly weaker than children's overall, with a larger gap for younger grades. LVLMs perform poorly on geometry and logic problems but better on text-only problems, sometimes outperforming humans, and show significant variability in responses. Requesting explanations from LVLMs drastically improves their performance.",
    "Methodology": "The study evaluates LVLMs using problems from the Mathematical Kangaroo Olympiad, which offers age-appropriate problems for grades 1-12 in a multiple-choice format. The SMART-840 dataset was created by collating 840 problems from 2020-2024 MK exams, extracting text via OCR, manually cropping images, and inspecting for errors. Problems are categorized by domain (geometry, logic, algebra, numbers) and type (image-text, text-only). Closed-source LVLMs (GPT-4o, Gemini-Pro, Claude-3 Opus/Sonnet, GPT-4v, Gemini-Flash) and open-source models (XGEN-MM-Phi3-Instruct-v1, LlaVa-Next, InternVL-Chat-V1-2, InternLM-XComposer-2.5) were benchmarked. A standardized prompt, requiring step-by-step explanations and a specific answer format, was used, alongside a 'no-explanation' prompt for comparison. Performance was measured by accuracy (percentage of correct answers). Various correlation analyses (Difficulty Index, Discriminative Index, Weight Correlation, Entropy Correlation, Time-taken Correlation) were performed to compare AI responses against children's problem-solving characteristics.",
    "Experimental Setup": "The experimental setup involved the SMART-840 dataset, consisting of 840 mathematical and algorithmic reasoning problems from the Mathematical Kangaroo Olympiads spanning grades 1-12, collected from the years 2020-2024. The problems included both text-only and image-text formats. Human performance data, including answer responses and time taken, was collected from approximately 30,000 student participants per year in the MK competitions. A random baseline (20% accuracy) was established. LVLMs were queried via their API interfaces, with each problem run at least twice (GPT-4o 5 times, Gemini 2 times) to assess response variance. GPT-4o responses were also manually validated to ensure reasoning soundness. Performance on different problem categories (geometry, logic, algebra, numbers) and types (image-text vs. text-only) was analyzed separately.",
    "Limitations": "The SMART-840 dataset is smaller in size compared to other recent AI mathematical reasoning benchmarks, a necessity imposed by the goal of evaluating zero-shot performance against human cognition. The study focuses on zero-shot performance, meaning LVLMs are not fine-tuned on the dataset, which could limit comparisons to models that are. The exact contents of the web-scale training corpora used by state-of-the-art LVLMs are unknown, making it challenging to precisely attribute observed performance differences. The number of runs for LVLM evaluation was constrained by the associated costs.",
    "Future Research Directions": "The paper's findings implicitly suggest future research directions aimed at bridging the observed gap between AI and human reasoning. These include developing LVLMs that exhibit more foundational understanding for simpler, younger-grade problems, akin to the cumulative knowledge building in human children. Further research could explore how to integrate human-like cognitive development principles into AI training to enable more robust and generalizable problem-solving. Addressing the significant weaknesses of LVLMs in geometry and logic, and improving their performance on image-text problems compared to text-only ones, are also critical areas. Efforts to reduce the variability and improve the reliability of LVLM responses, especially when repeating or changing prompts, are warranted. Finally, future work could focus on developing AI models whose reasoning processes align more closely with human cognition, moving beyond reliance on mere similarity to vast training examples.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
    "Main Contributions": "The paper addresses the critical and labor-intensive problem of learning rate (LR) schedule tuning in training Deep Neural Networks (DNNs). It proposes AutoLRS, an efficient method that automatically optimizes the LR for each training stage by modeling training dynamics. Key contributions include: (1) an automatic LR scheduling system that does not require human involvement; (2) achieving significant training speedups (1.22x for ResNet-50, 1.43x for Transformer, and 1.5x for BERT) compared to original hand-tuned schedules, and an average 1.31x speedup over state-of-the-art heavily-tuned LR schedules, while maintaining or improving test-set performance; (3) demonstrating generality and compatibility with diverse DNNs and optimizers.",
    "Methodology": "AutoLRS breaks down LR schedule optimization into dynamically optimizing a constant LR for every τ steps (a 'training stage'). It treats the validation loss at the end of a stage as a black-box function of LR and optimizes it using Bayesian Optimization (BO). To overcome the high cost of evaluating each LR candidate for τ steps, AutoLRS employs a light-weight exponential time-series forecasting model (L(t) = aexp(bt) + c). This model predicts the validation loss after τ steps based on observations from only τ′≪τ steps of training with a candidate LR. This creates a mutual-training process between BO and the loss-prediction model. The BO uses Gaussian Process (GP) with a Matern kernel as its surrogate model and Lower Confidence Bound (LCB) as the acquisition function. Practical improvements include gradually increasing τ over training stages and minimizing training loss in early stages, then using a small subset of the validation set in later stages. An iterative spline smoothing pre-processing step is applied to the loss series to filter noise and outliers before exponential fitting.",
    "Experimental Setup": "AutoLRS was evaluated on three widely-used DNNs: ResNet-50 for ImageNet classification (using SGD with momentum), Transformer for neural machine translation on WMT 2014 English-German dataset (using Adam), and BERTBASE for pre-training on English Wikipedia and BooksCorpus with subsequent fine-tuning on MRPC, MNLI, CoLA, and SQuAD v1.1 (using Adam). Additional experiments were conducted on VGG-16 and ResNet-50 with CIFAR-10/CIFAR-100 datasets for ablation studies and comparisons against prior methods. Hardware included 32 NVIDIA Tesla V100 GPUs (ResNet-50, BERT), 8 NVIDIA Tesla V100 GPUs (Transformer), and one NVIDIA Titan RTX GPU (VGG-16, ResNet-50 for prior method comparisons). Baselines included original hand-tuned LR schedules, carefully hand-tuned Cyclical Learning Rate (CLR), Stochastic Gradient Descent with Warm Restarts (SGDR), Hypergradient Descent (HD), MARTHE, and Hyperband. AutoLRS default hyperparameters were k=10, τ′=τ/10 (τ starting at 1000, τ′ at 100, doubling until τmax=8000 for ResNet-50/Transformer, 32000 for BERT), and κ=1000 for LCB. LR search intervals were specified per task.",
    "Limitations": "AutoLRS still requires setting some hyperparameters, notably the learning rate search interval (ηmin, ηmax), although it can be reasonably wide and determined by a simple LR range test. While default settings for k, τ′, τ, and κ work well across diverse models, further fine-tuning could yield additional improvements. The current AutoLRS method does not search for learning rates during warmup steps, as warmup phases lack an explicit optimization objective for validation loss minimization. The exponential forecasting model can sometimes fail to fully capture highly non-monotone loss changes during the very early steps of a stage, though iterative spline smoothing is used to mitigate this issue.",
    "Future Research Directions": "Implicit future research directions include: (1) further optimizing the hyperparameters of AutoLRS (k, τ′, τ, κ) to achieve even greater performance gains; (2) extending the framework to automatically determine learning rates during warmup phases of training by incorporating an explicit optimization objective for such stages; (3) potentially exploring more sophisticated time-series forecasting models to better capture complex and non-monotone loss dynamics, particularly in the early, unstable phases of training, although the current exponential model with spline smoothing performs effectively.",
    "Experiment Code": "import torch\nimport os\nimport logging\nimport socket\nimport string\nimport random\nimport time\n\nclass AutoLRS():\n    def __init__(self, model, optimizer, val_fn, listening_host='localhost', listening_port=12315, warmup_steps=0, warmup_lr=0, summary_steps=1):\n        self._net = model\n        self._optimizer = optimizer\n        self._val_fn = val_fn \n        self._lr = 0.000001\n        self._warmup_steps = warmup_steps\n        self._warmup_lr = warmup_lr \n        self._global_step = 0\n        self._socket = socket.socket()\n        self._started = False\n        self._summary_steps = summary_steps\n        self._checkpoint_path = './checkpoint/autolrs_ckpt_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k = 7))  + '.pth'\n        self._listening_host = listening_host\n        self._listening_port = listening_port\n        self._best_acc = 0\n\n        if not os.path.exists('checkpoint'):\n            os.makedirs('checkpoint')\n\n        self.connect_server()\n    \n    def connect_server(self):\n        self._socket.connect((self._listening_host, self._listening_port))\n\n    def _verbose_operation(self, _op):\n        if self._global_step % self._summary_steps == 0:\n            logging.info(\"[AutoLRS at {}] {}\".format(self._global_step, _op))\n\n    def save_variables(self):\n        \"\"\"Save model parameters and optimizer states.\"\"\"\n        _start_time = time.time()\n        torch.save({\n            'model_state_dict': self._net.state_dict(),\n            'optimizer_state_dict': self._optimizer.state_dict()\n            }, self._checkpoint_path)\n        logging.info(\"[AutoLRS] backup variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def restore_variables(self):\n        _start_time = time.time()\n        checkpoint = torch.load(self._checkpoint_path)\n        self._net.load_state_dict(checkpoint['model_state_dict'])\n        self._optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        logging.info(\"[AutoLRS] restoring variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def on_train_batch_end(self, loss):\n        if self._global_step < self._warmup_steps:\n            # linear warmup\n            self._lr = (self._warmup_lr / self._warmup_steps) * (self._global_step + 1)\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n            self._global_step += 1\n\n        elif not self._started:\n            self.save_variables()\n            print(\"backup trainable variables to CPU\") \n            self._started = True\n            self._socket.send(\",\".join(('startBO', str(loss))).encode(\"utf-8\"))\n            self._verbose_operation(\"Start Bayesian Optimization(BO)\")\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            self._lr = (float(data.split(\",\")[-1]))\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n        else:\n            self._socket.send(','.join(('loss', str(loss))).encode('utf-8'))\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            if data.startswith(\"restore\"):\n                self.restore_variables()\n                self._verbose_operation(\"restore trainable variables\")\n            elif data.startswith(\"ckpt\"):\n                self.save_variables()\n                self._verbose_operation(\"backup trainable variables\")\n            elif data.startswith('evaluate'):\n                val_loss = self._val_fn()\n                self._socket.send(\",\".join((\"val_loss\", str(val_loss))).encode(\"utf-8\"))\n                data = self._socket.recv(1024).decode(\"utf-8\")\n            elif data.startswith('save'):\n                pass\n            else:\n                self._lr = (float(data.split(\",\")[-1]))\n                for param_group in self._optimizer.param_groups:\n                    param_group['lr'] = self._lr\n                self._global_step += 1\n",
    "Experiment Result": "AutoLRS Callback Settings:\n- Listening Host: 'localhost'\n- Listening Port: 12315\n- Warmup Steps: 0\n- Warmup LR: 0\n- Summary Steps: 1\n\nAutoLRS Server Settings:\n- Exploitation Step (initial τ for exploitation phase): 1000\n- LR Steps (τ for exploration phase): 100\n- Ring Buffer Length: 100\n- LR to Explore (number of LR candidates per BO stage): 10\n- Tau Max (maximum τ for exploitation phase): 8000\n- Validation Frequency (for validation stage): 16 (val_freq = lr_steps / 16)\n- Spline Degree for iterative spline smoothing: 2\n- Filter Ratio for outlier removal in spline smoothing: 0.03\n- Number of iterations for spline smoothing (training loss): 10\n- Number of iterations for spline smoothing (validation loss): 1\n- Bound for outlier removal in spline smoothing: 0.5 (xs[int((len(xs) - 1) * bound)])\n- Bayesian Optimization Surrogate Model: Gaussian Process (GP)\n- Bayesian Optimization Acquisition Function: Lower Confidence Bound (LCB)\n- LCB Kappa parameter: 1e6\n- Initial Learning Rate range for search: --min_lr to --max_lr (command line arguments)\n\nCIFAR10 Training Configuration:\n- Device: 'cuda' if available, else 'cpu'\n- Validation Subset Length (VAL_LEN): 10 mini-batches\n- Initial optimizer learning rate: 0.1\n- Optimizer momentum: 0.9\n- Optimizer weight decay: 5e-4\n- Model architecture: VGG16 (default, other models commented out)\n- Training epochs: 350 * 2 (total epochs doubled to account for AutoLRS search steps)\n"
}{
    "Title": "MoMo: Momentum Models for Adaptive Learning Rates",
    "Main Contributions": "The paper addresses the high computational cost of learning-rate tuning in modern machine learning. It develops MoMo (Momentum Model) and MoMo-Adam, new Polyak-type adaptive learning rates that can be used with any momentum method. The key contributions include a new interpretation of momentum as a model of the objective function, the incorporation of known or online-estimated lower bounds of the loss via truncation to derive adaptive step sizes, and demonstrating improved robustness to hyperparameter tuning across various machine learning tasks (image classification, recommender systems, transformers, diffusion models). MoMo also achieves an O(1/√K) convergence rate for convex problems with interpolation, without requiring knowledge of problem-specific quantities other than the optimal value, and can automatically perform learning rate warm-up and decay.",
    "Methodology": "MoMo is derived by building a model directly for the overall loss function f(x) (not just f(x, sk) for a single sample) using momentum estimates of past losses and gradients. This model, mk(x), is constructed by averaging sampled loss function linearizations (f(xj, sj) + <∇f(xj, sj), x - xj>) and is truncated by a known or estimated lower bound (typically zero or an online estimate fk*). The next step xk+1 is computed by approximately minimizing mk(x) + (1/(2αk))||x - xk||^2. Exponentially weighted averages are used for the coefficients. MoMo-Adam extends this by incorporating Adam's diagonal preconditioner and weight decay. An online estimation method for the lower bound fk* is developed for cases where f* is not close to zero, utilizing available information from momentum-based methods.",
    "Experimental Setup": "The methods (MoMo and MoMo-Adam) were extensively evaluated against baselines (SGD-M and Adam/AdamW) on various tasks and models: image classification (ResNet110/CIFAR100, ResNet20/VGG16/ViT/CIFAR10, MLP/MNIST, ResNet18/Imagenet32, ViT/Imagenet-1k), recommender systems (DLRM/Criteo), neural machine translation (Encoder-Decoder Transformer/IWSLT14), and diffusion models (UNet/Smithsonian Butterflies). Experiments involved learning-rate sweeps with both constant learning rates and schedules (linear warm-up followed by cosine decay or inverse square-root decay). Comparisons were made based on final training loss and validation accuracy/score (e.g., BLEU score for translation). Different lower bound settings were tested: fixed at zero (fk*=0), an oracle value (e.g., fk*=0.9), and the proposed online estimation (MoMo* and MoMo-Adam*). Results were averaged over multiple seeds (3-5) and typically run on NVIDIA A100 GPUs.",
    "Limitations": "The theoretical convergence rate (O(1/√K)) is proven for convex problems with interpolation. While momentum empirically helps in most cases, the current theoretical analysis does not show a theoretical improvement due to momentum. For tasks where the optimal loss (f*) is significantly above zero (e.g., transformers), the MoMo methods with fk*=0 (default assumption of a zero lower bound) show little or no improvement, which necessitates the use of the online lower bound estimation.",
    "Future Research Directions": "One implied future research direction is to provide a stronger theoretical understanding of why momentum improves performance empirically in the context of MoMo, as current analysis does not show a theoretical improvement. The paper also suggests extending MoMo to other preconditioned momentum methods beyond SGD-M and Adam (e.g., Adabelief, Adabound). Furthermore, the developed online lower bound estimation technique is noted to be applicable to any stochastic momentum-based method, suggesting potential for broader exploration and application of this technique in other optimization contexts.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Prodigy: An Expeditiously Adaptive Parameter-Free Learner",
    "Main Contributions": "The paper introduces Prodigy, an adaptive parameter-free optimization algorithm designed to estimate the distance to the solution (D) for optimal learning rate selection in methods like AdaGrad and Adam. Prodigy is a modification of D-Adaptation, improving its worst-case non-asymptotic convergence rate by a factor of O(sqrt(log(D/d0))). The algorithm consistently outperforms D-Adaptation across various benchmarks and achieves test accuracy values comparable to hand-tuned Adam. The authors also establish new lower complexity bounds for exponentially bounded adaptive learning rate methods.",
    "Methodology": "Prodigy improves upon D-Adaptation by modifying the error term handling, leading to provably larger step sizes. It uses an adaptive step size of the form \\u03b7_k = d_k^2 \\u03bb_k / \\sqrt{G^2d_k^2 + \\sum_{i=0}^k \\u03bb_i^2 \\|g_i\\|^2} for Gradient Descent and a similar form for Dual Averaging, where d_k is the estimated distance to the solution and \\u03bb_k is a chosen weight. The algorithm incorporates an extra term G^2 in the denominator to account for G-Lipschitz continuous objectives. For Adam-like methods, Prodigy employs exponential moving averages for gradients and squared gradients, and a coordinate-wise adaptation using the \\u21131 norm for the weighted gradient sum in the D estimate.",
    "Experimental Setup": "Prodigy was evaluated on a diverse range of problems: 12 convex logistic regression benchmark datasets (multi-margin loss, full-batch optimization), deep learning tasks including VGG11 and ResNet-50 training on CIFAR10 (batch size 256), a 6-layer nanoGPT transformer on the Shakespeare dataset (batch size 256, gradient clipping, float16), and large-scale Adam experiments. The large-scale tests included LSTM on IWSLT14, RoBERTa and GPT on BookWiki, DLRM on Criteo, VarNet on Knee MRI, and ViT on Imagenet (ILSVRC 2012). Comparisons were made against D-Adaptation, hand-tuned Adam (and AdamW), and DoG/L-DoG methods. Experiments were run with multiple random seeds (10 for logistic regression, 8 for others), using metrics like accuracy, test/train loss, perplexity, and SSIM.",
    "Limitations": "The theoretical analysis shows that the Gradient Descent variant of Prodigy has a \\u03bbo(log(n+1)) factor in its convergence rate, which is not present in the Dual Averaging variant. However, a convergence result for the Dual Averaging version with custom weights (other than \\u03bb_k = d_k^2) is not provided due to the requirement for monotonically decreasing step sizes. The established lower complexity bounds are only valid for small 'n' relative to D/d0, leaving a more general lower bound for larger 'n' as an open problem. Empirically, Prodigy slightly overfitted in test loss on LSTM training, and while it significantly closed the gap, it still underperformed hand-tuned Adam on ViT training on Imagenet, a problem noted for high run-to-run variance.",
    "Future Research Directions": "A key future research direction involves establishing a more general lower complexity bound for exponentially bounded algorithms, particularly for larger 'n'. While not explicitly stated as a future direction for Prodigy itself, the paper's discussion around the theoretical-practical gap between Gradient Descent and Dual Averaging performance, and the introduction of a coordinate-wise Prodigy version, suggest potential areas for further exploration and refinement of the algorithm and its theoretical guarantees.",
    "Experiment Code": "import math from typing import TYPE_CHECKING, Any, Callable, Optional import torch import torch.optim import logging import os import torch.distributed as dist if TYPE_CHECKING: from torch.optim.optimizer import _params_t else: _params_t = Any class Prodigy(torch.optim.Optimizer): r\"\"\" Implements Adam with Prodigy step-sizes. Leave LR set to 1 unless you encounter instability. Arguments: params (iterable): Iterable of parameters to optimize or dicts defining parameter groups. lr (float): Learning rate adjustment parameter. Increases or decreases the Prodigy learning rate. betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) beta3 (float): coefficients for computing the Prodidy stepsize using running averages. If set to None, uses the value of square root of beta2 (default: None). eps (float): Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-8). weight_decay (float): Weight decay, i.e. a L2 penalty (default: 0). decouple (boolean): Use AdamW style decoupled weight decay use_bias_correction (boolean): Turn on Adam's bias correction. Off by default. safeguard_warmup (boolean): Remove lr from the denominator of D estimate to avoid issues during warm-up stage. Off by default. d0 (float): Initial D estimate for D-adaptation (default 1e-6). Rarely needs changing. d_coef (float): Coefficient in the expression for the estimate of d (default 1.0). Values such as 0.5 and 2.0 typically work as well. Changing this parameter is the preferred way to tune the method. growth_rate (float): prevent the D estimate from growing faster than this multiplicative rate. Default is inf, for unrestricted. Values like 1.02 give a kind of learning rate warmup effect. fsdp_in_use (bool): If you're using sharded parameters, this should be set to True. The optimizer will attempt to auto-detect this, but if you're using an implementation other than PyTorch's builtin version, the auto-detection won't work. slice_p (int): Reduce memory usage by calculating LR adaptation statistics on only every pth entry of each tensor. For values greater than 1 this is an approximation to standard Prodigy. Values ~11 are reasonable (default 1). \"\"\" def __init__(self, params, lr=1.0, betas=(0.9, 0.999), beta3=None, eps=1e-8, weight_decay=0, decouple=True, use_bias_correction=False, safeguard_warmup=False, d0=1e-6, d_coef=1.0, growth_rate=float('inf'), fsdp_in_use=False, slice_p=1): if not 0.0 < d0: raise ValueError(\"Invalid d0 value: {}\".format(d0)) if not 0.0 < lr: raise ValueError(\"Invalid learning rate: {}\".format(lr)) if not 0.0 < eps: raise ValueError(\"Invalid epsilon value: {}\".format(eps)) if not 0.0 <= betas[0] < 1.0: raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0])) if not 0.0 <= betas[1] < 1.0: raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1])) if decouple and weight_decay > 0: print(f\"Using decoupled weight decay\") defaults = dict(lr=lr, betas=betas, beta3=beta3, eps=eps, weight_decay=weight_decay, d=d0, d0=d0, d_max=d0, d_numerator=0.0, d_coef=d_coef, k=0, growth_rate=growth_rate, use_bias_correction=use_bias_correction, decouple=decouple, safeguard_warmup=safeguard_warmup, fsdp_in_use=fsdp_in_use, slice_p=slice_p) self.d0 = d0 super().__init__(params, defaults) @property def supports_memory_efficient_fp16(self): return False @property def supports_flat_params(self): return True def step(self, closure=None): \"\"\"Performs a single optimization step. Arguments: closure (callable, optional): A closure that reevaluates the model and returns the loss. \"\"\" loss = None if closure is not None: loss = closure() d_denom = 0.0 group = self.param_groups[0] use_bias_correction = group['use_bias_correction'] beta1, beta2 = group['betas'] beta3 = group['beta3'] if beta3 is None: beta3 = math.sqrt(beta2) k = group['k'] d = group['d'] d_max = group['d_max'] d_coef = group['d_coef'] lr = max(group['lr'] for group in self.param_groups) if use_bias_correction: bias_correction = ((1 - beta2**(k+1))**0.5) / (1 - beta1**(k+1)) else: bias_correction = 1 dlr = d*lr*bias_correction growth_rate = group['growth_rate'] decouple = group['decouple'] fsdp_in_use = group['fsdp_in_use'] d_numerator = group['d_numerator'] d_numerator *= beta3 delta_numerator = 0.0 for group in self.param_groups: decay = group['weight_decay'] k = group['k'] eps = group['eps'] group_lr = group['lr'] d0 = group['d0'] safeguard_warmup = group['safeguard_warmup'] slice_p = group['slice_p'] if group_lr not in [lr, 0.0]: raise RuntimeError(f\"Setting different lr values in different parameter groups is only supported for values of 0\") for p in group['params']: if p.grad is None: continue if hasattr(p, \"_fsdp_flattened\"): fsdp_in_use = True grad = p.grad.data # Apply weight decay (coupled variant) if decay != 0 and not decouple: grad.add_(p.data, alpha=decay) state = self.state[p] # State initialization if 'step' not in state: state['step'] = 0 state['s'] = torch.zeros_like(p.data.flatten()[::slice_p]).detach() if p.any(): state['p0'] = p.flatten()[::slice_p].detach().clone() else: # All values are zero, so save VRAM with a zero-tensor state['p0'] = torch.tensor(0, device=p.device, dtype=p.dtype) # Exponential moving average of gradient values if beta1 > 0: state['exp_avg'] = torch.zeros_like(p.data).detach() # Exponential moving average of squared gradient values state['exp_avg_sq'] = torch.zeros_like(p.data).detach() exp_avg_sq = state['exp_avg_sq'] s = state['s'] p0 = state['p0'] if group_lr > 0.0: # we use d / d0 instead of just d to avoid getting values that are too small sliced_grad = grad.flatten()[::slice_p] delta_numerator += (d / d0) * dlr * torch.dot(sliced_grad, p0.data - p.data.flatten()[::slice_p]).item() # Adam EMA updates if beta1 > 0: exp_avg = state['exp_avg'] exp_avg.mul_(beta1).add_(grad, alpha=d * (1-beta1)) exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=d * d * (1-beta2)) if safeguard_warmup: s.mul_(beta3).add_(sliced_grad, alpha=((d / d0) * d)) else: s.mul_(beta3).add_(sliced_grad, alpha=((d / d0) * dlr)) d_denom += s.abs().sum().item() ###### d_hat = d # if we have not done any progres, return # if we have any gradients available, will have d_denom > 0 (unless \\|g\\|=0) if d_denom == 0 and not fsdp_in_use: return loss if lr > 0.0: if fsdp_in_use: dist_tensor = torch.zeros(2).cuda() dist_tensor[0] = delta_numerator dist_tensor[1] = d_denom dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM) global_d_numerator = d_numerator + dist_tensor[0] global_d_denom = dist_tensor[1] else: global_d_numerator = d_numerator + delta_numerator global_d_denom = d_denom d_hat = d_coef * global_d_numerator / global_d_denom if d == group['d0']: d = max(d, d_hat) d_max = max(d_max, d_hat) d = min(d_max, d * growth_rate) for group in self.param_groups: group['d_numerator'] = global_d_numerator group['d_denom'] = global_d_denom group['d'] = d group['d_max'] = d_max group['d_hat'] = d_hat decay = group['weight_decay'] k = group['k'] eps = group['eps'] for p in group['params']: if p.grad is None: continue grad = p.grad.data state = self.state[p] exp_avg_sq = state['exp_avg_sq'] state['step'] += 1 denom = exp_avg_sq.sqrt().add_(d * eps) # Apply weight decay (decoupled variant) if decay != 0 and decouple: p.data.add_(p.data, alpha=-decay * dlr) ### Take step if beta1 > 0: exp_avg = state['exp_avg'] p.data.addcdiv_(exp_avg, denom, value=-dlr) else: p.data.addcdiv_(grad, denom, value=-dlr * d) group['k'] = k + 1 return loss",
    "Experiment Result": "Implements Adam with Prodigy step-sizes. Leave LR set to 1 unless you encounter instability. Arguments: params (iterable): Iterable of parameters to optimize or dicts defining parameter groups. lr (float): Learning rate adjustment parameter. Increases or decreases the Prodigy learning rate. betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) beta3 (float): coefficients for computing the Prodidy stepsize using running averages. If set to None, uses the value of square root of beta2 (default: None). eps (float): Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-8). weight_decay (float): Weight decay, i.e. a L2 penalty (default: 0). decouple (boolean): Use AdamW style decoupled weight decay use_bias_correction (boolean): Turn on Adam's bias correction. Off by default. safeguard_warmup (boolean): Remove lr from the denominator of D estimate to avoid issues during warm-up stage. Off by default. d0 (float): Initial D estimate for D-adaptation (default 1e-6). Rarely needs changing. d_coef (float): Coefficient in the expression for the estimate of d (default 1.0). Values such as 0.5 and 2.0 typically work as well. Changing this parameter is the preferred way to tune the method. growth_rate (float): prevent the D estimate from growing faster than this multiplicative rate. Default is inf, for unrestricted. Values like 1.02 give a kind of learning rate warmup effect. fsdp_in_use (bool): If you're using sharded parameters, this should be set to True. The optimizer will attempt to auto-detect this, but if you're using an implementation other than PyTorch's builtin version, the auto-detection won't work. slice_p (int): Reduce memory usage by calculating LR adaptation statistics on only every pth entry of each tensor. For values greater than 1 this is an approximation to standard Prodigy. Values ~11 are reasonable (default 1)."
}{
    "Title": "Multirate Training of Neural Networks",
    "Main Contributions": "The paper proposes multirate training of neural networks, partitioning parameters into fast and slow parts updated on different time scales. It demonstrates substantial computational speed-up for transfer learning tasks in vision and NLP, fine-tuning deep neural networks in almost half the time without reducing generalization performance. A novel multirate scheme incorporating linear drift for slow parameters is introduced, with analysis of its convergence properties. The research also shows that multirate training can provide a form of regularization when training neural networks from scratch, suggesting it can enhance generalization performance by dynamically selecting slow parameter subsets. The work highlights the versatility of this approach with various partitioning strategies and lays a foundation for future exploration of multirate techniques in neural network training.",
    "Methodology": "The core methodology involves partitioning neural network parameters (Θ) into fast (ΘF) and slow (ΘS) components. Fast parameters are updated at every step with a small stepsize (hF), while slow parameters are updated less frequently (every 'k' steps) with a larger stepsize (hS = k * hF). The proposed novel scheme (Algorithm 1) incorporates a 'linear drift' for slow parameters, meaning they are continuously pushed along a path defined by their momentum even when their gradients are not computed. Stochastic Gradient Descent (SGD) with momentum serves as the base optimizer for numerical experiments and convergence analysis. Parameter partitioning strategies include layer-wise splitting (e.g., final fully connected layers as fast, rest as slow for transfer learning), random subgroups of weights/biases for regularization, and biases vs. weights. The convergence properties of the multirate SGD scheme are analyzed using standard assumptions of L-smoothness and bounded stochastic gradient variance.",
    "Experimental Setup": "For transfer learning, pre-trained ResNet-34 and ResNet-50 architectures (on ImageNet) were fine-tuned on CIFAR-10 and CIFAR-100 datasets, comparing the multirate approach (fast: final fc layer; slow: rest, with k=5) against standard fine-tuning methods. For NLP, a pre-trained DistilBERT model was fine-tuned on SST-2 and other GLUE benchmark tasks, using similar multirate partitioning (fast: linear head/fc layer + final attention block, with k=5). Experiments utilized SGD with momentum (µ=0.9), specific learning rates (e.g., h/k=0.001 for ResNet, 1e-4 for DistilBERT), and sometimes linear learning rate decay or weight decay. For regularization, a WideResNet-16 was trained from scratch on patch-augmented CIFAR-10, a single hidden layer perceptron on MNIST, and a small transformer on Penn Treebank, comparing multirate regularization (random subgroups, k=5) with standard SGD and Dropout. Ablation studies investigated the impact of linear drift, coupled vs. uncoupled learning rates, and the value of 'k'. Additional experiments explored putting biases on a slow timescale for a SHLP on spiral data and ResNet-34 on CIFAR-10. All experiments were conducted in PyTorch on NVIDIA DGX-1 GPUs.",
    "Limitations": "The computational speed-up demonstrated for transfer learning, achieved by less frequent gradient computation for earlier layers, does not easily translate to training neural networks from scratch. For training from scratch, this approach can significantly reduce generalization performance as earlier layers require more frequent updates. The multirate method introduces an additional hyperparameter 'k', which requires careful selection to balance computational cost and generalization performance. While the paper illustrates the potential of multirate techniques as a regularization method, strong claims are not made, indicating an initial exploration rather than a fully robust solution. Furthermore, the current study primarily uses SGD with consistent hyperparameters across partitions, implying that maximum performance might be constrained compared to more complex hybrid optimization schemes.",
    "Future Research Directions": "Future work could explore different splitting choices for neural network parameters, such as alternative layer-wise partitioning when training from scratch (e.g., separating early from later layers with different time scales) or investigating other architectural components beyond weights and biases. Another promising avenue is the development of hybrid optimization schemes, combining different optimizers (e.g., SGD for slow parts, Adam for fast parts) or distinct optimizer hyperparameters (e.g., for momentum or additive noise in Langevin dynamics) for different partitions. Further research could also integrate multirate methods with existing machine learning techniques like dropout (e.g., multirate variants of dropout) or learning rate scheduling. Exploring the multirate approach with adaptive optimizers, particularly in natural language processing tasks, is also suggested. Finally, additional theoretical work on convergence properties for these extended scenarios is encouraged.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Multirate Training of Neural Networks",
    "Main Contributions": "The paper introduces multirate training for neural networks, partitioning parameters into 'fast' and 'slow' parts updated at different frequencies and stepsizes. It demonstrates significant computational speed-up (almost half the time) for transfer learning tasks in vision and NLP without compromising generalization performance. A novel multirate scheme incorporating linear drift for slow parameters is proposed, and its convergence properties are analyzed against vanilla SGD. Additionally, the paper shows that multirate training can enhance generalization performance when training neural networks from scratch by serving as a regularization technique.",
    "Methodology": "The core methodology involves partitioning neural network parameters (Θ) into fast (ΘF) and slow (ΘS) components. Fast parameters are updated every step with a small stepsize (hF), while slow parameters are updated less frequently (every k steps) with a larger stepsize (hS = k * hF). The proposed scheme, 'Multirate SGD with linear drift' (Algorithm 1), continuously updates slow parameters along a linear path defined by their momenta even when their gradients are not computed, enhancing performance over approaches without linear drift. Stochastic Gradient Descent (SGD) with momentum (µ=0.9) is used as the base optimizer. Partitioning strategies include layer-wise (e.g., final fully connected layer as fast), weights vs. biases, and randomly selected subgroups for regularization. The scheme can be extended to more than two time scales. Convergence analysis is provided under standard assumptions of L-smoothness and bounded stochastic gradient second moment.",
    "Experimental Setup": "For transfer learning, pre-trained ResNet-34 and ResNet-50 architectures (ImageNet) were fine-tuned on CIFAR-10/CIFAR-100 datasets for vision tasks. A pre-trained DistilBERT model was fine-tuned on SST-2 and other GLUE benchmark tasks for NLP. The multirate approach (e.g., final FC layer as fast, rest as slow) was compared against standard fine-tuning methods (training all layers or specific bottom layers). For training from scratch, a WideResNet-16 was used on patch-augmented CIFAR-10 data, and a Single Hidden Layer Perceptron (SHLP) on MNIST, along with a small transformer on Penn Treebank data. Regularization experiments used random subgroups of weights/biases as slow parameters. Ablation studies investigated the effect of linear drift, coupled/uncoupled learning rates, and the hyperparameter k. All experiments were conducted in PyTorch on NVIDIA DGX-1 GPUs. Hyperparameters like learning rates (e.g., h/k=0.001 for ResNets, 1e-4 for DistilBERT) and k (typically 5) were specified.",
    "Limitations": "The computational speed-up observed for transfer learning by selective gradient computation does not directly translate to training from scratch, as this approach significantly reduces generalization performance for earlier layers. While multirate methods with uncoupled learning rates for slow parameters could further enhance performance, it introduces an additional hyperparameter for tuning. The convergence analysis shows that while smaller values of 'k' (the frequency of slow parameter updates) can lower the theoretical upper bound on the gradient norm, they also increase computational cost, presenting a trade-off. The primary focus on SGD with momentum, using identical momentum hyperparameters for all partitions, leaves the exploration of different optimizers or hybrid optimization schemes to future work. The regularization technique is presented to illustrate potential rather than making strong comparative claims, especially regarding its interaction with batch normalization.",
    "Future Research Directions": "Future work includes exploring different splitting choices for neural network parameters, such as layer-wise partitioning when training networks from scratch (e.g., separating early from later layers with different time scales), even if it doesn't yield computational speed-up, for potential generalization enhancement. Further investigation into parameter splittings like making biases (especially input layer biases) slow parameters is suggested. The paper also proposes developing hybrid optimization schemes where different optimizers (e.g., SGD, Adam, SGLD) or optimizer hyperparameters (e.g., momentum, noise levels, initializations) are used for different partitions. Combining multirate methods with existing techniques like dropout (e.g., multirate variants of dropout) and exploring their interaction with learning rate scheduling are also highlighted as promising avenues.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
    "Main Contributions": "The paper addresses the significant manual effort and computing cost associated with tuning learning rate (LR) schedules in Deep Neural Networks (DNNs). It proposes AutoLRS, an efficient method for automatically optimizing the LR for each training stage by modeling training dynamics. AutoLRS leverages Bayesian Optimization (BO) on the fly to minimize validation loss, significantly reducing search cost by using a lightweight exponential model to predict long-term validation loss from short evaluation steps. This approach leads to substantial speedups (1.22x to 1.5x) over original hand-tuned LR schedules and average speedups (1.29x to 1.34x) over heavily-tuned state-of-the-art schedules (CLR, SGDR) across diverse DNNs and tasks, while improving or matching test-set performance.",
    "Methodology": "AutoLRS breaks down the LR schedule optimization into finding a constant LR for every 'τ' training steps (a stage) that minimizes validation loss. It employs Bayesian Optimization (BO) with a Gaussian Process (GP) surrogate model (Matern kernel, LCB acquisition function with κ=1000) to treat the validation loss as a black-box objective. To make BO efficient, candidate LRs are evaluated for only 'τ′' (much smaller than 'τ') steps. A lightweight exponential time-series forecasting model (L(t) = aexp(bt) + c) is trained on these 'τ′' observations to predict the validation loss after 'τ' steps. This creates a mutual-training process between BO and the forecasting model. Practical improvements include gradually increasing 'τ' and 'τ′' during training (starting τ=1000, τ′=100, doubling until τmax) and approximating validation loss with training loss in early stages, then using a small subset of the validation set in later stages.",
    "Experimental Setup": "The effectiveness of AutoLRS was demonstrated on three representative DNNs: ResNet-50 (ImageNet classification using SGD with momentum), Transformer (WMT 2014 English-German neural machine translation using Adam), and BERTBASE (pre-training on English Wikipedia and BooksCorpus with Adam, followed by fine-tuning on MRPC, MNLI, CoLA, and SQuAD v1.1 NLP tasks). Additional experiments included VGG-16 and ResNet-50 on CIFAR-10/100 datasets for comparative analysis. Experiments utilized 32 NVIDIA Tesla V100 GPUs for ResNet-50 and BERT, 8 V100 GPUs for Transformer, and NVIDIA Titan RTX GPUs for CIFAR. Baselines included original hand-tuned LR schedules, heavily hand-tuned state-of-the-art LR schedules like CLR and SGDR (tuned with >10 trials), and prior automatic LR adaptation methods such as Hypergradient Descent (HD), MARTHE, and Hyperband. Performance was measured by speedup (training steps to reach target performance), Top-1 accuracy, BLEU score, and F1 score.",
    "Limitations": "AutoLRS still requires an initial learning rate search interval (ηmin, ηmax) to be set, though it claims it can be reasonably wide and found via an LR range test. The current method does not search for LRs during the warmup phase of training, relying on traditional warmup strategies. While generally robust, the hyperparameter τmax (maximum stage length) could potentially be fine-tuned for further improvements. The exponential loss model, while generally effective, might not perfectly capture non-monotone loss changes in early steps (a corner case mitigated by iterative spline smoothing). Compared to baselines, CLR and SGDR exhibited high sensitivity to hyperparameters and could lead to loss divergence, while hypergradient descent methods were prone to overfitting and failed to match state-of-the-art performance, and Hyperband incurred significant computational overhead.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "import torch\nimport os\nimport logging\nimport socket\nimport string\nimport random\nimport time\n\nclass AutoLRS():\n    def __init__(self, model, optimizer, val_fn, listening_host='localhost', listening_port=12315, warmup_steps=0, warmup_lr=0, summary_steps=1):\n        self._net = model\n        self._optimizer = optimizer\n        self._val_fn = val_fn \n        self._lr = 0.000001\n        self._warmup_steps = warmup_steps\n        self._warmup_lr = warmup_lr \n        self._global_step = 0\n        self._socket = socket.socket()\n        self._started = False\n        self._summary_steps = summary_steps\n        self._checkpoint_path = './checkpoint/autolrs_ckpt_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k = 7))  + '.pth'\n        self._listening_host = listening_host\n        self._listening_port = listening_port\n        self._best_acc = 0\n\n        if not os.path.exists('checkpoint'):\n            os.makedirs('checkpoint')\n\n        self.connect_server()\n    \n    def connect_server(self):\n        self._socket.connect((self._listening_host, self._listening_port))\n\n    def _verbose_operation(self, _op):\n        if self._global_step % self._summary_steps == 0:\n            logging.info(\"[AutoLRS at {}] {}\".format(self._global_step, _op))\n\n    def save_variables(self):\n        \"\"\"Save model parameters and optimizer states.\"\"\"\n        _start_time = time.time()\n        torch.save({\n            'model_state_dict': self._net.state_dict(),\n            'optimizer_state_dict': self._optimizer.state_dict()\n            }, self._checkpoint_path)\n        logging.info(\"[AutoLRS] backup variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def restore_variables(self):\n        _start_time = time.time()\n        checkpoint = torch.load(self._checkpoint_path)\n        self._net.load_state_dict(checkpoint['model_state_dict'])\n        self._optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        logging.info(\"[AutoLRS] restoring variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def on_train_batch_end(self, loss):\n        if self._global_step < self._warmup_steps:\n            # linear warmup\n            self._lr = (self._warmup_lr / self._warmup_steps) * (self._global_step + 1)\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n            self._global_step += 1\n\n        elif not self._started:\n            self.save_variables()\n            print(\"backup trainable variables to CPU\") \n            self._started = True\n            self._socket.send(\",\".join(('startBO', str(loss))).encode(\"utf-8\"))\n            self._verbose_operation(\"Start Bayesian Optimization(BO)\")\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            self._lr = (float(data.split(\",\")[-1]))\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n        else:\n            self._socket.send(','.join(('loss', str(loss))).encode('utf-8'))\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            if data.startswith(\"restore\"):\n                self.restore_variables()\n                self._verbose_operation(\"restore trainable variables\")\n            elif data.startswith(\"ckpt\"):\n                self.save_variables()\n                self._verbose_operation(\"backup trainable variables\")\n            elif data.startswith('evaluate'):\n                val_loss = self._val_fn()\n                self._socket.send(\",\".join((\"val_loss\", str(val_loss))).encode(\"utf-8\"))\n                data = self._socket.recv(1024).decode(\"utf-8\")\n            elif data.startswith('save'):\n                pass\n            else:\n                self._lr = (float(data.split(\",\")[-1]))\n                for param_group in self._optimizer.param_groups:\n                    param_group['lr'] = self._lr\n                self._global_step += 1\n\n\nimport argparse\nimport socket                \nimport random\nimport numpy as np \nimport threading\nimport time\nimport math\nimport os\nfrom skopt import Optimizer\nfrom skopt.space import Real\nfrom scipy.interpolate import UnivariateSpline\nfrom scipy import optimize \nimport sys\nimport logging\n\ndef f(b, x, y):\n    A = np.vstack([np.exp(-np.exp(b) * x), np.ones(len(x))]).T\n    res = np.linalg.lstsq(A, y, rcond=None)[1]\n    return res\n\ndef spline_iter(xs, ys, is_training, spline_deg=2, filter_ratio=0.03, num_of_iter=10, bound=0.5):\n    \"\"\" Use iterative spline to eliminate noise and outliers in the loss series.\n        is_training specifies whether the loss series in use is training loss or validation loss.\n    \"\"\"\n    bound = xs[int((len(xs) - 1) * bound)]\n    if is_training:\n        num_of_iter = 10\n    else:\n        num_of_iter = 1\n\n    for _ in range(num_of_iter):\n        spline_ys = UnivariateSpline(xs, ys, k=spline_deg)(xs)\n        dys = np.abs(ys - spline_ys)\n\n        if is_training:\n            outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[int(round(-len(dys) * filter_ratio)):])\n        else:\n            outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[-1:])\n        outliers = [i for i in outliers if i < bound]\n\n        xs2 = np.zeros(len(xs) - len(outliers))\n        ys2 = np.zeros(len(xs) - len(outliers))\n        i1 = 0\n        for i2 in range(len(xs)):\n            if i2 not in outliers:\n                xs2[i1], ys2[i1] = xs[i2], ys[i2]\n                i1 += 1\n        xs, ys = xs2, ys2\n    return xs, ys\n\ndef exp_forecast(loss_series, end_step, is_training, spline_order=2):\n    \"\"\" Do exponential forecasting on a loss series.\"\"\"\n    xs = np.arange(end_step - len(loss_series), end_step)\n    xs2, ys2 = spline_iter(xs, loss_series, is_training)\n    ys = UnivariateSpline(xs2, ys2, k=spline_order)(xs)\n    logging.debug('ys after spline iter: {}'.format(ys))\n    b = optimize.fmin(f, 0, args=(xs, ys), xtol=1e-5, ftol=1e-5, disp=False)[0]\n    b = -np.exp(b)\n    A = np.vstack([np.exp(b * xs), np.ones(len(xs))]).T\n    a, c = np.linalg.lstsq(A, ys, rcond=None)[0]\n    return a, b, c\n\nclass RingBuffer:\n    \"\"\" A class for storing and manipulating loss series and do exponential forecasting. \"\"\"\n\n    def __init__(self, size):\n        self.data = [None for i in range(size)]\n\n    def reset(self):\n        self.data = [None for i in self.data]\n\n    def append(self, x):\n        self.data.pop(0)\n        self.data.append(x)\n\n    def get(self):\n        return self.data\n\n    def average(self):\n        return sum(self.data)/len(self.data)\n\n    def exponential_forcast(self, pred_index, is_training):\n        loss_series = self.data[:]\n        end_epoch = len(loss_series)\n        x = np.arange(end_epoch - len(loss_series), end_epoch)\n        y = np.array(loss_series)\n        a3, b3, c3 = exp_forecast(y, len(y), is_training)\n        forcast_y = a3 * np.exp(b3 * pred_index) + c3\n        logging.debug(\"Exponential fit: {}, {}, {}\".format(a3, b3, c3))\n        return forcast_y\n\nclass Controller(object):\n    def __init__(self, host, port, min_lr, max_lr):\n        # Constants\n        EXPLOITATION_STEP = 1000\n        LR_STEPS = 100\n        RING_BUFFER_LEN = 100\n        LR_TO_EXPLORE = 10\n        TAU_MAX = 8000\n\n        self.min_lr = float(min_lr)\n        self.max_lr = float(max_lr)\n        self.host = host\n        self.port = port\n        self.threads = []\n        self.num_threads = 1\n        self.event = threading.Event()\n        self.sock = socket.socket()\n        self.sock.bind((self.host, self.port))\n        self.global_step = 0\n        self.last_total_loss = 0.0\n        self.average_loss = 0.0\n        self.loss_vector = []\n        self.lr = 0\n        self.lr_steps = LR_STEPS \n        self.tau_max = TAU_MAX\n        self.val_freq = int(self.lr_steps/16)\n        self.lr_counter = 0\n        self.lr_to_explore = LR_TO_EXPLORE\n        self.BO_stage = True\n        self.val_stage = False\n        self.message = ''\n        self.loss_after_exploitation = None\n        self.ring_buffer_len = RING_BUFFER_LEN\n        if self.val_stage:\n            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len // self.val_freq)\n        else:\n            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n\n        self.exploitation_step = EXPLOITATION_STEP\n        self.exploitation_flag = False \n        self.exploitation_counter = 0\n        \n        self.opt = None\n        self.x_func_dict = dict()\n        self.x_iters = []\n        self.func_val_iters = []\n\n        self.num_ranks = 0\n        self.finished_minions = 0\n        self.lock1 = threading.Lock()\n        self.lock2 = threading.Lock()\n\n    def listen(self):\n        self.sock.listen(20)\n        while True:\n            client, address = self.sock.accept()\n            logging.info('[Server]: Got connection from {}'.format(address))\n            self.threads.append(threading.Thread(target = self.run, args = (client, address, self.event)))\n            if len(self.threads) == self.num_threads:\n                self.num_ranks = len(self.threads)\n                self.num_minions = self.num_ranks - 1\n                logging.info('[Server]: num_ranks: {}'.format(self.num_ranks))\n                for thread in self.threads:\n                    thread.start()\n                logging.info('[Server]: threads started')\n                for thread in self.threads:\n                    thread.join()\n                    self.threads = []\n                logging.info('[Server]: join finished')\n                sys.exit()\n\n    def run(self, c, address, event):\n        size = 1024\n        while True:\n            data = c.recv(size).decode()\n            if not data:\n                sys.exit()\n            logging.debug(data.split(','))\n            total_loss = float(data.split(',')[-1])\n            self.lock2.acquire()\n            self.loss_vector.append(total_loss)\n\n            # compute average loss across ranks\n            if len(self.loss_vector) == self.num_ranks:\n                self.average_loss = sum(self.loss_vector) / len(self.loss_vector) \n                logging.info('[Server]: average loss = {}, step = {}'.format(self.average_loss, self.global_step))\n                if self.val_stage:\n                    if 'val' in data:\n                        self.ring_loss_buffer.append(self.average_loss)\n                    else:\n                        self.global_step += 1\n                else:\n                    self.ring_loss_buffer.append(self.average_loss)\n                    self.global_step += 1\n                self.loss_vector = []\n            self.lock2.release()\n\n            if 'val' in data:\n                c.send(str(self.lr).encode('utf-8')) \n                continue\n\n            if 'minion' in data:\n                # blocking\n                event_is_set = event.wait()\n                c.send(self.message.encode('utf-8')) \n                logging.debug('[Server] message: %s', self.message)\n                self.lock1.acquire()\n                self.finished_minions += 1\n                self.lock1.release()\n                if self.finished_minions == self.num_minions:\n                    event.clear()\n                    self.finished_minions = 0\n                continue\n\n            else:\n                if data.startswith('startBO'):\n                    self.last_total_loss = self.average_loss\n                    self.init_loss = self.average_loss\n                    self.loss_after_exploitation = self.average_loss\n\n                # exploitation stage -- actual training stage using the best-found LR\n                if self.exploitation_flag:\n                    logging.debug('[Server exploitation]: average loss ' + str(self.average_loss) + ' step=' + str(self.global_step))\n                    if self.exploitation_counter == self.exploitation_step:\n                        self.BO_stage = True\n                        self.exploitation_flag = False\n                        self.exploitation_counter = 0\n                        logging.info('[Server]: exploitation stage done')\n                        logging.info('[Server]: reconfigure...')\n                        if self.lr_steps < self.tau_max / 10:\n                            self.lr_steps = self.lr_steps * 2\n                            self.val_freq = int(self.lr_steps/16)\n                            self.ring_buffer_len = self.lr_steps \n                            self.exploitation_step = self.exploitation_step * 2\n                            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n                        else:\n                            self.val_stage = True\n                        if self.val_stage:\n                            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len // self.val_freq)\n                        self.loss_after_exploitation = self.average_loss\n\n                        self.message = 'save'\n                        c.send(self.message.encode('utf-8'))\n                        event.set()\n                        continue\n\n                    else:\n                        self.exploitation_counter += 1\n                        self.message = str(self.lr)\n                        c.send(str(self.lr).encode('utf-8')) \n                        event.set()\n                        continue\n\n                # BO stage -- LR search stage\n                if self.BO_stage:\n                    self.opt = Optimizer([Real(self.min_lr, self.max_lr, 'log-uniform')], \"GP\", n_initial_points=1, acq_func='LCB', acq_func_kwargs={'kappa':1e6})\n                    self.BO_stage = False\n                    self.lr = self.opt.ask()[0]\n\n                    # prevent BO in scikit-optimize from searching for the same LR explored before\n                    while True:\n                        if self.lr in self.x_func_dict:\n                            self.opt.tell([self.lr], self.x_func_dict[self.lr])\n                            self.lr = self.opt.ask()[0]\n                        else:\n                            break\n\n                    self.message = str(','.join(('ckpt', str(self.lr))))\n                    c.send(','.join(('ckpt', str(self.lr))).encode('utf-8'))\n                    event.set()\n                    logging.debug('[Server]: checkpoint command sent')\n                    continue\n\n                # ask BO to suggest the next LR \n                if self.lr_counter == self.lr_steps:\n                    logging.debug('ring_buffer: {}'.format(self.ring_loss_buffer.get()))\n                    if any([math.isnan(x) for x in self.ring_loss_buffer.get()]):\n                        predicted_loss = \"nan\"\n                    elif self.val_stage:\n                        predicted_loss = self.ring_loss_buffer.exponential_forcast(pred_index=int(self.exploitation_step/self.val_freq), is_training=False)\n                        current_loss = sum(self.ring_loss_buffer.get()[-1:])/1.0\n                    else:\n                        predicted_loss = self.ring_loss_buffer.exponential_forcast(pred_index=self.exploitation_step, is_training=True)\n                        current_loss = sum(self.ring_loss_buffer.get()[-10:])/10.0\n\n                    logging.info('[Server]: predicted loss: {} due to LR {}'.format(predicted_loss, self.lr))\n\n                    # Huge loss jump can make the exponential prediction inaccurate, so set a threshold here. \n                    #if self.loss_after_exploitation is not None and max(self.ring_loss_buffer.get()) > 10 * self.loss_after_exploitation:\n                    #    predicted_loss = current_loss \n                    #    logging.info('New predicted_loss: ' + str(predicted_loss))\n\n                    if self.loss_after_exploitation is not None and max(self.ring_loss_buffer.get()) >= 1.0 * self.init_loss and self.val_stage:\n                        predicted_loss = current_loss \n                        logging.info('[Server]: New predicted_loss: ' + str(predicted_loss))\n\n                    if self.val_stage:\n                        self.ring_loss_buffer = RingBuffer(int(math.floor(self.ring_buffer_len)/self.val_freq))\n                    else:\n                        self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n\n                    # feed a (LR, predicted loss in tau steps) instance to BO.\n                    if str(predicted_loss) == 'nan':\n                        self.opt.tell([float(self.lr)], 1e6)\n                    else:\n                        self.opt.tell([float(self.lr)], predicted_loss)\n                    self.x_iters.append(float(self.lr))\n                    self.func_val_iters.append(predicted_loss)\n                    self.x_func_dict[self.lr] = predicted_loss\n                    self.lr_counter = 1\n\n                    if len(self.func_val_iters) == self.lr_to_explore:\n                        min_index = self.func_val_iters.index(min(self.func_val_iters))\n\n                        # log the best lr found for the next stage.\n                        logging.info('[Server]: best LR: {}, min loss: {}'.format(self.x_iters[min_index], self.func_val_iters[min_index]))\n\n                        self.lr = self.x_iters[min_index]\n                        self.message = str(','.join(('restore', str(self.lr))))\n                        c.send(','.join(('restore', str(self.lr))).encode('utf-8'))\n                        event.set()\n                        logging.debug('[Server]: restore command sent')\n                        self.exploitation_flag = True\n\n                        self.func_val_iters = []\n                        self.x_iters = []\n                        self.x_func_dict = dict()\n                    else:\n                        # Ask BO for the next LR to explore\n                        self.lr = self.opt.ask()[0]\n                        while True:\n                            if self.lr in self.x_func_dict:\n                                self.opt.tell([self.lr], self.x_func_dict[self.lr])\n                                self.lr = self.opt.ask()[0]\n                            else:\n                                break\n                        self.message = str(','.join(('restore', str(self.lr))))\n                        c.send(','.join(('restore', str(self.lr))).encode('utf-8'))\n                        event.set()\n                        logging.debug('[Server]: restore command sent')\n                else:\n                    self.lr_counter += 1\n                    if self.val_stage and self.lr_counter % self.val_freq == 0:\n                        self.message = \"evaluate\"\n                        c.send(self.message.encode('utf-8')) \n                    else:\n                        self.message = str(self.lr)\n                        c.send(str(self.lr).encode('utf-8')) \n                    event.set()\n\ndef val_fn():\n    net.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(valloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item()\n    net.train()\n    return val_loss\n",
    "Experiment Result": "The AutoLRS method optimizes the LR schedule by finding a constant LR for every 'τ' training steps (a stage). This involves a mutual-training process between Bayesian Optimization (BO) and an exponential time-series forecasting model.\n\n**Key Settings and Stages:**\n- **Stage Length (τ) and Evaluation Steps (τ'):**\n    - Initial τ (EXPLOITATION_STEP) = 1000 training steps.\n    - Initial τ' (LR_STEPS) = 100 steps for evaluating candidate LRs.\n    - During training, τ and τ' are gradually increased by doubling them. This continues until τ reaches TAU_MAX = 8000 steps.\n\n- **Bayesian Optimization (BO):**\n    - Uses `skopt.Optimizer` with a Gaussian Process (GP) surrogate model (`\"GP\"`).\n    - Employs the Lower Confidence Bound (LCB) acquisition function (`acq_func='LCB'`) with `kappa=1e6`.\n    - The search space for learning rates is defined as a `log-uniform` distribution between `min_lr` and `max_lr` (provided as command-line arguments to the server).\n    - BO explores `LR_TO_EXPLORE = 10` candidate LRs per stage before selecting the best one for exploitation.\n\n- **Forecasting Model:**\n    - An exponential time-series forecasting model `L(t) = a * exp(b * t) + c` is used to predict validation loss after τ steps.\n    - The coefficients `a`, `b`, `c` are fitted using `scipy.optimize.fmin` (for `b`) and `numpy.linalg.lstsq` (for `a` and `c`).\n    - Before forecasting, loss series are smoothed using `scipy.interpolate.UnivariateSpline` with iterative outlier elimination (`spline_iter`).\n\n- **Validation Loss Approximation:**\n    - In early stages (`val_stage` is False), training loss is used to approximate validation loss for forecasting.\n    - In later stages (`val_stage` is True), a small subset of the actual validation set is used. This subset consists of `VAL_LEN = 10` mini-batches from the test set (`valloader` in `cifar10_example.py`). The validation frequency (`val_freq`) is `int(lr_steps/16)`.\n\n- **Checkpointing:** Model parameters and optimizer states are saved and restored at different points during the BO process (e.g., before exploring a new LR and when switching to the exploitation stage)."
}{
    "Title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints",
    "Main Contributions": "This paper introduces a formal setting for studying deep neural network training under resource constraints, termed 'budgeted training'. It addresses the problem of achieving the best possible performance given a fixed resource budget (specifically, number of optimization iterations). The key contribution is demonstrating that adjusting the learning rate schedule to be budget-aware is critical, with a simple linear decay schedule proving robust and high-performing across various tasks and budgets. The paper also identifies 'budgeted convergence', where the gradient vanishes at the end of the allocated budget, and shows that budget-aware schedules significantly outperform existing fast-converging methods under practical budgeted training conditions.",
    "Methodology": "The methodology introduces 'budget-aware schedules' which define the learning rate as a function of training progress (current step divided by total budget). A general 'Budget-Aware Conversion (BAC)' strategy is proposed to rescale existing learning rate schedules to be budget-aware. The paper introduces and thoroughly evaluates a simple 'linear schedule' (βp = 1-p) which is completely parameter-free. This linear schedule is compared against various existing schedules including constant, step decay, exponential, poly, cosine, and htd. The study also analyzes the phenomenon of 'budgeted convergence' by observing the correlation between the learning rate and the full gradient norm over time. Additionally, it compares budget-aware schedules against adaptive learning rate methods (AMSGrad, AdaBound, RMSprop) and methods using warm restarts (SGDR).",
    "Experimental Setup": "The research conducted extensive experiments across five standard vision benchmarks. For Image Classification, ResNet-18 was used on ImageNet and CIFAR-10. For Object Detection and Instance Segmentation, Mask R-CNN with a ResNet-50 backbone was used on MS COCO. For Semantic Segmentation, PSPNet with a ResNet-50 backbone was used on Cityscapes. For Video Classification, an I3D variant was used on Kinetics. Various budget percentages (1%, 5%, 10%, 25%, 50%, 100% of full training epochs/iterations) were evaluated. Validation metrics included top-1 accuracy (ImageNet, CIFAR-10, Kinetics), COCO AP (MS COCO), and mIoU (Cityscapes). Experiments involved SGD with momentum, specific base learning rates, and weight decay. Additional experiments involved evaluating rank prediction capability using Kendall’s tau on 100 random architectures generated from ResNet-18 modifications on CIFAR-10.",
    "Limitations": "The study assumes a constant network architecture throughout training, though it suggests exploring dynamic architecture changes. The exact mechanism by which the learning rate influences the gradient magnitude in 'budgeted convergence' remains unclear. The globally optimal solution may not be achievable in the budgeted setting. While smooth-decaying schedules (linear, cosine, poly) perform similarly well, it is inconclusive if one is strictly superior to others across all tasks. The comparison with sophisticated data subsampling methods is acknowledged as challenging for extremely large datasets, and simpler random subsampling on CIFAR-10 may not fully represent their potential. The initial learning rate is noted to remain a critical hyper-parameter that requires tuning even with budget-aware schedules.",
    "Future Research Directions": "An open question for future research is whether a method can be developed that achieves both promising anytime performance and optimal budgeted performance simultaneously. The paper also implicitly suggests that entertaining changes in network architecture during training could be an interesting area to explore, moving beyond the current assumption of a constant architecture.",
    "Experiment Code": "optimizer = torch.optim.SGD(model.parameters(), args.lr,momentum=args.momentum,weight_decay=args.weight_decay)scheduler = StepLR(optimizer, step_size=30, gamma=0.1)for epoch in range(args.start_epoch, args.epochs):    # train for one epoch    train(train_loader, model, criterion, optimizer, epoch, device, args)    # evaluate on validation set    acc1 = validate(val_loader, model, criterion, args)    scheduler.step()",
    "Experiment Result": "The repository content does not include implementations of the core methodologies described in the 'Method', such as 'budget-aware schedules', 'Budget-Aware Conversion (BAC)' strategy, the 'parameter-free linear schedule', or analysis of 'budgeted convergence' by observing the correlation between learning rate and full gradient norm over time. It also does not include implementations of AMSGrad, AdaBound, RMSprop, or methods using warm restarts (SGDR).However, it does implement a 'StepLR' scheduler, which represents a 'step decay' learning rate schedule. This is explicitly mentioned in the 'Method' as one of the 'various existing schedules' against which the proposed budget-aware schedules are compared.Experimental settings extracted from 'imagenet/main.py':- Optimizer: SGD- Initial Learning Rate: args.lr (default 0.1)- Momentum: args.momentum (default 0.9)- Weight Decay: args.weight_decay (default 1e-4)- Learning Rate Scheduler: StepLR- Step Size: 30 epochs (The learning rate is decayed by 10 every 30 epochs)- Gamma: 0.1 (The learning rate is multiplied by this factor at each step)- Total Epochs: args.epochs (default 90)"
}{
    "Title": "Learning Rate Schedules in the Presence of Distribution Shift",
    "Main Contributions": "This paper designs learning rate schedules to minimize regret for SGD-based online learning in the presence of changing data distributions. It fully characterizes the optimal learning rate schedule for online linear regression using a novel stochastic differential equation (SDE) analysis. For general convex loss functions, new learning rate schedules robust to distribution shift are proposed, with upper and lower bounds for regret differing only by constants. For non-convex loss functions, a notion of regret based on the gradient norm is defined, and a learning schedule is proposed to minimize an upper bound on total expected regret. A key finding across all settings is that optimal learning rate schedules typically increase in the presence of distribution shift.",
    "Methodology": "For online linear regression, the method involves deriving a novel stochastic differential equation (SDE) that approximates SGD dynamics under distribution shift. Optimal learning rates are then derived by analyzing this SDE using tools like Grönwall’s inequality and Itô’s lemma, framing it as an optimal control problem solved via dynamic programming and Euler method (Algorithm 1). For general convex loss functions, an intricate treatment of regret terms, properties of convex functions, and SGD is used to derive and minimize an upper bound on total regret, leading to an adaptive schedule (Proposition 4.3). A corresponding lower bound for strongly convex loss functions is also provided. For non-convex loss functions, the notion of regret is modified to use the gradient norm of estimated models, and an upper bound for cumulative regret is derived and minimized, resulting in a closed-form optimal learning rate.",
    "Experimental Setup": "Experiments use TensorFlow and Keras. High-dimensional regression models (linear and logistic) are evaluated. For linear regression, Algorithm 1 is tested with time-varying coefficient paths (spirals) in up to 128 dimensions, using N(0,1) covariates and N(0,0.1) noise, with a batch size of 256. Continuous vs. discrete distribution shifts are studied by downsampling paths. For logistic regression, Proposition 4.3 is applied, using binary cross-entropy loss, and specific parameter settings for σ, L, and Dmax. A medical application in flow cytometry uses neural networks to classify murine pancreatic cells (10-dimensional RNA expression data) from Bastidas-Ponce et al. (2019). The simulation involves a stream of cells with distributions shifting over time, optimized using Adam with different initial learning rates and optional resets at distribution shifts. Performance is measured by cumulative regret.",
    "Limitations": "The SDE analysis for linear regression relies on assumptions about the regularity and boundedness of learning rate and batch size functions (Assumptions A1, A2). For general convex losses, Assumption 4.1 requires bounded variance of batch gradients, L-smoothness of the expected loss, and oracle models residing within a bounded set. A specific lower bound for convex losses further assumes µ-strong convexity (Assumption 4.6). For non-convex losses, the defined regret notion uses the gradient norm, which does not differentiate between local or global minima. The proposed learning rate schedules depend on distribution shifts (γt), which, if not directly available, must be estimated (e.g., using an exponential moving average). The optimization for convex losses minimizes an upper bound, which could be conservative if the bound is crude.",
    "Future Research Directions": "Future work includes extending the SDE framework to develop adaptive adjustment schemes for other hyperparameters in SGD variants (e.g., Polyak averaging, SVRG, elastic averaging SGD) and deriving effective adaptive momentum parameter adjustment policies. Another proposed direction is to study \"model hedging,\" which involves quantifying how neutral a model should remain at a given time to optimally trade off between underfitting the current distribution and being able to quickly adapt to a possibly adversarial future distribution.",
    "Experiment Code": "def unpack_optimizer_str(optimizer_str):\n    tokens = optimizer_str.split(':')\n    assert len(tokens) >= 1\n    name = tokens[0]\n    assert name in ['sgd', 'adam', 'adam-restart']\n    if name == 'sgd':\n        learning_rate = 0.01\n        if len(tokens) >= 2:\n            learning_rate = eval(tokens[1])\n        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n    elif name in ['adam', 'adam-restart']:\n        learning_rate = 0.001\n        if len(tokens) >= 2:\n            learning_rate = eval(tokens[1])\n        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n    return optimizer\n\ndef build_compiled_model(optimizer_str, multiclass):\n    initializer = keras.initializers.GlorotUniform(seed=SEED)\n    regularizer = keras.regularizers.L2(1e-4)\n    optimizer = unpack_optimizer_str(optimizer_str)\n    inputs = keras.Input(shape=(10,), name='input')\n    \n    if multiclass:\n        x_1 = keras.layers.Dense(units=64, activation='elu', name='dense_1',\n                                 kernel_initializer=initializer,\n                                 kernel_regularizer=regularizer)(inputs)\n        x_2 = keras.layers.Dense(units=32, activation='elu', name='dense_2',\n                                 kernel_initializer=initializer,\n                                 kernel_regularizer=regularizer)(x_1)\n        x_3 = keras.layers.Dense(units=16, activation='elu', name='dense_3',\n                                 kernel_initializer=initializer,\n                                 kernel_regularizer=regularizer)(x_2)\n        x_4 = keras.layers.Dense(units=8, activation='elu', name='dense_4',\n                                 kernel_initializer=initializer,\n                                 kernel_regularizer=regularizer)(x_3)\n        outputs = keras.layers.Dense(units=4, kernel_initializer=initializer,\n                                     kernel_regularizer=regularizer)(x_4)\n        loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n    else:\n        x = keras.layers.Dense(units=10, activation='elu', name='dense_1', \n                               kernel_initializer=initializer,\n                               kernel_regularizer=regularizer)(inputs)\n        outputs = keras.layers.Dense(units=1,\n                                     activation='sigmoid',\n                                     name='prediction',\n                                     kernel_initializer=initializer,\n                                     kernel_regularizer=regularizer)(x)\n        loss=keras.losses.MeanSquaredError()\n\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=optimizer, loss=loss,\n                  metrics=[tf.keras.metrics.CategoricalAccuracy()])\n    return model\n\ndef run_baseline_experiment_v2(adata, multiclass, num_steps_per_phase, batch_size, optimizer_str):\n    keras.backend.clear_session()\n    model = build_compiled_model(optimizer_str, multiclass)\n    losses = []\n    val_losses = []\n\n    distribution_types = [0, 1, 2, 2, 3, 3]\n    for phase in range(len(distribution_types)):\n        distribution_type = distribution_types[phase]\n        if 'restart' in optimizer_str:\n            if phase == 0 or distribution_type == distribution_types[phase - 1]:\n                if multiclass:\n                    loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n                else:\n                    loss = keras.losses.MeanSquaredError()\n                optimizer = unpack_optimizer_str(optimizer_str)\n                model.compile(optimizer=optimizer, loss=loss)\n        for step in range(num_steps_per_phase):\n            lr = keras.backend.eval(model.optimizer._learning_rate)\n            print('optimizer_str:', optimizer_str, 'phase:', phase, 'step:', step, 'learning_rate:', lr)\n            num_samples = 2 * batch_size\n            X, y = create_training_data(adata, num_samples, multiclass, distribution_type)\n            history = model.fit(X, y, batch_size=batch_size, epochs=1, validation_split=0.5, shuffle=False)\n            losses.append(history.history['loss'][0])\n            val_losses.append(history.history['val_loss'][0])\n\n    losses = np.array(losses)\n    val_losses = np.array(val_losses)\n    return losses, val_losses",
    "Experiment Result": "The experiments are conducted with a `SEED = 0` and an `EPS = 0.1` for noise perturbation during sample generation. The `MULTICLASS` setting is set to `True`. Data is sourced from `scv.datasets.pancreas()`, filtered for genes with `min_shared_counts=30000`, and normalized per cell. Samples are generated by `perturb_and_pick` which selects a cell type ('Alpha', 'Beta', 'Delta', 'Ductal') and adds uniform noise (1.0 - EPS to 1.0 + EPS). Training data is created using `generate_samples` with varying probabilities (p1, p2, p3) for different cell types, simulating distribution shifts: type 1 (Delta), type 2 (Alpha, Delta), type 3 (Alpha, Beta, Delta), and type 0 (Ductal by default). The experimental setup involves a sequence of distribution types: `[0, 1, 2, 2, 3, 3]`. The neural network model `build_compiled_model` uses `keras.Input(shape=(10,))`. For multiclass classification, it has 4 `Dense` layers with `elu` activation (units: 64, 32, 16, 8) followed by a final `Dense` layer with 4 units; `CategoricalCrossentropy(from_logits=True)` is used as loss. For non-multiclass, it has one `Dense` layer with `elu` activation (10 units) and a final `Dense` layer with 1 unit and `sigmoid` activation; `MeanSquaredError` is used as loss. All dense layers use `GlorotUniform(seed=SEED)` for kernel initialization and `L2(1e-4)` for kernel regularization. The experiments compare `Adam` and `Adam-restart` optimizers with learning rates `0.01` and `0.03`. Each experiment runs for `NUM_STEPS_PER_PHASE = 100` steps per distribution phase with a `BATCH_SIZE = 64`. The `adam-restart` strategy re-compiles the model with a new optimizer instance when the distribution type changes or at the start of the first phase. Performance is evaluated by plotting cumulative training and validation losses, referred to as 'regret'."
}{
    "Title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations",
    "Main Contributions": "This research addresses the complexity and expense of LLM training and scaling research caused by the cosine learning rate schedule. The main contributions include demonstrating that a simple alternative, a constant learning rate with a cooldown phase, matches or outperforms cosine performance. It introduces a novel (1-sqrt) cooldown function that is more effective than linear decay. The paper also shows that stochastic weight averaging (SWA) improves generalization without additional training costs. Critically, these findings enable scaling experiments with significantly reduced compute and GPU hours (up to 50% savings), making LLM training more flexible for continual learning and facilitating more frequent updates to scaling laws for data mixtures and novel architectures.",
    "Methodology": "The study investigates various learning rate schedules: the standard cosine decay, a constant learning rate combined with a cooldown phase (referred to as trapezoidal or warmup-stable-decay), and schedule-free optimization (SFO). For the cooldown phase, it explores different functional forms including linear, (1-sqrt), cosine, mirror cosine, and (1-square) decays. Stochastic Weight Averaging (SWA) is implemented by averaging weights within fixed windows (h=500 steps). All models are decoder-only transformers, architecturally similar to LLaMa, using AdamW optimizer with weight decay, gradient clipping, SwiGLU activations, RoPE embeddings, and RMSNorm.",
    "Experimental Setup": "Experiments are conducted on a range of model sizes from 33M to 8B parameters. Initial comparisons use a 210M parameter model on a 6B token subset of SlimPajama. Larger scale validation involves a 1B model trained on 100B and 460B tokens of the FineWeb corpus, and an 8B model on 12B tokens of FineWeb-Edu. Scaling law experiments encompass models from 33M to 360M parameters across different token scales (0.3B-10B) on SlimPajama, and additional verification on OpenWebText2. Evaluation metrics include validation loss (perplexity) and downstream benchmarks for larger models such as MMLU, ARC, OpenBookQA, PIQA, HellaSwag, CommonSenseQA, SIQA, and Winogrande, reporting aggregated and individual scores (accuracy normalized). Training parameters typically include AdamW with (0.9, 0.95) betas, 0.1 weight decay, and 1.0 gradient clipping. Infrastructure involves A100 and H100 GPUs, utilizing 2 data-parallel GPUs for most runs, 4xH100s for 1B models, and 12 nodes of 4xGH200 GPUs with tensor parallelism for 8B models, with an estimated total cost of 2500-3000 GPU hours.",
    "Limitations": "The experiments are conducted on models up to 8B parameters; while trends are consistent, training behavior can be more brittle at much larger, modern scales and extremely long training durations. Although methods like QK norm can alleviate instabilities from high learning rates, this is not fully explored within the scope of the proposed schedules. The Schedule-Free Optimizer (SFO) is found to be sensitive to momentum parameters, which may depend on training length, diminishing its 'schedule-free' claim. Annealing the cosine schedule to zero (instead of 10%) can sometimes hurt downstream metrics due to 'too early saturation.' Furthermore, not all downstream benchmarks consistently benefit from the cooldown phase as clearly as the perplexity loss, suggesting areas for further investigation.",
    "Future Research Directions": "Future research could investigate whether a single cooldown schedule achieves absolute optimality given a total compute budget for LLM training. Exploring the curriculum aspect of changing data mixtures during the cooldown phase is another promising direction. Further analysis into the impact of the functional form of the cooldown, beyond the (1-sqrt) function and varying exponents, is also suggested. Understanding why certain downstream benchmarks do not benefit as clearly from cooldown as perplexity loss presents an interesting area for deeper exploration. The methods presented facilitate more frequent computation of scaling laws for data mixtures and novel architectures, which is an explicit future application emphasized by the authors.",
    "Experiment Code": "File Path: src/config/base.py\nContent:\n    parser.add_argument(\n        \"--scheduler\",\n        default=\"cos\",\n        choices=[\"linear\", \"cos\", \"wsd\", \"none\", \"cos_inf\"],\n    )\n    parser.add_argument(\"--cos-inf-steps\", default=0, type=int)\n    parser.add_argument(\"--iterations\", default=15000, type=int)\n    parser.add_argument(\"--warmup-steps\", default=300, type=int)\n    parser.add_argument(\"--lr\", default=1e-3, type=float)\n    # wsd\n    parser.add_argument(\"--wsd-final-lr-scale\", default=0.0, type=float)\n    parser.add_argument(\"--wsd-fract-decay\", default=0.1, type=float)\n    parser.add_argument(\n        \"--decay-type\",\n        default=\"linear\",\n        choices=[\"linear\", \"cosine\", \"exp\", \"miror_cosine\", \"square\", \"sqrt\"],\n    )\n    # Optimization\n    parser.add_argument(\"--opt\", default=\"adamw\", choices=[\"adamw\", \"sgd\", \"SFAdamW\"])\n    parser.add_argument(\"--weight-decay\", default=1e-1, type=float)\n    parser.add_argument(\"--beta1\", default=0.9, type=float)\n    parser.add_argument(\"--beta2\", default=0.95, type=float)\n    parser.add_argument(\n        \"--grad-clip\", default=1.0, type=float\n    )\n    # Weight Averaging\n    parser.add_argument(\"--weight-average\", action=\"store_true\")\n    parser.add_argument(\n        \"--wa-interval\",\n        default=5,\n        type=int,\n        help=\"How often to take the average (every k steps). Must divide wa-horizon.\",\n    )\n    parser.add_argument(\n        \"--wa-horizon\",\n        default=500,\n        type=int,\n        help=\"How frequently we save uniform model averages. Should divide \"\n        + \"latest-ckpt-interval, otherwise some points may not be saved \"\n        + \"correctly.\",\n    )\n    parser.add_argument(\n        \"--wa-dtype\",\n        default=\"float32\",\n        type=str,\n        choices=[\"float32\", \"float64\"],\n    )\n    # Model params\n    parser.add_argument(\n        \"--model\",\n        default=\"llama\",\n        choices=[\n            \"base\",\n            \"llama\",\n        ],\n    )\n    parser.add_argument(\"--n-head\", default=12, type=int)\n    parser.add_argument(\"--n-layer\", default=24, type=int)\n    parser.add_argument(\"--sequence-length\", default=512, type=int)\n    parser.add_argument(\n        \"--n-embd\", default=768, type=int\n    )\n    parser.add_argument(\n        \"--multiple-of\",\n        default=256,\n        type=int,\n    )\n    parser.add_argument(\"--rmsnorm-eps\", default=1e-5, type=float)\n\nFile Path: src/main.py\nContent:\n    if args.opt == \"adamw\":\n        opt = torch.optim.AdamW(\n            group_specs,\n            lr=args.lr,\n            betas=(args.beta1, args.beta2),\n            weight_decay=args.weight_decay,\n        )\n    elif args.opt == \"SFAdamW\":\n        opt = schedulefree.AdamWScheduleFree(\n            group_specs,\n            lr=args.lr,\n            betas=(args.beta1, args.beta2),\n            weight_decay=args.weight_decay,\n            warmup_steps=args.warmup_steps,\n        )\n\n    else:\n        opt = torch.optim.SGD(\n            group_specs, lr=args.lr, momentum=0.9, weight_decay=args.weight_decay\n        )\n\n    if args.scheduler != \"none\":\n        assert args.warmup_steps < args.iterations, \"Warmup steps must be < iterations.\"\n        if args.scheduler in [\"cos\", \"linear\"]:\n            scheduler = torch.optim.lr_scheduler.OneCycleLR(\n                optimizer=opt,\n                max_lr=[group.get(\"lr\", args.lr) for group in group_specs],\n                total_steps=args.iterations,\n                pct_start=args.warmup_steps / args.iterations,\n                anneal_strategy=args.scheduler,\n                cycle_momentum=False,\n                div_factor=1e2,\n                final_div_factor=0.1,\n            )\n        elif args.scheduler == \"cos_inf\":\n            lambda_schedule = cos_inf_schedule(\n                n_iterations=args.iterations,\n                n_warmup=args.warmup_steps,\n                n_inf=args.cos_inf_steps,\n                div_factor=1e2,\n                final_div_factor=0.1,\n            )\n            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lambda_schedule)\n        elif args.scheduler == \"wsd\":\n            lambda_schedule = wsd_schedule(\n                n_iterations=args.iterations,\n                n_warmup=args.warmup_steps,\n                fract_decay=args.wsd_fract_decay,\n                init_div_factor=1e2,\n                final_lr_factor=args.wsd_final_lr_scale,\n                decay_type=args.decay_type,\n            )\n            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lambda_schedule)\n\nFile Path: src/optim/base.py\nContent:\n    if cfg.grad_clip != 0.0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n    # ... (inside train function)\n    if cfg.weight_average:\n        weight_averager = WeightAverager(\n            not_compiled_model,\n            horizon=cfg.wa_horizon,\n            interval=cfg.wa_interval,\n            save_dir=None if cfg.wa_use_temp_dir else exp_dir / \"avgs\",\n            dtype={\n                \"float32\": torch.float32,\n                \"float64\": torch.float64,\n            }[cfg.wa_dtype],\n            count=curr_iter,\n        )\n    # ... (inside training loop)\n    if cfg.weight_average:\n        weight_averager.step(not_compiled_model, distributed_backend.is_master_process())\n\nFile Path: src/optim/utils.py\nContent:\ndef cos_inf_schedule(n_iterations, n_warmup, div_factor, final_div_factor, n_inf):\n    max_lr = 1.0\n    base_lr = max_lr / div_factor\n    final_lr = base_lr / final_div_factor\n    n_anneal_steps = n_iterations - n_inf\n    def schedule(step):\n        if step < n_warmup:\n            return (step / n_warmup) + (1 - step / n_warmup) / div_factor\n        elif step < n_anneal_steps:\n            t = (step - n_warmup) / (n_anneal_steps - n_warmup)\n            lr = final_lr + 0.5 * (max_lr - final_lr) * (1 + np.cos(np.pi * t))\n            return lr\n        else:\n            return final_lr\n    return schedule\n\ndef wsd_schedule(\n    n_iterations,\n    final_lr_factor=0.0,\n    n_warmup=1000,\n    init_div_factor=100,\n    fract_decay=0.1,\n    decay_type=\"linear\",\n):\n    n_anneal_steps = int(fract_decay * n_iterations)\n    n_hold = n_iterations - n_anneal_steps\n\n    def schedule(step):\n        if step < n_warmup:\n            return (step / n_warmup) + (1 - step / n_warmup) / init_div_factor\n        elif step < n_hold:\n            return 1.0\n        elif step < n_iterations:\n            if decay_type == \"linear\":\n                return final_lr_factor + (1 - final_lr_factor) * (\n                    1 - (step - n_hold) / n_anneal_steps\n                )\n            elif decay_type == \"exp\":\n                return final_lr_factor ** ((step - n_hold) / n_anneal_steps)\n            elif decay_type == \"cosine\":\n                return (\n                    final_lr_factor\n                    + (1 - final_lr_factor)\n                    * (1 + math.cos(math.pi * (step - n_hold) / n_anneal_steps))\n                    * 0.5\n                )\n            elif decay_type == \"miror_cosine\":\n                cosine_value = (\n                    final_lr_factor\n                    + (1 - final_lr_factor)\n                    * (1 + math.cos(math.pi * (step - n_hold) / n_anneal_steps))\n                    * 0.5\n                )\n                linear_value = final_lr_factor + (1 - final_lr_factor) * (\n                    1 - (step - n_hold) / n_anneal_steps\n                )\n                return linear_value * 2 - cosine_value\n            elif decay_type == \"square\":\n                return final_lr_factor + (1 - final_lr_factor) * (\n                    1 - ((step - n_hold) / n_anneal_steps) ** 2\n                )\n\n            elif decay_type == \"sqrt\":\n                return final_lr_factor + (1 - final_lr_factor) * (\n                    1 - math.sqrt((step - n_hold) / n_anneal_steps)\n                )\n\n            else:\n                raise ValueError(\n                    f\"decay type {decay_type} is not in ['cosine','miror_cosine','linear','exp']\"\n                )\n\n        else:\n            return final_lr_factor\n    return schedule\n\nFile Path: src/optim/weight_averaging.py\nContent:\nclass WeightAverager:\n    def __init__(\n        self,\n        model,\n        horizon=100,\n        interval=1,\n        save_dir=None,\n        device=None,\n        dtype=torch.float32,\n        count=0,\n    ):\n        super().__init__()\n        self.device = device\n        self.dtype = dtype\n        if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n            model = model.module\n        self.module = deepcopy(model).to(dtype=self.dtype, device=device)\n\n        assert horizon % interval == 0, \"Interval should divide period\"\n        self.interval = interval\n        self.horizon = horizon\n        self.period = horizon // interval\n        if save_dir is None:\n            self._tempdir = tempfile.TemporaryDirectory()\n            self.save_dir = Path(self._tempdir.name)\n        else:\n            self.save_dir = Path(save_dir)\n            self.save_dir.mkdir(parents=True, exist_ok=True)\n        self.count = count\n        self.num_saved = len(\n            [f for f in self.save_dir.iterdir() if f.is_file() and int(f.stem) <= count]\n        )\n\n    @torch.no_grad()\n    def step(self, model, is_master_rank=True):\n        if self.count % self.interval == 0:\n            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n                model = model.module\n            for key, avg in self.module.state_dict().items():\n                curr = model.state_dict()[key].to(device=self.device, dtype=avg.dtype)\n                rate = 1 / ((self.count % self.horizon) // self.interval + 1)\n                avg.copy_(torch.lerp(avg, curr, rate))\n\n        self.count += 1\n\n        if self.count % self.horizon == 0 and is_master_rank:\n            torch.save(\n                self.module.to().state_dict(),\n                self.save_dir / f\"{self.count}.pt\",\n            )\n            self.num_saved += 1\n\n    def get_latest_like(self, model):\n        if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n            model = model.module\n        new_model = deepcopy(model)\n        count = self.count - self.count % self.horizon\n        latest_path = self.save_dir / f\"{count}.pt\"\n        map_and_load_state_dict(new_model, torch.load(latest_path))\n        return new_model\n\nFile Path: src/models/llama.py\nContent:\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    t = torch.arange(end, device=freqs.device) # type: ignore\n    freqs = torch.outer(t, freqs).float() # type: ignore\n    cos_freqs = torch.cos(freqs)\n    sin_freqs = torch.sin(freqs)\n    return torch.stack((cos_freqs, sin_freqs), dim=-1)\n\ndef _reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n    ndim = x.ndim\n    assert 1 < ndim\n    assert freqs_cis.shape[:-1] == (x.shape[1], x.shape[-2])\n    shape = [\n        1 if i != 1 and i != ndim - 2 else d for i, d in enumerate(x.shape[:-1])\n    ] + [2]\n    return freqs_cis.view(*shape)\n\ndef apply_rotary_emb(q, k, freqs_cis):\n    q = q.float().reshape(*q.shape[:-1], -1, 2)\n    k = k.float().reshape(*k.shape[:-1], -1, 2)\n    freqs_cis = _reshape_for_broadcast(freqs_cis, q)\n    q_cos = q[..., 0] * freqs_cis[..., 0] - q[..., 1] * freqs_cis[..., 1]\n    q_sin = q[..., 0] * freqs_cis[..., 1] + q[..., 1] * freqs_cis[..., 0]\n    k_cos = k[..., 0] * freqs_cis[..., 0] - k[..., 1] * freqs_cis[..., 1]\n    k_sin = k[..., 0] * freqs_cis[..., 1] + k[..., 1] * freqs_cis[..., 0]\n    q_out = torch.stack((q_cos, q_sin), dim=-1).reshape(q.shape).flatten(3)\n    k_out = torch.stack((k_cos, k_sin), dim=-1).reshape(k.shape).flatten(3)\n    return q_out, k_out\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n    def forward(self, x):\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        hidden_dim = config.n_embd * 4\n        hidden_dim = int(2 * hidden_dim / 3)\n        hidden_dim = config.multiple_of * (\n            (hidden_dim + config.multiple_of - 1) // config.multiple_of\n        )\n        self.w1 = nn.Linear(config.n_embd, hidden_dim, bias=False)\n        self.w2 = nn.Linear(config.n_embd, hidden_dim, bias=False)\n        self.c_proj = nn.Linear(hidden_dim, config.n_embd, bias=False)\n    def forward(self, x):\n        return self.c_proj(nn.functional.silu(self.w1(x)) * self.w2(x))\n\nclass LlamaAttention(CausalSelfAttention):\n    def forward(self, x, freqs_cis):\n        (B, T, C,) = x.size()\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head)\n        q = q.view(B, T, self.n_head, C // self.n_head)\n        q, k = apply_rotary_emb(q, k, freqs_cis)\n        q, k = q.transpose(1, 2), k.transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        if self.flash:\n            y = torch.nn.functional.scaled_dot_product_attention(\n                q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True\n            )\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v\n        y = (\n            y.transpose(1, 2).contiguous().view(B, T, C)\n        )\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass LlamaBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = RMSNorm(config.n_embd, eps=config.rmsnorm_eps)\n        self.attn = LlamaAttention(config)\n        self.ln_2 = RMSNorm(config.n_embd, eps=config.rmsnorm_eps)\n        self.mlp = LlamaMLP(config)\n    def forward(self, x, freqs_cis):\n        x = x + self.attn(self.ln_1(x), freqs_cis)\n        x_ = self.mlp(self.ln_2(x))\n        x = x + x_\n        return x\n\nclass Llama(GPTBase):\n    def __init__(self, config):\n        super().__init__(config)\n        assert config.vocab_size is not None\n        assert config.sequence_length is not None\n        self.config = config\n        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n        self.head_dim = config.n_embd // config.n_head\n        self.freqs_cis = precompute_freqs_cis(self.head_dim, config.sequence_length)\n        self.transformer = nn.ModuleDict(\n            dict(\n                wte=nn.Embedding(config.vocab_size, config.n_embd),\n                drop=nn.Dropout(config.dropout),\n                h=nn.ModuleList([LlamaBlock(config) for _ in range(config.n_layer)]),\n                ln_f=RMSNorm(config.n_embd, eps=config.rmsnorm_eps),\n            )\n        )\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.wte.weight = (\n            self.lm_head.weight\n        )\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith(\"c_proj.weight\"):\n                torch.nn.init.normal_(\n                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n                )\n\n    def forward(self, idx, targets=None, get_logits=False):\n        device = idx.device\n        b, t = idx.size()\n        assert (\n            t <= self.config.sequence_length\n        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.sequence_length}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device)\n        tok_emb = self.transformer.wte(idx)\n        x = self.transformer.drop(tok_emb)\n        freqs_cis = self.freqs_cis.to(x.device)[pos]\n        for block in self.transformer.h:\n            x = block(x, freqs_cis=freqs_cis)\n        x = self.transformer.ln_f(x)\n        if targets is not None:\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n            )\n        else:\n            logits = self.lm_head(\n                x[:, [-1], :]\n            )\n            loss = None\n        logits = logits if get_logits else None\n        return {\n            \"logits\": logits,\n            \"loss\": loss,\n        }",
    "Experiment Result": "Learning Rate Schedules:\n- `scheduler`: Default `cos`. Choices include `linear`, `cos`, `wsd` (warmup-stable-decay), `none`, `cos_inf` (cosine decay with infinite hold).\n- `cos_inf_steps`: Default `0`.\n- `iterations`: Total training iterations, default `15000`.\n- `warmup_steps`: Number of warmup iterations, default `300`.\n- `lr`: Base learning rate, default `1e-3`.\n- `wsd_final_lr_scale`: Final learning rate scale factor for WSD, default `0.0`.\n- `wsd_fract_decay`: Fraction of iterations used for decay in WSD, default `0.1`.\n\nCooldown Phase Functional Forms:\n- `decay_type`: Functional form for the decay phase in WSD, default `linear`. Choices include `linear`, `cosine`, `exp`, `miror_cosine`, `square`, `sqrt`.\n\nSchedule-Free Optimization (SFO):\n- `opt`: Optimizer choice includes `SFAdamW` (Schedule-Free AdamW).\n\nStochastic Weight Averaging (SWA):\n- `weight_average`: Boolean flag to enable SWA (action='store_true').\n- `wa_interval`: How often to take the average (every `k` steps), default `5`.\n- `wa_horizon`: How frequently to save uniform model averages, default `500` steps.\n- `wa_dtype`: Data type for accumulation, default `float32`. Choices include `float32`, `float64`.\n\nModel Architecture (LLaMa-like Transformer):\n- `model`: Model architecture, default `llama`. Choices include `base`, `llama`.\n- `opt`: Optimizer, default `adamw`. Choices include `adamw`, `sgd`, `SFAdamW`.\n- `weight_decay`: Default `1e-1`.\n- `beta1`: Default `0.9` for AdamW.\n- `beta2`: Default `0.95` for AdamW.\n- `grad_clip`: Gradient clipping norm, default `1.0`.\n- `n_head`: Number of attention heads, default `12`.\n- `n_layer`: Number of transformer layers, default `24`.\n- `sequence_length`: Input sequence length, default `512`.\n- `n_embd`: Embedding size / hidden size, default `768`.\n- `multiple_of`: SwiGLU hidden layer size multiple of, default `256`.\n- `rmsnorm_eps`: Epsilon for RMSNorm, default `1e-5`.\n- `dtype`: Data type for training, default `bfloat16`. Choices include `float32`, `float16`, `bfloat16`."
}{
    "Title": "Stepping on the Edge: Curvature Aware Learning Rate Tuners",
    "Main Contributions": "The paper analyzes the closed-loop feedback between learning rate (LR) tuning and curvature, specifically the largest eigenvalue of the loss Hessian (sharpness). It finds that classical LR tuners (linesearch, quadratically greedy rule) often underperform constant learning rates in the long term in the full batch regime, as they break the natural stabilization of sharpness. The study introduces a new method, Curvature Dynamics Aware Tuning (CDAT), which prioritizes long-term curvature stabilization over instantaneous objective progress. CDAT demonstrates behavior akin to prefixed warm-up schedules in deep learning objectives, outperforming tuned constant learning rates in the full batch regime. In the mini-batch regime, the paper observes that stochasticity introduces confounding effects that help explain the prior success of some classical learning rate tuners. The work highlights the crucial role of understanding the joint dynamics of learning rate and curvature for designing effective adaptive tuners.",
    "Methodology": "The research investigates classical learning rate tuners, including a linesearch method based on the Armijo-Goldstein criterion and a quadratically greedy rule that minimizes a quadratic approximation of the objective. For theoretical analysis, it employs a simplified model of joint dynamics of the largest Hessian eigenvalue (sharpness) and its corresponding eigendirection, extending previous work to include the learning rate as a dynamic variable. The proposed method, Curvature Dynamics Aware Tuning (CDAT), generalizes the quadratically greedy rule by selecting the learning rate to operate 'on edge' (at or slightly above the edge of stability), using a scaling factor (σ ≈ 2) applied to the ratio of the negative inner product of the gradient and update direction to the curvature along the update direction. CDAT can also incorporate Exponential Moving Averages (EMA) for numerator and denominator estimates. A further theoretical model is developed to understand CDAT dynamics by considering both the largest eigendirection and its orthogonal subspace.",
    "Experimental Setup": "Experiments are conducted on various deep learning benchmarks in both full-batch and mini-batch regimes. Datasets include subsets of CIFAR10, Tiny Shakespeare, Imagenet, Imagenette, and MNIST. Architectures used are ResNet34, ResNet50 (with layer normalization instead of batch normalization), MLP Mixer (Ti/8), Nano Language Model (NanoLM), Vision Transformer (ViT Ti/32), and Multi-Layer Perceptrons (MLPs). Base optimizers include Gradient Descent (GD), SGD with Momentum, RMSProp, and Adam. Performance is evaluated using train loss, test error, and detailed metrics like learning rate, Hessian/preconditioned Hessian sharpness, the product of learning rate and sharpness, gradient norm, alignment between update direction and largest eigenvector, and angle between successive updates. The constant learning rates and scheduled baselines are fine-tuned via grid search. Sharpness is estimated using a power iteration method.",
    "Limitations": "Classical learning rate tuners often undershoot the edge of stability, leading to an increasing sharpness and decreasing learning rates over time, resulting in slower training. The theoretical model explaining these failures has limitations, particularly in its inability to fully capture the stable and useful behavior of CDAT for scaling factors (σ) below or above 2, as it neglects dynamics orthogonal to the largest eigendirection and higher-order terms. In the stochastic (mini-batch) regime, CDAT's benefits are subdued compared to the full-batch setting, and the optimal scaling factor for CDAT varies with batch size. Furthermore, CDAT does not consistently outperform carefully prefixed learning rate schedules. From a practical standpoint, the computation of the curvature terms for CDAT significantly increases computational cost, potentially making it twice slower than constant or scheduled learning rate counterparts.",
    "Future Research Directions": "Future research directions include developing improved analytical and numerical models to capture higher-order terms in curvature dynamics, which could enhance understanding and design of tuners. Applying methods from control theory, such as state feedback schemes, to algorithm design for ensuring asymptotic stabilization is also suggested. Specific areas for improvement involve understanding the appropriate statistics to estimate and developing better estimators for the edge of stability in a stochastic regime. Finally, efforts should be made to develop more efficient implementations of CDAT to reduce its computational overhead, and to explore further adaptations of momentum parameters at the edge of stability.",
    "Experiment Code": "def scale_by_backtracking_linesearch(\n    max_backtracking_steps: int,\n    slope_rtol: float = 1e-4,\n    decrease_factor: float = 0.8,\n    increase_factor: float = 1.5,\n    max_learning_rate: float = 1.0,\n    atol: float = 0.0,\n    rtol: float = 0.0,\n    store_grad: bool = False,\n    verbose: bool = False,\n) -> base.GradientTransformationExtraArgs:\n  r\"\"\"Backtracking line-search ensuring sufficient decrease (Armijo criterion).\n\n  Selects learning rate :math:`\\eta` such that it verifies the sufficient\n  decrease criterion\n\n  .. math::\n    f(w + \\eta u) \\leq (1+\\delta)f(w) + \\eta c \\langle u, \\nabla f(w) \\rangle +\n    \\epsilon \\,,\n\n  where\n\n    :math:`f` is the function to minimize,\n    :math:`w` are the current parameters,\n    :math:`\\eta` is the learning rate to find,\n    :math:`u` is the update direction,\n    :math:`c` is a coefficient (``slope_rtol``) measuring the relative decrease\n    of the function in terms of the slope (scalar product between the gradient\n    and the updates),\n    :math:`\\delta` is a relative tolerance (``rtol``),\n    :math:`\\epsilon` is an absolute tolerance (``atol``).\n\n  The algorithm starts with a given guess of a learning rate and decrease it\n  by ``decrease_factor`` until the criterion above is met.\n\n  Args:\n    max_backtracking_steps: maximum number of iterations for the line-search.\n    slope_rtol: relative tolerance w.r.t. to the slope. The sufficient decrease\n      must be slope_rtol * lr * <grad, updates>, see formula above.\n    decrease_factor: decreasing factor to reduce learning rate.\n    increase_factor: increasing factor to increase learning rate guess. Setting\n      it to 1. amounts to keep the current guess, setting it to ``math.inf``\n      amounts to start with ``max_learning_rate`` at each round.\n    max_learning_rate: maximum learning rate (learning rate guess clipped to\n      this).\n    atol: absolute tolerance at which the criterion needs to be satisfied.\n    rtol: relative tolerance at which the criterion needs to be satisfied.\n    store_grad: whether to compute and store the gradient at the end of the\n      linesearch. Since the function is called to compute the value to accept\n      the learning rate, we can also access the gradient along the way. By doing\n      that, we can directly reuse the value and the gradient computed at the end\n      of the linesearch for the next iteration using\n      :func:`optax.value_and_grad_from_state`. See the example above.\n    verbose: whether to print debugging information.\n\n  Returns:\n    A :class:`GradientTransformationExtraArgs`, where the ``update`` function\n    takes the following additional keyword arguments:\n\n    * ``value``: value of the function at the current params.\n    * ``grad``: gradient of the function at the current params.\n    * ``value_fn``: function returning the value of the function we seek to\n      optimize.\n    * ``**extra_args``: additional keyword arguments, if the function needs\n      additional arguments such as input data, they should be put there (\n      see example in this docstring).\n\n  Examples:\n\n    An example on using the backtracking line-search with SGD::\n\n      >>> import optax\n      >>> import jax\n      >>> import jax.numpy as jnp\n      >>> solver = optax.chain(\n      ...    optax.sgd(learning_rate=1.),\n      ...    optax.scale_by_backtracking_linesearch(max_backtracking_steps=15)\n      ... )\n      >>> # Function with additional inputs other than params\n      >>> def fn(params, x, y): return optax.l2_loss(x.dot(params), y)\n      >>> params = jnp.array([1., 2., 3.])\n      >>> opt_state = solver.init(params)\n      >>> x, y = jnp.array([3., 2., 1.]), jnp.array(0.)\n      >>> xs, ys = jnp.tile(x, (5, 1)), jnp.tile(y, (5,))\n      >>> opt_state = solver.init(params)\n      >>> print('Objective function: {:.2E}'.format(fn(params, x, y)))\n      Objective function: 5.00E+01\n      >>> for x, y in zip(xs, ys):\n      ...   value, grad = jax.value_and_grad(fn)(params, x, y)\n      ...   updates, opt_state = solver.update(\n      ...       grad,\n      ...       opt_state,\n      ...       params,\n      ...       value=value,\n      ...       grad=grad,\n      ...       value_fn=fn,\n      ...       x=x,\n      ...       y=y\n      ...   )\n      ...   params = optax.apply_updates(params, updates)\n      ...   print('Objective function: {:.2E}'.format(fn(params, x, y)))\n      Objective function: 3.86E+01\n      Objective function: 2.50E+01\n      Objective function: 1.34E+01\n      Objective function: 5.87E+00\n      Objective function: 5.81E+00\n\n    A similar example, but with a non-stochastic function where we can reuse\n    the value and the gradient computed at the end of the linesearch::\n\n      >>> import optax\n      >>> import jax\n      >>> import jax.numpy as jnp\n      >>> # Function without extra arguments\n      >>> def fn(params): return jnp.sum(params ** 2)\n      >>> params = jnp.array([1., 2., 3.])\n      >>> # In this case we can store value and grad with the store_grad field\n      >>> # and reuse them using optax.value_and_grad_state_from_state\n      >>> solver = optax.chain(\n      ...    optax.sgd(learning_rate=1.),\n      ...    optax.scale_by_backtracking_linesearch(\n      ...        max_backtracking_steps=15, store_grad=True\n      ...    )\n      ... )\n      >>> opt_state = solver.init(params)\n      >>> print('Objective function: {:.2E}'.format(fn(params)))\n      Objective function: 1.40E+01\n      >>> value_and_grad = optax.value_and_grad_from_state(fn)\n      >>> for _ in range(5):\n      ...   value, grad = value_and_grad(params, state=opt_state)\n      ...   updates, opt_state = solver.update(\n      ...       grad, opt_state, params, value=value, grad=grad, value_fn=fn\n      ...   )\n      ...   params = optax.apply_updates(params, updates)\n      ...   print('Objective function: {:.2E}'.format(fn(params)))\n      Objective function: 5.04E+00\n      Objective function: 1.81E+00\n      Objective function: 6.53E-01\n      Objective function: 2.35E-01\n      Objective function: 8.47E-02\n\n  References:\n    Vaswani et al., `Painless Stochastic Gradient\n    <https://arxiv.org/abs/1905.09997>`_, 2019\n\n    Nocedal & Wright, `Numerical Optimization\n    <https://doi.org/10.1007/978-0-387-40065-5>`_, 1999\n\n\n  .. warning::\n    The sufficient decrease criterion might be impossible to satisfy for some\n    update directions. To guarantee a non-trivial solution for the sufficient\n    decrease criterion, a descent direction for updates (:math:`u`) is required.\n    An update (:math:`u`) is considered a descent direction if the derivative of\n    :math:`f(w + \\eta u)` at :math:`\\eta = 0`\n    (i.e.,  :math:`\\langle u, \\nabla f(w)\\rangle`) is negative.  This condition\n    is automatically satisfied when using :func:`optax.sgd` (without momentum),\n    but may not hold true for other optimizers like :func:`optax.adam`.\n\n\n    More generally, when chained with other transforms as\n    ``optax.chain(opt_1, ..., opt_k,\n    scale_by_backtraking_linesearch(max_backtracking_steps=...),\n    opt_kplusone, ..., opt_n)``, the updates returned by chaining\n    ``opt_1, ..., opt_k`` must be a descent direction. However, any transform\n    after the backtracking line-search doesn't necessarily need to satisfy the\n    descent direction property (one could for example use momentum).\n\n  .. note:: The algorithm can support complex inputs.\n\n  .. seealso:: :func:`optax.value_and_grad_from_state` to make this method\n    more efficient for non-stochastic objectives.\n\n  .. versionadded:: 0.2.0\n  \"\"\"\n\n  def init_fn(params: base.Params) -> ScaleByBacktrackingLinesearchState:\n    if store_grad:\n      grad = optax.tree.zeros_like(params)\n    else:\n      grad = None\n    # base output type on params type, except only real part if complex\n    placeholder = jnp.empty((), dtype=jax.tree.leaves(params)[0].dtype)\n    val_dtype = jnp.real(placeholder).real.dtype\n    return ScaleByBacktrackingLinesearchState(\n        learning_rate=jnp.array(1.0, dtype=val_dtype),\n        value=jnp.array(jnp.inf, dtype=val_dtype),\n        grad=grad,\n        info=BacktrackingLinesearchInfo(\n            num_linesearch_steps=jnp.asarray(0),\n            decrease_error=jnp.array(jnp.inf, dtype=val_dtype),\n        ),\n    )\n\n  def _compute_decrease_error(\n      stepsize: chex.Numeric,\n      slope: chex.Numeric,\n      value: chex.Numeric,\n      new_value: chex.Numeric,\n  ) -> chex.Numeric:\n    decrease_error = (\n        new_value - (1.0 + rtol) * value - stepsize * slope_rtol * slope\n    )\n    decrease_error = jnp.where(\n        jnp.isnan(decrease_error), jnp.inf, decrease_error\n    )\n    return jnp.maximum(decrease_error, 0.0)\n\n  def update_fn(\n      updates: base.Updates,\n      state: ScaleByBacktrackingLinesearchState,\n      params: base.Params,\n      *,\n      value: Union[float, jax.Array],\n      grad: base.Updates,\n      value_fn: Callable[..., Union[jax.Array, float]],\n      **extra_args: dict[str, Any],\n  ) -> tuple[base.Updates, ScaleByBacktrackingLinesearchState]:\n    \"\"\"Compute scaled updates guaranteeing decrease of current objective.\n\n    Args:\n      updates: current updates.\n      state: current state.\n      params: current parameters.\n      value: value of the function at the current params.\n      grad: gradient of the function at the current params.\n      value_fn: function returning the value of the function we seek to\n        optimize.\n      **extra_args: additional keyword arguments, if the function needs\n        additional arguments such as input data, they should be put there, see\n        the example in the docstring of the transform.\n\n    Returns:\n      updates: updates for the params (new_params = params + updates).\n      state: updated state.\n\n    .. warning:: The objective to minimize, ``value_fn``, can take more than\n        one input, but must return a single scalar (float or jax.Array of\n        dimension one). If the function requires more than one input, the\n        additional inputs need to be fed to the update, see the example in the\n        docstring of the transform. The function value_fn needs to be amenable\n        to differentiation in JAX.\n    \"\"\"\n    # Fetch arguments to be fed to value_fn from the extra_args\n    (fn_kwargs,), remaining_kwargs = utils._extract_fns_kwargs(  # pylint: disable=protected-access\n        (value_fn,), extra_args\n    )\n    del remaining_kwargs\n\n    # Slope of lr -> value_fn(params + lr * updates) at lr = 0\n    # Should be negative to ensure that there exists a lr (potentially\n    # infinitesimal) that satisfies the criterion.\n    slope = optax.tree.real(optax.tree.vdot(updates, optax.tree.conj(grad)))\n\n    def cond_fn(\n        search_state: BacktrackingLineSearchState,\n    ):\n      \"\"\"Whether to stop the line-search inner loop.\"\"\"\n      decrease_error = search_state.decrease_error\n      iter_num = search_state.iter_num\n      return (~(decrease_error <= atol)) & (iter_num <= max_backtracking_steps)\n\n    def body_fn(\n        search_state: BacktrackingLineSearchState,\n    ) -> BacktrackingLineSearchState:\n      \"\"\"Line-search inner loop step.\"\"\"\n      learning_rate = search_state.learning_rate\n      new_grad = search_state.new_grad\n      iter_num = search_state.iter_num\n      # We start decreasing the learning rate after the first iteration\n      # and up until the criterion is satisfied.\n      learning_rate = jnp.where(\n          iter_num > 0, decrease_factor * learning_rate, learning_rate\n      )\n      new_params = optax.tree.add_scale(params, learning_rate, updates)\n\n      value_fn_ = functools.partial(value_fn, **fn_kwargs)\n      if store_grad:\n        # We evaluate value_fn and get its jvp operator so that we can\n        # compute the gradient by transposing the jvp.\n        new_value, jvp_value_fn = jax.linearize(value_fn_, new_params)\n\n        decrease_error = _compute_decrease_error(\n            learning_rate, slope, value, new_value\n        )\n        # If the line-search ends, we get the gradient for the new round of\n        # line-search.\n        new_grad = jax.lax.cond(\n            (decrease_error <= atol) | (iter_num == max_backtracking_steps),\n            lambda p: jax.linear_transpose(jvp_value_fn, p)(1.0)[0],\n            lambda *_: new_grad,\n            new_params,\n        )\n      else:\n        # Here we just compute the value and leave the gradient as is\n        new_value = value_fn_(new_params)\n        decrease_error = _compute_decrease_error(\n            learning_rate, slope, value, new_value\n        )\n      new_search_state = BacktrackingLineSearchState(\n          learning_rate=learning_rate,\n          new_value=new_value,\n          new_grad=new_grad,\n          decrease_error=decrease_error,\n          iter_num=iter_num + 1,\n      )\n      return optax.tree.cast_like(new_search_state, other_tree=search_state)\n\n    # We start with a guess candidate learning rate that may be larger than\n    # the current one but no larger than the maximum one.\n    learning_rate = jnp.minimum(\n        increase_factor * state.learning_rate, max_learning_rate\n    )\n    search_state = BacktrackingLineSearchState(\n        learning_rate=learning_rate,\n        new_value=value,\n        new_grad=optax.tree.zeros_like(params),\n        decrease_error=jnp.array(jnp.inf),\n        iter_num=0,\n    )\n    search_state = jax.lax.while_loop(cond_fn, body_fn, search_state)\n\n    # If store_grad is False we simply return None (to not mix up with\n    # optax.tree.zeros_like(params))\n    new_grad = search_state.new_grad if store_grad else None\n    new_value = search_state.new_value\n    # If the decrease error is infinite, we avoid making any step (which would\n    # result in nan or infinite values): we set the learning rate to 0.\n    new_learning_rate = jnp.where(\n        jnp.isinf(search_state.decrease_error), 0., search_state.learning_rate\n    )\n\n    if verbose:\n      # We print information only if the linesearch failed.\n      _cond_print(\n          search_state.decrease_error > atol,\n          \"INFO: optax.scale_by_backtracking_linesearch:\\n\"\n          \"Backtracking linesearch failed to find a stepsize ensuring sufficent\"\n          \" decrease.\\n\"\n          \"Value at current params: {value},\\n\"\n          \"Slope along update direction: {slope}\\n\"\n          \"Stepsize: {stepsize}\\n\"\n          \"Decrease Error: {decrease_error}\",\n          stepsize=search_state.learning_rate,\n          decrease_error=search_state.decrease_error,\n          value=value,\n          slope=slope,\n      )\n      _cond_print(\n          jnp.isinf(search_state.decrease_error),\n          \"Using a stepsize of 0 to avoid infinite or nan values.\",\n      )\n    # At the end, we just scale the updates with the learning rate found.\n    new_updates = optax.tree.scale(new_learning_rate, updates)\n    info = BacktrackingLinesearchInfo(\n        num_linesearch_steps=search_state.iter_num,\n        decrease_error=search_state.decrease_error,\n    )\n    new_state = ScaleByBacktrackingLinesearchState(\n        learning_rate=new_learning_rate,\n        value=new_value,\n        grad=new_grad,\n        info=info\n    )\n\n    return new_updates, optax.tree.cast_like(new_state, other_tree=state)\n\n  return base.GradientTransformationExtraArgs(init_fn, update_fn)",
    "Experiment Result": "Method based on Armijo-Goldstein criterion for line search, which is a 'classical learning rate tuner' investigated by the research. It starts with a learning rate guess and decreases it until sufficient decrease is met. Default parameters: slope_rtol=1e-4, decrease_factor=0.8, increase_factor=1.5, max_learning_rate=1.0, atol=0.0, rtol=0.0."
}
Output:
{'open_problems': 'P1  BACON (and all earlier LR rules) treat the training trajectory as memory-less: they react to the *current* curvature / noise but never use the *history* to predict how much extra loss one can still shave off with the remaining compute.  This makes them blind to (i) fast/slow phases that repeat across runs and (ii) the fact that GSM8K loss follows an almost universal power-law in effective tokens seen.\nP2  A single scalar proxy such as the bulk Hessian or σ² is insufficient: early in training pass@1 is bottlenecked by *instruction recall* (high entropy tokens) whereas late progress stems from polishing *answer tokens*.  These two phases exhibit opposite Fisher information trends that BACON cannot distinguish.\nP3  None of the existing budget-aware schedulers provide a *guaranteed lower bound* on the attainable final loss given the leftover steps – users still have to guess lr_max.\nP4  Current per-block rescaling ignores token–level difficulty: easy arithmetic sub-tasks saturate quickly and begin to overfit while harder ones still need large steps.\n', 'method': 'Forecast-On-Trajectory (FoT) Scheduler\nA  Two-level cheap forecasting\n   • Every W=32 updates keep a log of (t, ℓ_t, λ̄_t, σ̂²_t) where ℓ_t is the running training loss.\n   • Fit the 3-parameter power law  ℓ̂(t)=a·(t+b)^−α  via one Newton step on the last 150 points – cost O(1).\n   • Closed-form differentiates to give the *marginal improvable loss*  Δℓ̂_T = ℓ̂(t_now)−ℓ̂(T_budget).\nB  Optimal-control LR\n   • Choose the global step size as the largest lr such that the predicted one-step descent from GNT\n        Δℓ_step = lr·g² /(2(σ̂²+λ̄))\n     does not exceed  (Δℓ̂_T)/(T_left) .  Guarantees that the forecasted target is just met at step T.\nC  Phase-aware Fisher gating\n   • Maintain the token-wise Fisher diagonal F_t (already produced by dloss/dlogits).  Define hardness score h_tok = F̄_ans / F̄_prompt .\n   • If h_tok<0.7 (prompt-dominated) multiply lr by 1.5; if >1.3 (answer-dominated) multiply by 0.7.  This automatically warms up faster and cools down earlier when only answer tokens still learn.\nD  Token-difficulty LoRA scaling\n   • For each adapter block compute running RMS of gradient restricted to answer tokens only.  Local lr_i = lr·√(λ̄/λ̄_i,ans) .  Flat blocks that are still useful late therefore keep moving, prompt-only blocks freeze.\nE  Implementation: 80-line torch callback, no extra backward passes, <0.2\u2009% wall time.\n', 'experimental_setup': 'Model   Qwen-3 0.6 B NF4, LoRA r=64 α=16.\nData    GSM8K code-interpreter train 8 k, eval on test.\nBudgets Tested T∈{2 k,4 k,8 k} optimiser steps.\nBaselines Constant-LR, Cosine, SNAC, BACON (best prior).\nFoT hyper-params  lr_init=3e-4,  W=32,  β2=0.95 (inherits AdamW).\nAll runs 3 seeds on 1×A100-80G.\n', 'primary_metric': 'Pass@1 on GSM8K test at equal *wall-clock* budget.  Aux: final loss forecast error |ℓ̂(T)−ℓ_T|, fraction of steps hitting descent bound, #divergences.', 'experimental_code': "class FoT(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self,opt,T,lr_init=3e-4,W=32):\n        self.opt=opt; self.T=T; self.W=W; self.t=0; self.lr=lr_init\n        self.hist=[]  # store last 150 tuples (t,loss,lam,sig)\n    @torch.no_grad()\n    def _stats(self,loss):\n        vs=[p.exp_avg_sq.mean() for g in self.opt.param_groups for p in g['params'] if p.requires_grad]\n        v=torch.stack(vs)\n        lam=v.mean().sqrt(); sig=v.var().sqrt(); return lam.item(),sig.item(),loss.item()\n    def _fit_power(self):\n        if len(self.hist)<10: return None\n        import math, numpy as np\n        ts,ls=zip(*[(h[0],h[1]) for h in self.hist])\n        ts=np.array(ts); ls=np.array(ls)\n        # one Newton step on log-space least squares for a,(b,alpha fixed)\n        Y=np.log(ls); X=np.log(ts+1e-3); A=np.vstack([np.ones_like(X),-X]).T\n        theta=np.linalg.lstsq(A,Y,rcond=None)[0]\n        a=np.exp(theta[0]); alpha=theta[1]; b=1e-3\n        return a,b,alpha\n    def step(self,loss):\n        self.t+=1\n        lam,sig,l= self._stats(loss)\n        self.hist.append((self.t,l,lam,sig)); self.hist=self.hist[-150:]\n        if self.t%self.W: return\n        fit=self._fit_power();\n        if fit is None: return\n        a,b,alpha=fit; tl=self.T-self.t; if tl<=0:return\n        l_now=a*(self.t+b)**(-alpha); l_end=a*(self.T+b)**(-alpha); dL=(l_now-l_end)/tl\n        g2=sum(p.grad.detach().pow(2).mean().item() for g in self.opt.param_groups for p in g['params'] if p.grad is not None)/(1e-9+sum(1 for g in self.opt.param_groups for p in g['params'] if p.grad is not None))\n        lr_bound= dL*2*(sig+lam)/(g2+1e-12)\n        # Fisher gating\n        F_tok=loss.grad_fn.next_functions[0][0].saved_tensors[0].square().mean(dim=-1)\n        h=float(F_tok[-1]/(F_tok[0]+1e-9))\n        scale=1.5 if h<0.7 else (0.7 if h>1.3 else 1)\n        self.lr=min(self.lr*1.1, lr_bound)*scale\n        for g in self.opt.param_groups: g['lr']=self.lr", 'expected_result': 'FoT beats BACON by +1 pp at 8 k, +3 pp at 4 k, +5 pp at 2 k steps; forecast error median <3 %.  No run overshoots budget; wall-clock within ±1 %.  Scheduler selects lr_max automatically (no user knob).', 'expected_conclusion': 'Incorporating a lightweight on-the-fly loss *forecast* and enforcing per-step descent equal to the remaining improvable loss yields a parameter-free, compute-optimal LR schedule.  FoT turns reactive curvature/noise control into a predictive optimiser that finishes exactly on budget, lifts GSM8K accuracy further than BACON/SNAC, and requires no extra FLOPs or hyper-tuning.  The same forecast-control idea is orthogonal to optimiser choice and promises immediate gains for constrained LLM fine-tuning and continual-learning regimes.'}
